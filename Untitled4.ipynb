{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb5fd72-8d74-46f0-b29f-622014ea5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.resnet=models.resnet50(pretrained=True)\n",
    "        # 删除 fc 层\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.resnet.fc = nn.Sequential()\n",
    "        # self.fc = nn.Linear(in_features=2048*2, out_features=num_classes, bias=True)   \n",
    "        self.fc = nn.Linear(in_features=2048, out_features=num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x1,x2):\n",
    "        x1=self.resnet(x1)\n",
    "        x2=self.resnet(x2)\n",
    "        x=x1-x2\n",
    "        # x=torch.cat((x1, x2), dim=1)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# model=ResNet()\n",
    "# inp = torch.randn(1, 3, 224, 224)  # 假设输入为 224x224 的图像\n",
    "# output = model(inp,inp)\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de4310b-7868-4637-8418-c695cc0fa56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from matplotlib import pyplot as plt\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06a8dff7-5b8f-4003-955c-97690e2f99a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmileDataset(Dataset):\n",
    "    def __init__(self, data_dir,transform=None):\n",
    "        self.data_info = self.get_img_info(data_dir)  \n",
    "        self.transform = transform\n",
    "        print(transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path_img,path_img1, label = self.data_info[index]\n",
    "        img = Image.open(path_img).convert('RGB')\n",
    "        img1 = Image.open(path_img1).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            img1 = self.transform(img1)\n",
    "        label = torch.tensor(np.array(label))\n",
    "        return img,img1, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    def get_img_info(self,data_dir):\n",
    "        data_info = []\n",
    "        label_names=os.listdir(data_dir)\n",
    "        for label in label_names:\n",
    "            img_names = os.listdir(os.path.join(data_dir, label))\n",
    "            #选取jpg结尾的图像\n",
    "            img_names = list(filter(lambda x: x.endswith('.jpg'), img_names))\n",
    "            # 遍历图片\n",
    "            for img_name in img_names:\n",
    "                path_img = os.path.join(data_dir, label, img_name)\n",
    "                path_img1 = path_img.replace(\"train\",\"train1\").replace(\"val\",\"val1\")\n",
    "                data_info.append((path_img,path_img1, label_names.index(label)))\n",
    "        return data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bb6fb03-00b8-46ef-9bee-9a050a173fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 数据的路径\n",
    "split_dir = './pics2_imglabel_datasets/'\n",
    "train_dir = os.path.join(split_dir, 'train/')\n",
    "valid_dir = os.path.join(split_dir, 'val/')\n",
    "\n",
    "# 超参数设置\n",
    "EPOCH = 135   #遍历数据集次数\n",
    "pre_epoch = 0  # 定义已经遍历数据集的次数\n",
    "LR = 0.0001        #学习率\n",
    "\n",
    "# 准备数据集并预处理\n",
    "transform_train = transforms.Compose([\n",
    "    # transforms.RandomCrop(32, padding=4),  #先四周填充0，在吧图像随机裁剪成32*32\n",
    "    transforms.Resize((256,256)),\n",
    "    # transforms.RandomHorizontalFlip(),  #图像一半的概率翻转，一半的概率不翻转\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), #R,G,B每层的归一化用到的均值和方差\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "## 构建MyDataset实例\n",
    "train_data = SmileDataset(data_dir=train_dir, transform=transform_train)\n",
    "valid_data = SmileDataset(data_dir=valid_dir, transform=transform_test)\n",
    "\n",
    "# 构建DataLoader\n",
    "trainloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(dataset=valid_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e11719a8-8a6c-40c0-9f4d-19bc08f6c885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boer/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/boer/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training, Resnet-50!\n",
      "\n",
      "Epoch: 1\n",
      "[epoch:1, iter:1] Loss: 0.699 | Acc: 56.250% \n",
      "[epoch:1, iter:2] Loss: 0.706 | Acc: 50.000% \n",
      "[epoch:1, iter:3] Loss: 0.713 | Acc: 47.917% \n",
      "[epoch:1, iter:4] Loss: 0.710 | Acc: 48.438% \n",
      "[epoch:1, iter:5] Loss: 0.698 | Acc: 50.000% \n",
      "[epoch:1, iter:6] Loss: 0.697 | Acc: 53.125% \n",
      "[epoch:1, iter:7] Loss: 0.687 | Acc: 56.250% \n",
      "[epoch:1, iter:8] Loss: 0.680 | Acc: 58.594% \n",
      "[epoch:1, iter:9] Loss: 0.677 | Acc: 57.639% \n",
      "[epoch:1, iter:10] Loss: 0.678 | Acc: 57.500% \n",
      "[epoch:1, iter:11] Loss: 0.678 | Acc: 56.818% \n",
      "[epoch:1, iter:12] Loss: 0.678 | Acc: 56.250% \n",
      "[epoch:1, iter:13] Loss: 0.685 | Acc: 55.288% \n",
      "[epoch:1, iter:14] Loss: 0.686 | Acc: 54.464% \n",
      "[epoch:1, iter:15] Loss: 0.693 | Acc: 54.167% \n",
      "[epoch:1, iter:16] Loss: 0.698 | Acc: 53.906% \n",
      "[epoch:1, iter:17] Loss: 0.691 | Acc: 54.779% \n",
      "[epoch:1, iter:18] Loss: 0.698 | Acc: 54.514% \n",
      "[epoch:1, iter:19] Loss: 0.704 | Acc: 54.605% \n",
      "[epoch:1, iter:20] Loss: 0.701 | Acc: 54.375% \n",
      "[epoch:1, iter:21] Loss: 0.702 | Acc: 54.464% \n",
      "[epoch:1, iter:22] Loss: 0.701 | Acc: 54.261% \n",
      "[epoch:1, iter:23] Loss: 0.705 | Acc: 54.076% \n",
      "[epoch:1, iter:24] Loss: 0.702 | Acc: 54.167% \n",
      "[epoch:1, iter:25] Loss: 0.705 | Acc: 53.750% \n",
      "[epoch:1, iter:26] Loss: 0.699 | Acc: 55.048% \n",
      "[epoch:1, iter:27] Loss: 0.699 | Acc: 54.861% \n",
      "[epoch:1, iter:28] Loss: 0.700 | Acc: 54.688% \n",
      "[epoch:1, iter:29] Loss: 0.698 | Acc: 54.741% \n",
      "[epoch:1, iter:30] Loss: 0.696 | Acc: 54.792% \n",
      "[epoch:1, iter:31] Loss: 0.689 | Acc: 55.645% \n",
      "[epoch:1, iter:32] Loss: 0.687 | Acc: 56.250% \n",
      "[epoch:1, iter:33] Loss: 0.685 | Acc: 57.008% \n",
      "[epoch:1, iter:34] Loss: 0.686 | Acc: 57.169% \n",
      "[epoch:1, iter:35] Loss: 0.689 | Acc: 56.607% \n",
      "[epoch:1, iter:36] Loss: 0.693 | Acc: 55.903% \n",
      "[epoch:1, iter:37] Loss: 0.693 | Acc: 56.081% \n",
      "[epoch:1, iter:38] Loss: 0.692 | Acc: 56.414% \n",
      "[epoch:1, iter:39] Loss: 0.692 | Acc: 56.250% \n",
      "[epoch:1, iter:40] Loss: 0.692 | Acc: 55.938% \n",
      "[epoch:1, iter:41] Loss: 0.692 | Acc: 55.945% \n",
      "[epoch:1, iter:42] Loss: 0.693 | Acc: 55.952% \n",
      "[epoch:1, iter:43] Loss: 0.694 | Acc: 55.669% \n",
      "[epoch:1, iter:44] Loss: 0.695 | Acc: 55.540% \n",
      "[epoch:1, iter:45] Loss: 0.694 | Acc: 55.972% \n",
      "[epoch:1, iter:46] Loss: 0.695 | Acc: 55.842% \n",
      "[epoch:1, iter:47] Loss: 0.696 | Acc: 55.452% \n",
      "[epoch:1, iter:48] Loss: 0.695 | Acc: 55.859% \n",
      "[epoch:1, iter:49] Loss: 0.696 | Acc: 55.740% \n",
      "[epoch:1, iter:50] Loss: 0.695 | Acc: 55.875% \n",
      "[epoch:1, iter:51] Loss: 0.694 | Acc: 55.882% \n",
      "[epoch:1, iter:52] Loss: 0.694 | Acc: 55.889% \n",
      "[epoch:1, iter:53] Loss: 0.695 | Acc: 55.778% \n",
      "[epoch:1, iter:54] Loss: 0.695 | Acc: 55.556% \n",
      "[epoch:1, iter:55] Loss: 0.696 | Acc: 55.227% \n",
      "[epoch:1, iter:56] Loss: 0.696 | Acc: 55.022% \n",
      "[epoch:1, iter:57] Loss: 0.695 | Acc: 55.154% \n",
      "[epoch:1, iter:58] Loss: 0.694 | Acc: 55.388% \n",
      "[epoch:1, iter:59] Loss: 0.694 | Acc: 55.403% \n",
      "[epoch:1, iter:60] Loss: 0.694 | Acc: 55.417% \n",
      "[epoch:1, iter:61] Loss: 0.694 | Acc: 55.328% \n",
      "[epoch:1, iter:62] Loss: 0.694 | Acc: 55.242% \n",
      "[epoch:1, iter:63] Loss: 0.697 | Acc: 54.960% \n",
      "[epoch:1, iter:64] Loss: 0.698 | Acc: 54.688% \n",
      "[epoch:1, iter:65] Loss: 0.699 | Acc: 54.519% \n",
      "[epoch:1, iter:66] Loss: 0.699 | Acc: 54.451% \n",
      "[epoch:1, iter:67] Loss: 0.698 | Acc: 54.757% \n",
      "[epoch:1, iter:68] Loss: 0.698 | Acc: 54.871% \n",
      "[epoch:1, iter:69] Loss: 0.698 | Acc: 54.710% \n",
      "[epoch:1, iter:70] Loss: 0.697 | Acc: 55.089% \n",
      "[epoch:1, iter:71] Loss: 0.697 | Acc: 55.018% \n",
      "[epoch:1, iter:72] Loss: 0.696 | Acc: 55.382% \n",
      "[epoch:1, iter:73] Loss: 0.696 | Acc: 55.223% \n",
      "[epoch:1, iter:74] Loss: 0.695 | Acc: 55.405% \n",
      "[epoch:1, iter:75] Loss: 0.695 | Acc: 55.500% \n",
      "[epoch:1, iter:76] Loss: 0.694 | Acc: 55.757% \n",
      "[epoch:1, iter:77] Loss: 0.694 | Acc: 55.682% \n",
      "[epoch:1, iter:78] Loss: 0.694 | Acc: 55.609% \n",
      "[epoch:1, iter:79] Loss: 0.694 | Acc: 55.538% \n",
      "[epoch:1, iter:80] Loss: 0.694 | Acc: 55.469% \n",
      "[epoch:1, iter:81] Loss: 0.694 | Acc: 55.633% \n",
      "[epoch:1, iter:82] Loss: 0.695 | Acc: 55.412% \n",
      "[epoch:1, iter:83] Loss: 0.694 | Acc: 55.648% \n",
      "[epoch:1, iter:84] Loss: 0.694 | Acc: 55.804% \n",
      "[epoch:1, iter:85] Loss: 0.693 | Acc: 55.809% \n",
      "[epoch:1, iter:86] Loss: 0.693 | Acc: 55.887% \n",
      "[epoch:1, iter:87] Loss: 0.693 | Acc: 55.819% \n",
      "[epoch:1, iter:88] Loss: 0.693 | Acc: 55.895% \n",
      "[epoch:1, iter:89] Loss: 0.693 | Acc: 55.829% \n",
      "[epoch:1, iter:90] Loss: 0.693 | Acc: 55.764% \n",
      "[epoch:1, iter:91] Loss: 0.692 | Acc: 55.907% \n",
      "[epoch:1, iter:92] Loss: 0.694 | Acc: 55.707% \n",
      "[epoch:1, iter:93] Loss: 0.694 | Acc: 55.645% \n",
      "[epoch:1, iter:94] Loss: 0.693 | Acc: 55.718% \n",
      "[epoch:1, iter:95] Loss: 0.694 | Acc: 55.658% \n",
      "[epoch:1, iter:96] Loss: 0.693 | Acc: 55.657% \n",
      "Waiting Test!\n",
      "测试分类准确率为：71.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 2\n",
      "[epoch:2, iter:97] Loss: 0.614 | Acc: 68.750% \n",
      "[epoch:2, iter:98] Loss: 0.586 | Acc: 68.750% \n",
      "[epoch:2, iter:99] Loss: 0.550 | Acc: 77.083% \n",
      "[epoch:2, iter:100] Loss: 0.536 | Acc: 81.250% \n",
      "[epoch:2, iter:101] Loss: 0.549 | Acc: 80.000% \n",
      "[epoch:2, iter:102] Loss: 0.558 | Acc: 78.125% \n",
      "[epoch:2, iter:103] Loss: 0.549 | Acc: 78.571% \n",
      "[epoch:2, iter:104] Loss: 0.529 | Acc: 81.250% \n",
      "[epoch:2, iter:105] Loss: 0.525 | Acc: 81.944% \n",
      "[epoch:2, iter:106] Loss: 0.541 | Acc: 79.375% \n",
      "[epoch:2, iter:107] Loss: 0.540 | Acc: 78.409% \n",
      "[epoch:2, iter:108] Loss: 0.537 | Acc: 78.125% \n",
      "[epoch:2, iter:109] Loss: 0.539 | Acc: 76.923% \n",
      "[epoch:2, iter:110] Loss: 0.534 | Acc: 77.679% \n",
      "[epoch:2, iter:111] Loss: 0.550 | Acc: 76.667% \n",
      "[epoch:2, iter:112] Loss: 0.542 | Acc: 77.734% \n",
      "[epoch:2, iter:113] Loss: 0.529 | Acc: 79.044% \n",
      "[epoch:2, iter:114] Loss: 0.533 | Acc: 77.778% \n",
      "[epoch:2, iter:115] Loss: 0.522 | Acc: 78.289% \n",
      "[epoch:2, iter:116] Loss: 0.516 | Acc: 78.750% \n",
      "[epoch:2, iter:117] Loss: 0.514 | Acc: 78.869% \n",
      "[epoch:2, iter:118] Loss: 0.520 | Acc: 78.409% \n",
      "[epoch:2, iter:119] Loss: 0.521 | Acc: 78.533% \n",
      "[epoch:2, iter:120] Loss: 0.521 | Acc: 78.125% \n",
      "[epoch:2, iter:121] Loss: 0.523 | Acc: 78.250% \n",
      "[epoch:2, iter:122] Loss: 0.525 | Acc: 78.125% \n",
      "[epoch:2, iter:123] Loss: 0.520 | Acc: 78.241% \n",
      "[epoch:2, iter:124] Loss: 0.516 | Acc: 78.348% \n",
      "[epoch:2, iter:125] Loss: 0.525 | Acc: 77.802% \n",
      "[epoch:2, iter:126] Loss: 0.526 | Acc: 77.500% \n",
      "[epoch:2, iter:127] Loss: 0.525 | Acc: 77.823% \n",
      "[epoch:2, iter:128] Loss: 0.527 | Acc: 77.539% \n",
      "[epoch:2, iter:129] Loss: 0.530 | Acc: 77.462% \n",
      "[epoch:2, iter:130] Loss: 0.528 | Acc: 77.757% \n",
      "[epoch:2, iter:131] Loss: 0.536 | Acc: 77.321% \n",
      "[epoch:2, iter:132] Loss: 0.541 | Acc: 77.083% \n",
      "[epoch:2, iter:133] Loss: 0.547 | Acc: 76.520% \n",
      "[epoch:2, iter:134] Loss: 0.543 | Acc: 76.809% \n",
      "[epoch:2, iter:135] Loss: 0.550 | Acc: 76.603% \n",
      "[epoch:2, iter:136] Loss: 0.548 | Acc: 76.875% \n",
      "[epoch:2, iter:137] Loss: 0.542 | Acc: 77.287% \n",
      "[epoch:2, iter:138] Loss: 0.549 | Acc: 76.637% \n",
      "[epoch:2, iter:139] Loss: 0.546 | Acc: 76.890% \n",
      "[epoch:2, iter:140] Loss: 0.546 | Acc: 76.847% \n",
      "[epoch:2, iter:141] Loss: 0.549 | Acc: 76.667% \n",
      "[epoch:2, iter:142] Loss: 0.550 | Acc: 76.495% \n",
      "[epoch:2, iter:143] Loss: 0.549 | Acc: 76.330% \n",
      "[epoch:2, iter:144] Loss: 0.546 | Acc: 76.693% \n",
      "[epoch:2, iter:145] Loss: 0.542 | Acc: 76.913% \n",
      "[epoch:2, iter:146] Loss: 0.544 | Acc: 76.875% \n",
      "[epoch:2, iter:147] Loss: 0.548 | Acc: 76.716% \n",
      "[epoch:2, iter:148] Loss: 0.545 | Acc: 76.923% \n",
      "[epoch:2, iter:149] Loss: 0.545 | Acc: 76.769% \n",
      "[epoch:2, iter:150] Loss: 0.545 | Acc: 76.852% \n",
      "[epoch:2, iter:151] Loss: 0.548 | Acc: 76.705% \n",
      "[epoch:2, iter:152] Loss: 0.549 | Acc: 76.451% \n",
      "[epoch:2, iter:153] Loss: 0.549 | Acc: 76.645% \n",
      "[epoch:2, iter:154] Loss: 0.548 | Acc: 76.724% \n",
      "[epoch:2, iter:155] Loss: 0.551 | Acc: 76.377% \n",
      "[epoch:2, iter:156] Loss: 0.549 | Acc: 76.562% \n",
      "[epoch:2, iter:157] Loss: 0.548 | Acc: 76.537% \n",
      "[epoch:2, iter:158] Loss: 0.549 | Acc: 76.411% \n",
      "[epoch:2, iter:159] Loss: 0.551 | Acc: 76.290% \n",
      "[epoch:2, iter:160] Loss: 0.553 | Acc: 75.977% \n",
      "[epoch:2, iter:161] Loss: 0.552 | Acc: 76.154% \n",
      "[epoch:2, iter:162] Loss: 0.551 | Acc: 76.042% \n",
      "[epoch:2, iter:163] Loss: 0.551 | Acc: 76.119% \n",
      "[epoch:2, iter:164] Loss: 0.549 | Acc: 76.287% \n",
      "[epoch:2, iter:165] Loss: 0.553 | Acc: 76.087% \n",
      "[epoch:2, iter:166] Loss: 0.557 | Acc: 75.446% \n",
      "[epoch:2, iter:167] Loss: 0.555 | Acc: 75.528% \n",
      "[epoch:2, iter:168] Loss: 0.557 | Acc: 75.434% \n",
      "[epoch:2, iter:169] Loss: 0.557 | Acc: 75.342% \n",
      "[epoch:2, iter:170] Loss: 0.559 | Acc: 75.169% \n",
      "[epoch:2, iter:171] Loss: 0.559 | Acc: 75.083% \n",
      "[epoch:2, iter:172] Loss: 0.557 | Acc: 75.164% \n",
      "[epoch:2, iter:173] Loss: 0.557 | Acc: 75.162% \n",
      "[epoch:2, iter:174] Loss: 0.557 | Acc: 74.920% \n",
      "[epoch:2, iter:175] Loss: 0.557 | Acc: 74.921% \n",
      "[epoch:2, iter:176] Loss: 0.556 | Acc: 75.000% \n",
      "[epoch:2, iter:177] Loss: 0.558 | Acc: 75.000% \n",
      "[epoch:2, iter:178] Loss: 0.558 | Acc: 75.000% \n",
      "[epoch:2, iter:179] Loss: 0.560 | Acc: 74.849% \n",
      "[epoch:2, iter:180] Loss: 0.562 | Acc: 74.554% \n",
      "[epoch:2, iter:181] Loss: 0.561 | Acc: 74.559% \n",
      "[epoch:2, iter:182] Loss: 0.560 | Acc: 74.564% \n",
      "[epoch:2, iter:183] Loss: 0.560 | Acc: 74.497% \n",
      "[epoch:2, iter:184] Loss: 0.562 | Acc: 74.290% \n",
      "[epoch:2, iter:185] Loss: 0.560 | Acc: 74.368% \n",
      "[epoch:2, iter:186] Loss: 0.559 | Acc: 74.375% \n",
      "[epoch:2, iter:187] Loss: 0.562 | Acc: 74.038% \n",
      "[epoch:2, iter:188] Loss: 0.561 | Acc: 74.117% \n",
      "[epoch:2, iter:189] Loss: 0.560 | Acc: 74.126% \n",
      "[epoch:2, iter:190] Loss: 0.561 | Acc: 74.069% \n",
      "[epoch:2, iter:191] Loss: 0.562 | Acc: 74.013% \n",
      "[epoch:2, iter:192] Loss: 0.564 | Acc: 73.839% \n",
      "Waiting Test!\n",
      "测试分类准确率为：68.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 3\n",
      "[epoch:3, iter:193] Loss: 0.572 | Acc: 68.750% \n",
      "[epoch:3, iter:194] Loss: 0.575 | Acc: 68.750% \n",
      "[epoch:3, iter:195] Loss: 0.532 | Acc: 75.000% \n",
      "[epoch:3, iter:196] Loss: 0.517 | Acc: 75.000% \n",
      "[epoch:3, iter:197] Loss: 0.508 | Acc: 76.250% \n",
      "[epoch:3, iter:198] Loss: 0.504 | Acc: 76.042% \n",
      "[epoch:3, iter:199] Loss: 0.496 | Acc: 75.893% \n",
      "[epoch:3, iter:200] Loss: 0.482 | Acc: 78.125% \n",
      "[epoch:3, iter:201] Loss: 0.466 | Acc: 79.861% \n",
      "[epoch:3, iter:202] Loss: 0.456 | Acc: 79.375% \n",
      "[epoch:3, iter:203] Loss: 0.455 | Acc: 78.977% \n",
      "[epoch:3, iter:204] Loss: 0.451 | Acc: 79.167% \n",
      "[epoch:3, iter:205] Loss: 0.441 | Acc: 80.288% \n",
      "[epoch:3, iter:206] Loss: 0.424 | Acc: 81.696% \n",
      "[epoch:3, iter:207] Loss: 0.422 | Acc: 81.667% \n",
      "[epoch:3, iter:208] Loss: 0.414 | Acc: 82.031% \n",
      "[epoch:3, iter:209] Loss: 0.407 | Acc: 82.353% \n",
      "[epoch:3, iter:210] Loss: 0.399 | Acc: 82.639% \n",
      "[epoch:3, iter:211] Loss: 0.393 | Acc: 82.895% \n",
      "[epoch:3, iter:212] Loss: 0.397 | Acc: 81.875% \n",
      "[epoch:3, iter:213] Loss: 0.392 | Acc: 82.440% \n",
      "[epoch:3, iter:214] Loss: 0.381 | Acc: 83.239% \n",
      "[epoch:3, iter:215] Loss: 0.374 | Acc: 83.696% \n",
      "[epoch:3, iter:216] Loss: 0.368 | Acc: 83.854% \n",
      "[epoch:3, iter:217] Loss: 0.361 | Acc: 84.250% \n",
      "[epoch:3, iter:218] Loss: 0.366 | Acc: 83.654% \n",
      "[epoch:3, iter:219] Loss: 0.368 | Acc: 83.565% \n",
      "[epoch:3, iter:220] Loss: 0.361 | Acc: 84.152% \n",
      "[epoch:3, iter:221] Loss: 0.357 | Acc: 84.698% \n",
      "[epoch:3, iter:222] Loss: 0.362 | Acc: 84.375% \n",
      "[epoch:3, iter:223] Loss: 0.359 | Acc: 84.677% \n",
      "[epoch:3, iter:224] Loss: 0.354 | Acc: 84.961% \n",
      "[epoch:3, iter:225] Loss: 0.350 | Acc: 85.038% \n",
      "[epoch:3, iter:226] Loss: 0.348 | Acc: 85.110% \n",
      "[epoch:3, iter:227] Loss: 0.352 | Acc: 85.000% \n",
      "[epoch:3, iter:228] Loss: 0.351 | Acc: 85.243% \n",
      "[epoch:3, iter:229] Loss: 0.349 | Acc: 85.135% \n",
      "[epoch:3, iter:230] Loss: 0.351 | Acc: 84.704% \n",
      "[epoch:3, iter:231] Loss: 0.347 | Acc: 84.936% \n",
      "[epoch:3, iter:232] Loss: 0.349 | Acc: 84.844% \n",
      "[epoch:3, iter:233] Loss: 0.349 | Acc: 84.909% \n",
      "[epoch:3, iter:234] Loss: 0.354 | Acc: 84.673% \n",
      "[epoch:3, iter:235] Loss: 0.352 | Acc: 84.738% \n",
      "[epoch:3, iter:236] Loss: 0.351 | Acc: 84.659% \n",
      "[epoch:3, iter:237] Loss: 0.353 | Acc: 84.444% \n",
      "[epoch:3, iter:238] Loss: 0.350 | Acc: 84.783% \n",
      "[epoch:3, iter:239] Loss: 0.353 | Acc: 84.574% \n",
      "[epoch:3, iter:240] Loss: 0.353 | Acc: 84.505% \n",
      "[epoch:3, iter:241] Loss: 0.355 | Acc: 84.439% \n",
      "[epoch:3, iter:242] Loss: 0.350 | Acc: 84.750% \n",
      "[epoch:3, iter:243] Loss: 0.348 | Acc: 84.926% \n",
      "[epoch:3, iter:244] Loss: 0.347 | Acc: 85.096% \n",
      "[epoch:3, iter:245] Loss: 0.343 | Acc: 85.259% \n",
      "[epoch:3, iter:246] Loss: 0.345 | Acc: 85.417% \n",
      "[epoch:3, iter:247] Loss: 0.345 | Acc: 85.455% \n",
      "[epoch:3, iter:248] Loss: 0.345 | Acc: 85.491% \n",
      "[epoch:3, iter:249] Loss: 0.349 | Acc: 85.307% \n",
      "[epoch:3, iter:250] Loss: 0.347 | Acc: 85.345% \n",
      "[epoch:3, iter:251] Loss: 0.349 | Acc: 85.169% \n",
      "[epoch:3, iter:252] Loss: 0.349 | Acc: 85.208% \n",
      "[epoch:3, iter:253] Loss: 0.352 | Acc: 84.836% \n",
      "[epoch:3, iter:254] Loss: 0.350 | Acc: 84.980% \n",
      "[epoch:3, iter:255] Loss: 0.348 | Acc: 85.218% \n",
      "[epoch:3, iter:256] Loss: 0.352 | Acc: 85.156% \n",
      "[epoch:3, iter:257] Loss: 0.351 | Acc: 85.288% \n",
      "[epoch:3, iter:258] Loss: 0.354 | Acc: 85.227% \n",
      "[epoch:3, iter:259] Loss: 0.360 | Acc: 84.888% \n",
      "[epoch:3, iter:260] Loss: 0.364 | Acc: 84.835% \n",
      "[epoch:3, iter:261] Loss: 0.361 | Acc: 85.054% \n",
      "[epoch:3, iter:262] Loss: 0.359 | Acc: 85.089% \n",
      "[epoch:3, iter:263] Loss: 0.366 | Acc: 84.771% \n",
      "[epoch:3, iter:264] Loss: 0.364 | Acc: 84.809% \n",
      "[epoch:3, iter:265] Loss: 0.365 | Acc: 84.760% \n",
      "[epoch:3, iter:266] Loss: 0.364 | Acc: 84.713% \n",
      "[epoch:3, iter:267] Loss: 0.367 | Acc: 84.750% \n",
      "[epoch:3, iter:268] Loss: 0.373 | Acc: 84.622% \n",
      "[epoch:3, iter:269] Loss: 0.379 | Acc: 84.253% \n",
      "[epoch:3, iter:270] Loss: 0.380 | Acc: 84.215% \n",
      "[epoch:3, iter:271] Loss: 0.385 | Acc: 84.019% \n",
      "[epoch:3, iter:272] Loss: 0.386 | Acc: 83.906% \n",
      "[epoch:3, iter:273] Loss: 0.387 | Acc: 83.642% \n",
      "[epoch:3, iter:274] Loss: 0.392 | Acc: 83.537% \n",
      "[epoch:3, iter:275] Loss: 0.394 | Acc: 83.358% \n",
      "[epoch:3, iter:276] Loss: 0.398 | Acc: 83.259% \n",
      "[epoch:3, iter:277] Loss: 0.397 | Acc: 83.309% \n",
      "[epoch:3, iter:278] Loss: 0.401 | Acc: 83.067% \n",
      "[epoch:3, iter:279] Loss: 0.405 | Acc: 82.902% \n",
      "[epoch:3, iter:280] Loss: 0.403 | Acc: 82.955% \n",
      "[epoch:3, iter:281] Loss: 0.406 | Acc: 82.795% \n",
      "[epoch:3, iter:282] Loss: 0.407 | Acc: 82.778% \n",
      "[epoch:3, iter:283] Loss: 0.408 | Acc: 82.761% \n",
      "[epoch:3, iter:284] Loss: 0.407 | Acc: 82.880% \n",
      "[epoch:3, iter:285] Loss: 0.406 | Acc: 82.997% \n",
      "[epoch:3, iter:286] Loss: 0.406 | Acc: 83.045% \n",
      "[epoch:3, iter:287] Loss: 0.406 | Acc: 83.092% \n",
      "[epoch:3, iter:288] Loss: 0.404 | Acc: 83.126% \n",
      "Waiting Test!\n",
      "测试分类准确率为：71.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 4\n",
      "[epoch:4, iter:289] Loss: 0.248 | Acc: 100.000% \n",
      "[epoch:4, iter:290] Loss: 0.266 | Acc: 93.750% \n",
      "[epoch:4, iter:291] Loss: 0.307 | Acc: 89.583% \n",
      "[epoch:4, iter:292] Loss: 0.345 | Acc: 89.062% \n",
      "[epoch:4, iter:293] Loss: 0.330 | Acc: 90.000% \n",
      "[epoch:4, iter:294] Loss: 0.365 | Acc: 86.458% \n",
      "[epoch:4, iter:295] Loss: 0.360 | Acc: 86.607% \n",
      "[epoch:4, iter:296] Loss: 0.346 | Acc: 86.719% \n",
      "[epoch:4, iter:297] Loss: 0.337 | Acc: 87.500% \n",
      "[epoch:4, iter:298] Loss: 0.351 | Acc: 87.500% \n",
      "[epoch:4, iter:299] Loss: 0.345 | Acc: 87.500% \n",
      "[epoch:4, iter:300] Loss: 0.346 | Acc: 86.979% \n",
      "[epoch:4, iter:301] Loss: 0.347 | Acc: 87.500% \n",
      "[epoch:4, iter:302] Loss: 0.342 | Acc: 87.946% \n",
      "[epoch:4, iter:303] Loss: 0.340 | Acc: 87.917% \n",
      "[epoch:4, iter:304] Loss: 0.327 | Acc: 88.672% \n",
      "[epoch:4, iter:305] Loss: 0.316 | Acc: 89.338% \n",
      "[epoch:4, iter:306] Loss: 0.328 | Acc: 88.194% \n",
      "[epoch:4, iter:307] Loss: 0.330 | Acc: 87.500% \n",
      "[epoch:4, iter:308] Loss: 0.322 | Acc: 87.812% \n",
      "[epoch:4, iter:309] Loss: 0.329 | Acc: 87.798% \n",
      "[epoch:4, iter:310] Loss: 0.329 | Acc: 88.068% \n",
      "[epoch:4, iter:311] Loss: 0.338 | Acc: 87.772% \n",
      "[epoch:4, iter:312] Loss: 0.332 | Acc: 88.021% \n",
      "[epoch:4, iter:313] Loss: 0.328 | Acc: 88.000% \n",
      "[epoch:4, iter:314] Loss: 0.326 | Acc: 87.981% \n",
      "[epoch:4, iter:315] Loss: 0.322 | Acc: 88.194% \n",
      "[epoch:4, iter:316] Loss: 0.326 | Acc: 88.170% \n",
      "[epoch:4, iter:317] Loss: 0.321 | Acc: 88.362% \n",
      "[epoch:4, iter:318] Loss: 0.315 | Acc: 88.750% \n",
      "[epoch:4, iter:319] Loss: 0.310 | Acc: 89.113% \n",
      "[epoch:4, iter:320] Loss: 0.306 | Acc: 89.062% \n",
      "[epoch:4, iter:321] Loss: 0.302 | Acc: 89.394% \n",
      "[epoch:4, iter:322] Loss: 0.299 | Acc: 89.522% \n",
      "[epoch:4, iter:323] Loss: 0.312 | Acc: 89.286% \n",
      "[epoch:4, iter:324] Loss: 0.315 | Acc: 89.062% \n",
      "[epoch:4, iter:325] Loss: 0.326 | Acc: 88.851% \n",
      "[epoch:4, iter:326] Loss: 0.328 | Acc: 88.816% \n",
      "[epoch:4, iter:327] Loss: 0.336 | Acc: 88.622% \n",
      "[epoch:4, iter:328] Loss: 0.332 | Acc: 88.750% \n",
      "[epoch:4, iter:329] Loss: 0.328 | Acc: 88.872% \n",
      "[epoch:4, iter:330] Loss: 0.326 | Acc: 88.988% \n",
      "[epoch:4, iter:331] Loss: 0.321 | Acc: 89.099% \n",
      "[epoch:4, iter:332] Loss: 0.316 | Acc: 89.347% \n",
      "[epoch:4, iter:333] Loss: 0.315 | Acc: 89.306% \n",
      "[epoch:4, iter:334] Loss: 0.314 | Acc: 89.130% \n",
      "[epoch:4, iter:335] Loss: 0.314 | Acc: 89.096% \n",
      "[epoch:4, iter:336] Loss: 0.312 | Acc: 89.193% \n",
      "[epoch:4, iter:337] Loss: 0.310 | Acc: 89.286% \n",
      "[epoch:4, iter:338] Loss: 0.308 | Acc: 89.375% \n",
      "[epoch:4, iter:339] Loss: 0.309 | Acc: 89.216% \n",
      "[epoch:4, iter:340] Loss: 0.307 | Acc: 89.423% \n",
      "[epoch:4, iter:341] Loss: 0.304 | Acc: 89.505% \n",
      "[epoch:4, iter:342] Loss: 0.308 | Acc: 89.352% \n",
      "[epoch:4, iter:343] Loss: 0.308 | Acc: 89.432% \n",
      "[epoch:4, iter:344] Loss: 0.307 | Acc: 89.509% \n",
      "[epoch:4, iter:345] Loss: 0.303 | Acc: 89.693% \n",
      "[epoch:4, iter:346] Loss: 0.303 | Acc: 89.763% \n",
      "[epoch:4, iter:347] Loss: 0.312 | Acc: 89.513% \n",
      "[epoch:4, iter:348] Loss: 0.310 | Acc: 89.583% \n",
      "[epoch:4, iter:349] Loss: 0.307 | Acc: 89.652% \n",
      "[epoch:4, iter:350] Loss: 0.309 | Acc: 89.516% \n",
      "[epoch:4, iter:351] Loss: 0.307 | Acc: 89.583% \n",
      "[epoch:4, iter:352] Loss: 0.308 | Acc: 89.355% \n",
      "[epoch:4, iter:353] Loss: 0.307 | Acc: 89.231% \n",
      "[epoch:4, iter:354] Loss: 0.305 | Acc: 89.394% \n",
      "[epoch:4, iter:355] Loss: 0.302 | Acc: 89.552% \n",
      "[epoch:4, iter:356] Loss: 0.300 | Acc: 89.706% \n",
      "[epoch:4, iter:357] Loss: 0.300 | Acc: 89.583% \n",
      "[epoch:4, iter:358] Loss: 0.299 | Acc: 89.554% \n",
      "[epoch:4, iter:359] Loss: 0.295 | Acc: 89.701% \n",
      "[epoch:4, iter:360] Loss: 0.295 | Acc: 89.757% \n",
      "[epoch:4, iter:361] Loss: 0.294 | Acc: 89.726% \n",
      "[epoch:4, iter:362] Loss: 0.293 | Acc: 89.696% \n",
      "[epoch:4, iter:363] Loss: 0.295 | Acc: 89.667% \n",
      "[epoch:4, iter:364] Loss: 0.294 | Acc: 89.556% \n",
      "[epoch:4, iter:365] Loss: 0.292 | Acc: 89.610% \n",
      "[epoch:4, iter:366] Loss: 0.296 | Acc: 89.503% \n",
      "[epoch:4, iter:367] Loss: 0.296 | Acc: 89.478% \n",
      "[epoch:4, iter:368] Loss: 0.297 | Acc: 89.531% \n",
      "[epoch:4, iter:369] Loss: 0.295 | Acc: 89.583% \n",
      "[epoch:4, iter:370] Loss: 0.293 | Acc: 89.710% \n",
      "[epoch:4, iter:371] Loss: 0.292 | Acc: 89.759% \n",
      "[epoch:4, iter:372] Loss: 0.299 | Acc: 89.509% \n",
      "[epoch:4, iter:373] Loss: 0.297 | Acc: 89.632% \n",
      "[epoch:4, iter:374] Loss: 0.296 | Acc: 89.680% \n",
      "[epoch:4, iter:375] Loss: 0.295 | Acc: 89.727% \n",
      "[epoch:4, iter:376] Loss: 0.298 | Acc: 89.631% \n",
      "[epoch:4, iter:377] Loss: 0.300 | Acc: 89.537% \n",
      "[epoch:4, iter:378] Loss: 0.302 | Acc: 89.514% \n",
      "[epoch:4, iter:379] Loss: 0.299 | Acc: 89.629% \n",
      "[epoch:4, iter:380] Loss: 0.299 | Acc: 89.606% \n",
      "[epoch:4, iter:381] Loss: 0.298 | Acc: 89.651% \n",
      "[epoch:4, iter:382] Loss: 0.296 | Acc: 89.694% \n",
      "[epoch:4, iter:383] Loss: 0.297 | Acc: 89.671% \n",
      "[epoch:4, iter:384] Loss: 0.305 | Acc: 89.470% \n",
      "Waiting Test!\n",
      "测试分类准确率为：66.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 5\n",
      "[epoch:5, iter:385] Loss: 0.122 | Acc: 100.000% \n",
      "[epoch:5, iter:386] Loss: 0.117 | Acc: 100.000% \n",
      "[epoch:5, iter:387] Loss: 0.260 | Acc: 97.917% \n",
      "[epoch:5, iter:388] Loss: 0.241 | Acc: 95.312% \n",
      "[epoch:5, iter:389] Loss: 0.209 | Acc: 96.250% \n",
      "[epoch:5, iter:390] Loss: 0.215 | Acc: 95.833% \n",
      "[epoch:5, iter:391] Loss: 0.206 | Acc: 96.429% \n",
      "[epoch:5, iter:392] Loss: 0.209 | Acc: 96.094% \n",
      "[epoch:5, iter:393] Loss: 0.196 | Acc: 96.528% \n",
      "[epoch:5, iter:394] Loss: 0.191 | Acc: 96.875% \n",
      "[epoch:5, iter:395] Loss: 0.190 | Acc: 96.591% \n",
      "[epoch:5, iter:396] Loss: 0.188 | Acc: 96.354% \n",
      "[epoch:5, iter:397] Loss: 0.179 | Acc: 96.635% \n",
      "[epoch:5, iter:398] Loss: 0.175 | Acc: 96.875% \n",
      "[epoch:5, iter:399] Loss: 0.199 | Acc: 95.417% \n",
      "[epoch:5, iter:400] Loss: 0.203 | Acc: 94.922% \n",
      "[epoch:5, iter:401] Loss: 0.205 | Acc: 94.485% \n",
      "[epoch:5, iter:402] Loss: 0.202 | Acc: 94.444% \n",
      "[epoch:5, iter:403] Loss: 0.195 | Acc: 94.737% \n",
      "[epoch:5, iter:404] Loss: 0.194 | Acc: 94.688% \n",
      "[epoch:5, iter:405] Loss: 0.194 | Acc: 94.643% \n",
      "[epoch:5, iter:406] Loss: 0.194 | Acc: 94.602% \n",
      "[epoch:5, iter:407] Loss: 0.191 | Acc: 94.565% \n",
      "[epoch:5, iter:408] Loss: 0.199 | Acc: 94.271% \n",
      "[epoch:5, iter:409] Loss: 0.211 | Acc: 93.750% \n",
      "[epoch:5, iter:410] Loss: 0.207 | Acc: 93.750% \n",
      "[epoch:5, iter:411] Loss: 0.208 | Acc: 93.519% \n",
      "[epoch:5, iter:412] Loss: 0.205 | Acc: 93.750% \n",
      "[epoch:5, iter:413] Loss: 0.211 | Acc: 93.750% \n",
      "[epoch:5, iter:414] Loss: 0.209 | Acc: 93.750% \n",
      "[epoch:5, iter:415] Loss: 0.205 | Acc: 93.750% \n",
      "[epoch:5, iter:416] Loss: 0.211 | Acc: 93.750% \n",
      "[epoch:5, iter:417] Loss: 0.220 | Acc: 93.371% \n",
      "[epoch:5, iter:418] Loss: 0.220 | Acc: 93.382% \n",
      "[epoch:5, iter:419] Loss: 0.220 | Acc: 93.393% \n",
      "[epoch:5, iter:420] Loss: 0.218 | Acc: 93.403% \n",
      "[epoch:5, iter:421] Loss: 0.230 | Acc: 93.243% \n",
      "[epoch:5, iter:422] Loss: 0.227 | Acc: 93.257% \n",
      "[epoch:5, iter:423] Loss: 0.224 | Acc: 93.269% \n",
      "[epoch:5, iter:424] Loss: 0.223 | Acc: 93.125% \n",
      "[epoch:5, iter:425] Loss: 0.231 | Acc: 92.683% \n",
      "[epoch:5, iter:426] Loss: 0.227 | Acc: 92.857% \n",
      "[epoch:5, iter:427] Loss: 0.224 | Acc: 93.023% \n",
      "[epoch:5, iter:428] Loss: 0.225 | Acc: 93.040% \n",
      "[epoch:5, iter:429] Loss: 0.223 | Acc: 93.056% \n",
      "[epoch:5, iter:430] Loss: 0.230 | Acc: 92.935% \n",
      "[epoch:5, iter:431] Loss: 0.227 | Acc: 93.085% \n",
      "[epoch:5, iter:432] Loss: 0.229 | Acc: 92.839% \n",
      "[epoch:5, iter:433] Loss: 0.232 | Acc: 92.730% \n",
      "[epoch:5, iter:434] Loss: 0.230 | Acc: 92.750% \n",
      "[epoch:5, iter:435] Loss: 0.228 | Acc: 92.770% \n",
      "[epoch:5, iter:436] Loss: 0.227 | Acc: 92.788% \n",
      "[epoch:5, iter:437] Loss: 0.229 | Acc: 92.807% \n",
      "[epoch:5, iter:438] Loss: 0.228 | Acc: 92.824% \n",
      "[epoch:5, iter:439] Loss: 0.226 | Acc: 92.841% \n",
      "[epoch:5, iter:440] Loss: 0.228 | Acc: 92.634% \n",
      "[epoch:5, iter:441] Loss: 0.226 | Acc: 92.654% \n",
      "[epoch:5, iter:442] Loss: 0.232 | Acc: 92.349% \n",
      "[epoch:5, iter:443] Loss: 0.233 | Acc: 92.267% \n",
      "[epoch:5, iter:444] Loss: 0.233 | Acc: 92.083% \n",
      "[epoch:5, iter:445] Loss: 0.233 | Acc: 92.213% \n",
      "[epoch:5, iter:446] Loss: 0.236 | Acc: 91.935% \n",
      "[epoch:5, iter:447] Loss: 0.238 | Acc: 91.964% \n",
      "[epoch:5, iter:448] Loss: 0.237 | Acc: 91.992% \n",
      "[epoch:5, iter:449] Loss: 0.235 | Acc: 92.115% \n",
      "[epoch:5, iter:450] Loss: 0.236 | Acc: 92.045% \n",
      "[epoch:5, iter:451] Loss: 0.235 | Acc: 91.978% \n",
      "[epoch:5, iter:452] Loss: 0.233 | Acc: 92.096% \n",
      "[epoch:5, iter:453] Loss: 0.233 | Acc: 92.029% \n",
      "[epoch:5, iter:454] Loss: 0.233 | Acc: 91.964% \n",
      "[epoch:5, iter:455] Loss: 0.235 | Acc: 91.901% \n",
      "[epoch:5, iter:456] Loss: 0.234 | Acc: 92.014% \n",
      "[epoch:5, iter:457] Loss: 0.235 | Acc: 91.952% \n",
      "[epoch:5, iter:458] Loss: 0.234 | Acc: 91.976% \n",
      "[epoch:5, iter:459] Loss: 0.237 | Acc: 91.917% \n",
      "[epoch:5, iter:460] Loss: 0.240 | Acc: 91.941% \n",
      "[epoch:5, iter:461] Loss: 0.242 | Acc: 91.883% \n",
      "[epoch:5, iter:462] Loss: 0.241 | Acc: 91.907% \n",
      "[epoch:5, iter:463] Loss: 0.240 | Acc: 91.930% \n",
      "[epoch:5, iter:464] Loss: 0.240 | Acc: 91.875% \n",
      "[epoch:5, iter:465] Loss: 0.239 | Acc: 91.975% \n",
      "[epoch:5, iter:466] Loss: 0.240 | Acc: 91.921% \n",
      "[epoch:5, iter:467] Loss: 0.242 | Acc: 91.717% \n",
      "[epoch:5, iter:468] Loss: 0.241 | Acc: 91.741% \n",
      "[epoch:5, iter:469] Loss: 0.240 | Acc: 91.765% \n",
      "[epoch:5, iter:470] Loss: 0.239 | Acc: 91.715% \n",
      "[epoch:5, iter:471] Loss: 0.238 | Acc: 91.810% \n",
      "[epoch:5, iter:472] Loss: 0.241 | Acc: 91.690% \n",
      "[epoch:5, iter:473] Loss: 0.246 | Acc: 91.573% \n",
      "[epoch:5, iter:474] Loss: 0.244 | Acc: 91.667% \n",
      "[epoch:5, iter:475] Loss: 0.243 | Acc: 91.621% \n",
      "[epoch:5, iter:476] Loss: 0.245 | Acc: 91.644% \n",
      "[epoch:5, iter:477] Loss: 0.246 | Acc: 91.599% \n",
      "[epoch:5, iter:478] Loss: 0.245 | Acc: 91.622% \n",
      "[epoch:5, iter:479] Loss: 0.251 | Acc: 91.447% \n",
      "[epoch:5, iter:480] Loss: 0.251 | Acc: 91.498% \n",
      "Waiting Test!\n",
      "测试分类准确率为：65.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 6\n",
      "[epoch:6, iter:481] Loss: 0.180 | Acc: 100.000% \n",
      "[epoch:6, iter:482] Loss: 0.178 | Acc: 96.875% \n",
      "[epoch:6, iter:483] Loss: 0.152 | Acc: 95.833% \n",
      "[epoch:6, iter:484] Loss: 0.142 | Acc: 95.312% \n",
      "[epoch:6, iter:485] Loss: 0.124 | Acc: 96.250% \n",
      "[epoch:6, iter:486] Loss: 0.115 | Acc: 96.875% \n",
      "[epoch:6, iter:487] Loss: 0.123 | Acc: 96.429% \n",
      "[epoch:6, iter:488] Loss: 0.128 | Acc: 96.094% \n",
      "[epoch:6, iter:489] Loss: 0.118 | Acc: 96.528% \n",
      "[epoch:6, iter:490] Loss: 0.109 | Acc: 96.875% \n",
      "[epoch:6, iter:491] Loss: 0.118 | Acc: 96.023% \n",
      "[epoch:6, iter:492] Loss: 0.129 | Acc: 95.833% \n",
      "[epoch:6, iter:493] Loss: 0.132 | Acc: 95.673% \n",
      "[epoch:6, iter:494] Loss: 0.147 | Acc: 95.536% \n",
      "[epoch:6, iter:495] Loss: 0.143 | Acc: 95.833% \n",
      "[epoch:6, iter:496] Loss: 0.145 | Acc: 96.094% \n",
      "[epoch:6, iter:497] Loss: 0.149 | Acc: 95.956% \n",
      "[epoch:6, iter:498] Loss: 0.151 | Acc: 95.833% \n",
      "[epoch:6, iter:499] Loss: 0.149 | Acc: 95.724% \n",
      "[epoch:6, iter:500] Loss: 0.162 | Acc: 95.625% \n",
      "[epoch:6, iter:501] Loss: 0.159 | Acc: 95.833% \n",
      "[epoch:6, iter:502] Loss: 0.176 | Acc: 95.170% \n",
      "[epoch:6, iter:503] Loss: 0.169 | Acc: 95.380% \n",
      "[epoch:6, iter:504] Loss: 0.164 | Acc: 95.573% \n",
      "[epoch:6, iter:505] Loss: 0.162 | Acc: 95.500% \n",
      "[epoch:6, iter:506] Loss: 0.159 | Acc: 95.673% \n",
      "[epoch:6, iter:507] Loss: 0.159 | Acc: 95.833% \n",
      "[epoch:6, iter:508] Loss: 0.159 | Acc: 95.759% \n",
      "[epoch:6, iter:509] Loss: 0.157 | Acc: 95.690% \n",
      "[epoch:6, iter:510] Loss: 0.170 | Acc: 95.208% \n",
      "[epoch:6, iter:511] Loss: 0.169 | Acc: 95.161% \n",
      "[epoch:6, iter:512] Loss: 0.174 | Acc: 95.117% \n",
      "[epoch:6, iter:513] Loss: 0.171 | Acc: 95.265% \n",
      "[epoch:6, iter:514] Loss: 0.169 | Acc: 95.221% \n",
      "[epoch:6, iter:515] Loss: 0.165 | Acc: 95.357% \n",
      "[epoch:6, iter:516] Loss: 0.161 | Acc: 95.486% \n",
      "[epoch:6, iter:517] Loss: 0.160 | Acc: 95.439% \n",
      "[epoch:6, iter:518] Loss: 0.159 | Acc: 95.559% \n",
      "[epoch:6, iter:519] Loss: 0.161 | Acc: 95.353% \n",
      "[epoch:6, iter:520] Loss: 0.164 | Acc: 95.312% \n",
      "[epoch:6, iter:521] Loss: 0.165 | Acc: 95.122% \n",
      "[epoch:6, iter:522] Loss: 0.167 | Acc: 95.089% \n",
      "[epoch:6, iter:523] Loss: 0.166 | Acc: 95.203% \n",
      "[epoch:6, iter:524] Loss: 0.172 | Acc: 95.028% \n",
      "[epoch:6, iter:525] Loss: 0.170 | Acc: 95.139% \n",
      "[epoch:6, iter:526] Loss: 0.169 | Acc: 95.245% \n",
      "[epoch:6, iter:527] Loss: 0.169 | Acc: 95.213% \n",
      "[epoch:6, iter:528] Loss: 0.168 | Acc: 95.182% \n",
      "[epoch:6, iter:529] Loss: 0.166 | Acc: 95.281% \n",
      "[epoch:6, iter:530] Loss: 0.166 | Acc: 95.250% \n",
      "[epoch:6, iter:531] Loss: 0.164 | Acc: 95.343% \n",
      "[epoch:6, iter:532] Loss: 0.165 | Acc: 95.312% \n",
      "[epoch:6, iter:533] Loss: 0.164 | Acc: 95.283% \n",
      "[epoch:6, iter:534] Loss: 0.163 | Acc: 95.255% \n",
      "[epoch:6, iter:535] Loss: 0.162 | Acc: 95.341% \n",
      "[epoch:6, iter:536] Loss: 0.159 | Acc: 95.424% \n",
      "[epoch:6, iter:537] Loss: 0.166 | Acc: 95.285% \n",
      "[epoch:6, iter:538] Loss: 0.169 | Acc: 95.151% \n",
      "[epoch:6, iter:539] Loss: 0.166 | Acc: 95.233% \n",
      "[epoch:6, iter:540] Loss: 0.170 | Acc: 95.104% \n",
      "[epoch:6, iter:541] Loss: 0.167 | Acc: 95.184% \n",
      "[epoch:6, iter:542] Loss: 0.167 | Acc: 95.060% \n",
      "[epoch:6, iter:543] Loss: 0.165 | Acc: 95.139% \n",
      "[epoch:6, iter:544] Loss: 0.164 | Acc: 95.215% \n",
      "[epoch:6, iter:545] Loss: 0.165 | Acc: 95.192% \n",
      "[epoch:6, iter:546] Loss: 0.164 | Acc: 95.265% \n",
      "[epoch:6, iter:547] Loss: 0.163 | Acc: 95.336% \n",
      "[epoch:6, iter:548] Loss: 0.161 | Acc: 95.404% \n",
      "[epoch:6, iter:549] Loss: 0.160 | Acc: 95.471% \n",
      "[epoch:6, iter:550] Loss: 0.160 | Acc: 95.357% \n",
      "[epoch:6, iter:551] Loss: 0.161 | Acc: 95.158% \n",
      "[epoch:6, iter:552] Loss: 0.160 | Acc: 95.226% \n",
      "[epoch:6, iter:553] Loss: 0.163 | Acc: 95.120% \n",
      "[epoch:6, iter:554] Loss: 0.162 | Acc: 95.186% \n",
      "[epoch:6, iter:555] Loss: 0.161 | Acc: 95.250% \n",
      "[epoch:6, iter:556] Loss: 0.162 | Acc: 95.230% \n",
      "[epoch:6, iter:557] Loss: 0.168 | Acc: 95.049% \n",
      "[epoch:6, iter:558] Loss: 0.170 | Acc: 94.872% \n",
      "[epoch:6, iter:559] Loss: 0.172 | Acc: 94.858% \n",
      "[epoch:6, iter:560] Loss: 0.175 | Acc: 94.766% \n",
      "[epoch:6, iter:561] Loss: 0.175 | Acc: 94.676% \n",
      "[epoch:6, iter:562] Loss: 0.178 | Acc: 94.588% \n",
      "[epoch:6, iter:563] Loss: 0.180 | Acc: 94.503% \n",
      "[epoch:6, iter:564] Loss: 0.180 | Acc: 94.494% \n",
      "[epoch:6, iter:565] Loss: 0.180 | Acc: 94.485% \n",
      "[epoch:6, iter:566] Loss: 0.187 | Acc: 94.331% \n",
      "[epoch:6, iter:567] Loss: 0.187 | Acc: 94.325% \n",
      "[epoch:6, iter:568] Loss: 0.186 | Acc: 94.389% \n",
      "[epoch:6, iter:569] Loss: 0.185 | Acc: 94.452% \n",
      "[epoch:6, iter:570] Loss: 0.185 | Acc: 94.444% \n",
      "[epoch:6, iter:571] Loss: 0.188 | Acc: 94.299% \n",
      "[epoch:6, iter:572] Loss: 0.190 | Acc: 94.226% \n",
      "[epoch:6, iter:573] Loss: 0.189 | Acc: 94.288% \n",
      "[epoch:6, iter:574] Loss: 0.194 | Acc: 94.149% \n",
      "[epoch:6, iter:575] Loss: 0.194 | Acc: 94.145% \n",
      "[epoch:6, iter:576] Loss: 0.194 | Acc: 94.179% \n",
      "Waiting Test!\n",
      "测试分类准确率为：71.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 7\n",
      "[epoch:7, iter:577] Loss: 0.074 | Acc: 100.000% \n",
      "[epoch:7, iter:578] Loss: 0.133 | Acc: 96.875% \n",
      "[epoch:7, iter:579] Loss: 0.165 | Acc: 97.917% \n",
      "[epoch:7, iter:580] Loss: 0.141 | Acc: 98.438% \n",
      "[epoch:7, iter:581] Loss: 0.137 | Acc: 98.750% \n",
      "[epoch:7, iter:582] Loss: 0.124 | Acc: 98.958% \n",
      "[epoch:7, iter:583] Loss: 0.115 | Acc: 99.107% \n",
      "[epoch:7, iter:584] Loss: 0.107 | Acc: 99.219% \n",
      "[epoch:7, iter:585] Loss: 0.116 | Acc: 98.611% \n",
      "[epoch:7, iter:586] Loss: 0.111 | Acc: 98.750% \n",
      "[epoch:7, iter:587] Loss: 0.106 | Acc: 98.864% \n",
      "[epoch:7, iter:588] Loss: 0.101 | Acc: 98.958% \n",
      "[epoch:7, iter:589] Loss: 0.099 | Acc: 99.038% \n",
      "[epoch:7, iter:590] Loss: 0.097 | Acc: 99.107% \n",
      "[epoch:7, iter:591] Loss: 0.093 | Acc: 99.167% \n",
      "[epoch:7, iter:592] Loss: 0.090 | Acc: 99.219% \n",
      "[epoch:7, iter:593] Loss: 0.089 | Acc: 99.265% \n",
      "[epoch:7, iter:594] Loss: 0.101 | Acc: 98.958% \n",
      "[epoch:7, iter:595] Loss: 0.099 | Acc: 99.013% \n",
      "[epoch:7, iter:596] Loss: 0.096 | Acc: 99.062% \n",
      "[epoch:7, iter:597] Loss: 0.096 | Acc: 98.810% \n",
      "[epoch:7, iter:598] Loss: 0.099 | Acc: 98.580% \n",
      "[epoch:7, iter:599] Loss: 0.103 | Acc: 98.370% \n",
      "[epoch:7, iter:600] Loss: 0.105 | Acc: 98.177% \n",
      "[epoch:7, iter:601] Loss: 0.101 | Acc: 98.250% \n",
      "[epoch:7, iter:602] Loss: 0.100 | Acc: 98.317% \n",
      "[epoch:7, iter:603] Loss: 0.097 | Acc: 98.380% \n",
      "[epoch:7, iter:604] Loss: 0.102 | Acc: 97.991% \n",
      "[epoch:7, iter:605] Loss: 0.102 | Acc: 97.845% \n",
      "[epoch:7, iter:606] Loss: 0.101 | Acc: 97.917% \n",
      "[epoch:7, iter:607] Loss: 0.099 | Acc: 97.984% \n",
      "[epoch:7, iter:608] Loss: 0.103 | Acc: 97.852% \n",
      "[epoch:7, iter:609] Loss: 0.102 | Acc: 97.727% \n",
      "[epoch:7, iter:610] Loss: 0.101 | Acc: 97.794% \n",
      "[epoch:7, iter:611] Loss: 0.098 | Acc: 97.857% \n",
      "[epoch:7, iter:612] Loss: 0.096 | Acc: 97.917% \n",
      "[epoch:7, iter:613] Loss: 0.105 | Acc: 97.635% \n",
      "[epoch:7, iter:614] Loss: 0.104 | Acc: 97.533% \n",
      "[epoch:7, iter:615] Loss: 0.109 | Acc: 97.276% \n",
      "[epoch:7, iter:616] Loss: 0.107 | Acc: 97.344% \n",
      "[epoch:7, iter:617] Loss: 0.107 | Acc: 97.409% \n",
      "[epoch:7, iter:618] Loss: 0.105 | Acc: 97.470% \n",
      "[epoch:7, iter:619] Loss: 0.105 | Acc: 97.384% \n",
      "[epoch:7, iter:620] Loss: 0.106 | Acc: 97.301% \n",
      "[epoch:7, iter:621] Loss: 0.109 | Acc: 96.944% \n",
      "[epoch:7, iter:622] Loss: 0.114 | Acc: 96.739% \n",
      "[epoch:7, iter:623] Loss: 0.115 | Acc: 96.676% \n",
      "[epoch:7, iter:624] Loss: 0.115 | Acc: 96.615% \n",
      "[epoch:7, iter:625] Loss: 0.113 | Acc: 96.684% \n",
      "[epoch:7, iter:626] Loss: 0.119 | Acc: 96.250% \n",
      "[epoch:7, iter:627] Loss: 0.118 | Acc: 96.324% \n",
      "[epoch:7, iter:628] Loss: 0.124 | Acc: 96.154% \n",
      "[epoch:7, iter:629] Loss: 0.122 | Acc: 96.226% \n",
      "[epoch:7, iter:630] Loss: 0.124 | Acc: 96.065% \n",
      "[epoch:7, iter:631] Loss: 0.125 | Acc: 96.136% \n",
      "[epoch:7, iter:632] Loss: 0.128 | Acc: 95.982% \n",
      "[epoch:7, iter:633] Loss: 0.127 | Acc: 96.053% \n",
      "[epoch:7, iter:634] Loss: 0.127 | Acc: 95.905% \n",
      "[epoch:7, iter:635] Loss: 0.132 | Acc: 95.445% \n",
      "[epoch:7, iter:636] Loss: 0.132 | Acc: 95.417% \n",
      "[epoch:7, iter:637] Loss: 0.131 | Acc: 95.492% \n",
      "[epoch:7, iter:638] Loss: 0.129 | Acc: 95.565% \n",
      "[epoch:7, iter:639] Loss: 0.128 | Acc: 95.635% \n",
      "[epoch:7, iter:640] Loss: 0.127 | Acc: 95.703% \n",
      "[epoch:7, iter:641] Loss: 0.128 | Acc: 95.577% \n",
      "[epoch:7, iter:642] Loss: 0.130 | Acc: 95.549% \n",
      "[epoch:7, iter:643] Loss: 0.130 | Acc: 95.522% \n",
      "[epoch:7, iter:644] Loss: 0.133 | Acc: 95.496% \n",
      "[epoch:7, iter:645] Loss: 0.131 | Acc: 95.562% \n",
      "[epoch:7, iter:646] Loss: 0.136 | Acc: 95.446% \n",
      "[epoch:7, iter:647] Loss: 0.135 | Acc: 95.511% \n",
      "[epoch:7, iter:648] Loss: 0.136 | Acc: 95.486% \n",
      "[epoch:7, iter:649] Loss: 0.139 | Acc: 95.377% \n",
      "[epoch:7, iter:650] Loss: 0.140 | Acc: 95.186% \n",
      "[epoch:7, iter:651] Loss: 0.139 | Acc: 95.250% \n",
      "[epoch:7, iter:652] Loss: 0.138 | Acc: 95.312% \n",
      "[epoch:7, iter:653] Loss: 0.140 | Acc: 95.130% \n",
      "[epoch:7, iter:654] Loss: 0.138 | Acc: 95.192% \n",
      "[epoch:7, iter:655] Loss: 0.141 | Acc: 95.095% \n",
      "[epoch:7, iter:656] Loss: 0.141 | Acc: 95.156% \n",
      "[epoch:7, iter:657] Loss: 0.141 | Acc: 95.139% \n",
      "[epoch:7, iter:658] Loss: 0.140 | Acc: 95.198% \n",
      "[epoch:7, iter:659] Loss: 0.138 | Acc: 95.256% \n",
      "[epoch:7, iter:660] Loss: 0.138 | Acc: 95.238% \n",
      "[epoch:7, iter:661] Loss: 0.137 | Acc: 95.294% \n",
      "[epoch:7, iter:662] Loss: 0.141 | Acc: 95.276% \n",
      "[epoch:7, iter:663] Loss: 0.142 | Acc: 95.187% \n",
      "[epoch:7, iter:664] Loss: 0.142 | Acc: 95.170% \n",
      "[epoch:7, iter:665] Loss: 0.141 | Acc: 95.225% \n",
      "[epoch:7, iter:666] Loss: 0.140 | Acc: 95.278% \n",
      "[epoch:7, iter:667] Loss: 0.139 | Acc: 95.330% \n",
      "[epoch:7, iter:668] Loss: 0.140 | Acc: 95.245% \n",
      "[epoch:7, iter:669] Loss: 0.146 | Acc: 94.960% \n",
      "[epoch:7, iter:670] Loss: 0.145 | Acc: 95.013% \n",
      "[epoch:7, iter:671] Loss: 0.144 | Acc: 95.066% \n",
      "[epoch:7, iter:672] Loss: 0.142 | Acc: 95.095% \n",
      "Waiting Test!\n",
      "测试分类准确率为：77.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 8\n",
      "[epoch:8, iter:673] Loss: 0.021 | Acc: 100.000% \n",
      "[epoch:8, iter:674] Loss: 0.026 | Acc: 100.000% \n",
      "[epoch:8, iter:675] Loss: 0.025 | Acc: 100.000% \n",
      "[epoch:8, iter:676] Loss: 0.034 | Acc: 98.438% \n",
      "[epoch:8, iter:677] Loss: 0.037 | Acc: 98.750% \n",
      "[epoch:8, iter:678] Loss: 0.043 | Acc: 98.958% \n",
      "[epoch:8, iter:679] Loss: 0.047 | Acc: 99.107% \n",
      "[epoch:8, iter:680] Loss: 0.049 | Acc: 99.219% \n",
      "[epoch:8, iter:681] Loss: 0.057 | Acc: 98.611% \n",
      "[epoch:8, iter:682] Loss: 0.061 | Acc: 98.125% \n",
      "[epoch:8, iter:683] Loss: 0.056 | Acc: 98.295% \n",
      "[epoch:8, iter:684] Loss: 0.052 | Acc: 98.438% \n",
      "[epoch:8, iter:685] Loss: 0.049 | Acc: 98.558% \n",
      "[epoch:8, iter:686] Loss: 0.048 | Acc: 98.661% \n",
      "[epoch:8, iter:687] Loss: 0.046 | Acc: 98.750% \n",
      "[epoch:8, iter:688] Loss: 0.048 | Acc: 98.438% \n",
      "[epoch:8, iter:689] Loss: 0.052 | Acc: 98.529% \n",
      "[epoch:8, iter:690] Loss: 0.056 | Acc: 98.264% \n",
      "[epoch:8, iter:691] Loss: 0.059 | Acc: 98.026% \n",
      "[epoch:8, iter:692] Loss: 0.056 | Acc: 98.125% \n",
      "[epoch:8, iter:693] Loss: 0.055 | Acc: 98.214% \n",
      "[epoch:8, iter:694] Loss: 0.055 | Acc: 98.295% \n",
      "[epoch:8, iter:695] Loss: 0.061 | Acc: 97.826% \n",
      "[epoch:8, iter:696] Loss: 0.061 | Acc: 97.917% \n",
      "[epoch:8, iter:697] Loss: 0.068 | Acc: 97.250% \n",
      "[epoch:8, iter:698] Loss: 0.068 | Acc: 97.356% \n",
      "[epoch:8, iter:699] Loss: 0.070 | Acc: 97.222% \n",
      "[epoch:8, iter:700] Loss: 0.068 | Acc: 97.321% \n",
      "[epoch:8, iter:701] Loss: 0.067 | Acc: 97.198% \n",
      "[epoch:8, iter:702] Loss: 0.065 | Acc: 97.292% \n",
      "[epoch:8, iter:703] Loss: 0.064 | Acc: 97.379% \n",
      "[epoch:8, iter:704] Loss: 0.078 | Acc: 97.070% \n",
      "[epoch:8, iter:705] Loss: 0.077 | Acc: 97.159% \n",
      "[epoch:8, iter:706] Loss: 0.077 | Acc: 96.875% \n",
      "[epoch:8, iter:707] Loss: 0.084 | Acc: 96.786% \n",
      "[epoch:8, iter:708] Loss: 0.083 | Acc: 96.875% \n",
      "[epoch:8, iter:709] Loss: 0.085 | Acc: 96.959% \n",
      "[epoch:8, iter:710] Loss: 0.085 | Acc: 96.875% \n",
      "[epoch:8, iter:711] Loss: 0.084 | Acc: 96.795% \n",
      "[epoch:8, iter:712] Loss: 0.098 | Acc: 96.562% \n",
      "[epoch:8, iter:713] Loss: 0.097 | Acc: 96.646% \n",
      "[epoch:8, iter:714] Loss: 0.095 | Acc: 96.726% \n",
      "[epoch:8, iter:715] Loss: 0.095 | Acc: 96.802% \n",
      "[epoch:8, iter:716] Loss: 0.104 | Acc: 96.591% \n",
      "[epoch:8, iter:717] Loss: 0.104 | Acc: 96.528% \n",
      "[epoch:8, iter:718] Loss: 0.105 | Acc: 96.467% \n",
      "[epoch:8, iter:719] Loss: 0.109 | Acc: 96.277% \n",
      "[epoch:8, iter:720] Loss: 0.109 | Acc: 96.224% \n",
      "[epoch:8, iter:721] Loss: 0.108 | Acc: 96.301% \n",
      "[epoch:8, iter:722] Loss: 0.107 | Acc: 96.375% \n",
      "[epoch:8, iter:723] Loss: 0.106 | Acc: 96.446% \n",
      "[epoch:8, iter:724] Loss: 0.104 | Acc: 96.514% \n",
      "[epoch:8, iter:725] Loss: 0.107 | Acc: 96.462% \n",
      "[epoch:8, iter:726] Loss: 0.108 | Acc: 96.412% \n",
      "[epoch:8, iter:727] Loss: 0.108 | Acc: 96.364% \n",
      "[epoch:8, iter:728] Loss: 0.106 | Acc: 96.429% \n",
      "[epoch:8, iter:729] Loss: 0.105 | Acc: 96.491% \n",
      "[epoch:8, iter:730] Loss: 0.103 | Acc: 96.552% \n",
      "[epoch:8, iter:731] Loss: 0.103 | Acc: 96.610% \n",
      "[epoch:8, iter:732] Loss: 0.109 | Acc: 96.562% \n",
      "[epoch:8, iter:733] Loss: 0.108 | Acc: 96.619% \n",
      "[epoch:8, iter:734] Loss: 0.108 | Acc: 96.573% \n",
      "[epoch:8, iter:735] Loss: 0.114 | Acc: 96.230% \n",
      "[epoch:8, iter:736] Loss: 0.112 | Acc: 96.289% \n",
      "[epoch:8, iter:737] Loss: 0.111 | Acc: 96.346% \n",
      "[epoch:8, iter:738] Loss: 0.112 | Acc: 96.212% \n",
      "[epoch:8, iter:739] Loss: 0.112 | Acc: 96.175% \n",
      "[epoch:8, iter:740] Loss: 0.112 | Acc: 96.140% \n",
      "[epoch:8, iter:741] Loss: 0.116 | Acc: 95.924% \n",
      "[epoch:8, iter:742] Loss: 0.114 | Acc: 95.982% \n",
      "[epoch:8, iter:743] Loss: 0.119 | Acc: 95.775% \n",
      "[epoch:8, iter:744] Loss: 0.118 | Acc: 95.833% \n",
      "[epoch:8, iter:745] Loss: 0.119 | Acc: 95.805% \n",
      "[epoch:8, iter:746] Loss: 0.118 | Acc: 95.861% \n",
      "[epoch:8, iter:747] Loss: 0.124 | Acc: 95.750% \n",
      "[epoch:8, iter:748] Loss: 0.126 | Acc: 95.641% \n",
      "[epoch:8, iter:749] Loss: 0.124 | Acc: 95.698% \n",
      "[epoch:8, iter:750] Loss: 0.127 | Acc: 95.513% \n",
      "[epoch:8, iter:751] Loss: 0.129 | Acc: 95.491% \n",
      "[epoch:8, iter:752] Loss: 0.128 | Acc: 95.469% \n",
      "[epoch:8, iter:753] Loss: 0.127 | Acc: 95.525% \n",
      "[epoch:8, iter:754] Loss: 0.129 | Acc: 95.503% \n",
      "[epoch:8, iter:755] Loss: 0.134 | Acc: 95.256% \n",
      "[epoch:8, iter:756] Loss: 0.136 | Acc: 95.164% \n",
      "[epoch:8, iter:757] Loss: 0.136 | Acc: 95.147% \n",
      "[epoch:8, iter:758] Loss: 0.135 | Acc: 95.203% \n",
      "[epoch:8, iter:759] Loss: 0.134 | Acc: 95.259% \n",
      "[epoch:8, iter:760] Loss: 0.134 | Acc: 95.241% \n",
      "[epoch:8, iter:761] Loss: 0.134 | Acc: 95.295% \n",
      "[epoch:8, iter:762] Loss: 0.134 | Acc: 95.347% \n",
      "[epoch:8, iter:763] Loss: 0.133 | Acc: 95.398% \n",
      "[epoch:8, iter:764] Loss: 0.132 | Acc: 95.448% \n",
      "[epoch:8, iter:765] Loss: 0.134 | Acc: 95.363% \n",
      "[epoch:8, iter:766] Loss: 0.135 | Acc: 95.346% \n",
      "[epoch:8, iter:767] Loss: 0.135 | Acc: 95.263% \n",
      "[epoch:8, iter:768] Loss: 0.134 | Acc: 95.291% \n",
      "Waiting Test!\n",
      "测试分类准确率为：76.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 9\n",
      "[epoch:9, iter:769] Loss: 0.657 | Acc: 87.500% \n",
      "[epoch:9, iter:770] Loss: 0.356 | Acc: 93.750% \n",
      "[epoch:9, iter:771] Loss: 0.247 | Acc: 95.833% \n",
      "[epoch:9, iter:772] Loss: 0.193 | Acc: 96.875% \n",
      "[epoch:9, iter:773] Loss: 0.171 | Acc: 97.500% \n",
      "[epoch:9, iter:774] Loss: 0.154 | Acc: 97.917% \n",
      "[epoch:9, iter:775] Loss: 0.159 | Acc: 97.321% \n",
      "[epoch:9, iter:776] Loss: 0.146 | Acc: 97.656% \n",
      "[epoch:9, iter:777] Loss: 0.135 | Acc: 97.917% \n",
      "[epoch:9, iter:778] Loss: 0.129 | Acc: 98.125% \n",
      "[epoch:9, iter:779] Loss: 0.121 | Acc: 98.295% \n",
      "[epoch:9, iter:780] Loss: 0.131 | Acc: 97.396% \n",
      "[epoch:9, iter:781] Loss: 0.125 | Acc: 97.596% \n",
      "[epoch:9, iter:782] Loss: 0.172 | Acc: 95.982% \n",
      "[epoch:9, iter:783] Loss: 0.168 | Acc: 95.833% \n",
      "[epoch:9, iter:784] Loss: 0.191 | Acc: 95.312% \n",
      "[epoch:9, iter:785] Loss: 0.188 | Acc: 95.221% \n",
      "[epoch:9, iter:786] Loss: 0.183 | Acc: 95.139% \n",
      "[epoch:9, iter:787] Loss: 0.180 | Acc: 95.395% \n",
      "[epoch:9, iter:788] Loss: 0.173 | Acc: 95.625% \n",
      "[epoch:9, iter:789] Loss: 0.166 | Acc: 95.833% \n",
      "[epoch:9, iter:790] Loss: 0.161 | Acc: 96.023% \n",
      "[epoch:9, iter:791] Loss: 0.160 | Acc: 95.652% \n",
      "[epoch:9, iter:792] Loss: 0.157 | Acc: 95.573% \n",
      "[epoch:9, iter:793] Loss: 0.154 | Acc: 95.750% \n",
      "[epoch:9, iter:794] Loss: 0.149 | Acc: 95.913% \n",
      "[epoch:9, iter:795] Loss: 0.145 | Acc: 96.065% \n",
      "[epoch:9, iter:796] Loss: 0.145 | Acc: 96.205% \n",
      "[epoch:9, iter:797] Loss: 0.140 | Acc: 96.336% \n",
      "[epoch:9, iter:798] Loss: 0.136 | Acc: 96.458% \n",
      "[epoch:9, iter:799] Loss: 0.136 | Acc: 96.371% \n",
      "[epoch:9, iter:800] Loss: 0.133 | Acc: 96.484% \n",
      "[epoch:9, iter:801] Loss: 0.130 | Acc: 96.591% \n",
      "[epoch:9, iter:802] Loss: 0.130 | Acc: 96.507% \n",
      "[epoch:9, iter:803] Loss: 0.126 | Acc: 96.607% \n",
      "[epoch:9, iter:804] Loss: 0.124 | Acc: 96.701% \n",
      "[epoch:9, iter:805] Loss: 0.122 | Acc: 96.622% \n",
      "[epoch:9, iter:806] Loss: 0.120 | Acc: 96.711% \n",
      "[epoch:9, iter:807] Loss: 0.119 | Acc: 96.795% \n",
      "[epoch:9, iter:808] Loss: 0.116 | Acc: 96.875% \n",
      "[epoch:9, iter:809] Loss: 0.115 | Acc: 96.799% \n",
      "[epoch:9, iter:810] Loss: 0.116 | Acc: 96.726% \n",
      "[epoch:9, iter:811] Loss: 0.113 | Acc: 96.802% \n",
      "[epoch:9, iter:812] Loss: 0.111 | Acc: 96.875% \n",
      "[epoch:9, iter:813] Loss: 0.117 | Acc: 96.528% \n",
      "[epoch:9, iter:814] Loss: 0.116 | Acc: 96.603% \n",
      "[epoch:9, iter:815] Loss: 0.123 | Acc: 96.410% \n",
      "[epoch:9, iter:816] Loss: 0.122 | Acc: 96.484% \n",
      "[epoch:9, iter:817] Loss: 0.123 | Acc: 96.301% \n",
      "[epoch:9, iter:818] Loss: 0.122 | Acc: 96.250% \n",
      "[epoch:9, iter:819] Loss: 0.121 | Acc: 96.324% \n",
      "[epoch:9, iter:820] Loss: 0.123 | Acc: 96.274% \n",
      "[epoch:9, iter:821] Loss: 0.121 | Acc: 96.344% \n",
      "[epoch:9, iter:822] Loss: 0.118 | Acc: 96.412% \n",
      "[epoch:9, iter:823] Loss: 0.120 | Acc: 96.364% \n",
      "[epoch:9, iter:824] Loss: 0.123 | Acc: 96.317% \n",
      "[epoch:9, iter:825] Loss: 0.124 | Acc: 96.272% \n",
      "[epoch:9, iter:826] Loss: 0.126 | Acc: 96.228% \n",
      "[epoch:9, iter:827] Loss: 0.132 | Acc: 95.975% \n",
      "[epoch:9, iter:828] Loss: 0.130 | Acc: 96.042% \n",
      "[epoch:9, iter:829] Loss: 0.129 | Acc: 96.107% \n",
      "[epoch:9, iter:830] Loss: 0.127 | Acc: 96.169% \n",
      "[epoch:9, iter:831] Loss: 0.126 | Acc: 96.230% \n",
      "[epoch:9, iter:832] Loss: 0.132 | Acc: 95.996% \n",
      "[epoch:9, iter:833] Loss: 0.130 | Acc: 96.058% \n",
      "[epoch:9, iter:834] Loss: 0.137 | Acc: 95.928% \n",
      "[epoch:9, iter:835] Loss: 0.141 | Acc: 95.802% \n",
      "[epoch:9, iter:836] Loss: 0.141 | Acc: 95.772% \n",
      "[epoch:9, iter:837] Loss: 0.142 | Acc: 95.652% \n",
      "[epoch:9, iter:838] Loss: 0.142 | Acc: 95.625% \n",
      "[epoch:9, iter:839] Loss: 0.144 | Acc: 95.511% \n",
      "[epoch:9, iter:840] Loss: 0.144 | Acc: 95.486% \n",
      "[epoch:9, iter:841] Loss: 0.143 | Acc: 95.548% \n",
      "[epoch:9, iter:842] Loss: 0.143 | Acc: 95.524% \n",
      "[epoch:9, iter:843] Loss: 0.141 | Acc: 95.583% \n",
      "[epoch:9, iter:844] Loss: 0.145 | Acc: 95.477% \n",
      "[epoch:9, iter:845] Loss: 0.150 | Acc: 95.292% \n",
      "[epoch:9, iter:846] Loss: 0.148 | Acc: 95.353% \n",
      "[epoch:9, iter:847] Loss: 0.147 | Acc: 95.411% \n",
      "[epoch:9, iter:848] Loss: 0.145 | Acc: 95.469% \n",
      "[epoch:9, iter:849] Loss: 0.146 | Acc: 95.448% \n",
      "[epoch:9, iter:850] Loss: 0.144 | Acc: 95.503% \n",
      "[epoch:9, iter:851] Loss: 0.143 | Acc: 95.557% \n",
      "[epoch:9, iter:852] Loss: 0.145 | Acc: 95.536% \n",
      "[epoch:9, iter:853] Loss: 0.147 | Acc: 95.441% \n",
      "[epoch:9, iter:854] Loss: 0.146 | Acc: 95.494% \n",
      "[epoch:9, iter:855] Loss: 0.146 | Acc: 95.474% \n",
      "[epoch:9, iter:856] Loss: 0.150 | Acc: 95.099% \n",
      "[epoch:9, iter:857] Loss: 0.149 | Acc: 95.154% \n",
      "[epoch:9, iter:858] Loss: 0.147 | Acc: 95.208% \n",
      "[epoch:9, iter:859] Loss: 0.150 | Acc: 95.192% \n",
      "[epoch:9, iter:860] Loss: 0.149 | Acc: 95.245% \n",
      "[epoch:9, iter:861] Loss: 0.151 | Acc: 95.094% \n",
      "[epoch:9, iter:862] Loss: 0.151 | Acc: 95.146% \n",
      "[epoch:9, iter:863] Loss: 0.149 | Acc: 95.197% \n",
      "[epoch:9, iter:864] Loss: 0.153 | Acc: 95.095% \n",
      "Waiting Test!\n",
      "测试分类准确率为：77.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 10\n",
      "[epoch:10, iter:865] Loss: 0.194 | Acc: 93.750% \n",
      "[epoch:10, iter:866] Loss: 0.121 | Acc: 96.875% \n",
      "[epoch:10, iter:867] Loss: 0.096 | Acc: 97.917% \n",
      "[epoch:10, iter:868] Loss: 0.086 | Acc: 98.438% \n",
      "[epoch:10, iter:869] Loss: 0.071 | Acc: 98.750% \n",
      "[epoch:10, iter:870] Loss: 0.122 | Acc: 96.875% \n",
      "[epoch:10, iter:871] Loss: 0.138 | Acc: 96.429% \n",
      "[epoch:10, iter:872] Loss: 0.128 | Acc: 96.875% \n",
      "[epoch:10, iter:873] Loss: 0.142 | Acc: 95.833% \n",
      "[epoch:10, iter:874] Loss: 0.131 | Acc: 96.250% \n",
      "[epoch:10, iter:875] Loss: 0.126 | Acc: 96.591% \n",
      "[epoch:10, iter:876] Loss: 0.120 | Acc: 96.875% \n",
      "[epoch:10, iter:877] Loss: 0.128 | Acc: 95.673% \n",
      "[epoch:10, iter:878] Loss: 0.128 | Acc: 95.536% \n",
      "[epoch:10, iter:879] Loss: 0.125 | Acc: 95.833% \n",
      "[epoch:10, iter:880] Loss: 0.119 | Acc: 96.094% \n",
      "[epoch:10, iter:881] Loss: 0.129 | Acc: 95.956% \n",
      "[epoch:10, iter:882] Loss: 0.128 | Acc: 95.833% \n",
      "[epoch:10, iter:883] Loss: 0.124 | Acc: 96.053% \n",
      "[epoch:10, iter:884] Loss: 0.131 | Acc: 95.938% \n",
      "[epoch:10, iter:885] Loss: 0.140 | Acc: 95.833% \n",
      "[epoch:10, iter:886] Loss: 0.136 | Acc: 96.023% \n",
      "[epoch:10, iter:887] Loss: 0.132 | Acc: 96.196% \n",
      "[epoch:10, iter:888] Loss: 0.128 | Acc: 96.354% \n",
      "[epoch:10, iter:889] Loss: 0.123 | Acc: 96.500% \n",
      "[epoch:10, iter:890] Loss: 0.119 | Acc: 96.635% \n",
      "[epoch:10, iter:891] Loss: 0.121 | Acc: 96.296% \n",
      "[epoch:10, iter:892] Loss: 0.119 | Acc: 96.429% \n",
      "[epoch:10, iter:893] Loss: 0.117 | Acc: 96.552% \n",
      "[epoch:10, iter:894] Loss: 0.114 | Acc: 96.667% \n",
      "[epoch:10, iter:895] Loss: 0.115 | Acc: 96.573% \n",
      "[epoch:10, iter:896] Loss: 0.113 | Acc: 96.680% \n",
      "[epoch:10, iter:897] Loss: 0.110 | Acc: 96.780% \n",
      "[epoch:10, iter:898] Loss: 0.109 | Acc: 96.875% \n",
      "[epoch:10, iter:899] Loss: 0.118 | Acc: 96.429% \n",
      "[epoch:10, iter:900] Loss: 0.125 | Acc: 96.181% \n",
      "[epoch:10, iter:901] Loss: 0.127 | Acc: 96.115% \n",
      "[epoch:10, iter:902] Loss: 0.125 | Acc: 96.217% \n",
      "[epoch:10, iter:903] Loss: 0.123 | Acc: 96.154% \n",
      "[epoch:10, iter:904] Loss: 0.121 | Acc: 96.250% \n",
      "[epoch:10, iter:905] Loss: 0.120 | Acc: 96.341% \n",
      "[epoch:10, iter:906] Loss: 0.117 | Acc: 96.429% \n",
      "[epoch:10, iter:907] Loss: 0.115 | Acc: 96.512% \n",
      "[epoch:10, iter:908] Loss: 0.113 | Acc: 96.591% \n",
      "[epoch:10, iter:909] Loss: 0.110 | Acc: 96.667% \n",
      "[epoch:10, iter:910] Loss: 0.112 | Acc: 96.467% \n",
      "[epoch:10, iter:911] Loss: 0.110 | Acc: 96.543% \n",
      "[epoch:10, iter:912] Loss: 0.120 | Acc: 96.224% \n",
      "[epoch:10, iter:913] Loss: 0.118 | Acc: 96.301% \n",
      "[epoch:10, iter:914] Loss: 0.119 | Acc: 96.375% \n",
      "[epoch:10, iter:915] Loss: 0.117 | Acc: 96.446% \n",
      "[epoch:10, iter:916] Loss: 0.122 | Acc: 96.394% \n",
      "[epoch:10, iter:917] Loss: 0.121 | Acc: 96.462% \n",
      "[epoch:10, iter:918] Loss: 0.120 | Acc: 96.412% \n",
      "[epoch:10, iter:919] Loss: 0.123 | Acc: 96.136% \n",
      "[epoch:10, iter:920] Loss: 0.124 | Acc: 96.094% \n",
      "[epoch:10, iter:921] Loss: 0.123 | Acc: 96.162% \n",
      "[epoch:10, iter:922] Loss: 0.121 | Acc: 96.228% \n",
      "[epoch:10, iter:923] Loss: 0.120 | Acc: 96.292% \n",
      "[epoch:10, iter:924] Loss: 0.118 | Acc: 96.354% \n",
      "[epoch:10, iter:925] Loss: 0.123 | Acc: 96.311% \n",
      "[epoch:10, iter:926] Loss: 0.121 | Acc: 96.371% \n",
      "[epoch:10, iter:927] Loss: 0.120 | Acc: 96.429% \n",
      "[epoch:10, iter:928] Loss: 0.121 | Acc: 96.387% \n",
      "[epoch:10, iter:929] Loss: 0.120 | Acc: 96.346% \n",
      "[epoch:10, iter:930] Loss: 0.120 | Acc: 96.402% \n",
      "[epoch:10, iter:931] Loss: 0.120 | Acc: 96.362% \n",
      "[epoch:10, iter:932] Loss: 0.119 | Acc: 96.415% \n",
      "[epoch:10, iter:933] Loss: 0.118 | Acc: 96.467% \n",
      "[epoch:10, iter:934] Loss: 0.116 | Acc: 96.518% \n",
      "[epoch:10, iter:935] Loss: 0.120 | Acc: 96.391% \n",
      "[epoch:10, iter:936] Loss: 0.119 | Acc: 96.441% \n",
      "[epoch:10, iter:937] Loss: 0.118 | Acc: 96.490% \n",
      "[epoch:10, iter:938] Loss: 0.116 | Acc: 96.537% \n",
      "[epoch:10, iter:939] Loss: 0.124 | Acc: 96.250% \n",
      "[epoch:10, iter:940] Loss: 0.125 | Acc: 96.217% \n",
      "[epoch:10, iter:941] Loss: 0.124 | Acc: 96.266% \n",
      "[epoch:10, iter:942] Loss: 0.122 | Acc: 96.314% \n",
      "[epoch:10, iter:943] Loss: 0.124 | Acc: 96.282% \n",
      "[epoch:10, iter:944] Loss: 0.123 | Acc: 96.328% \n",
      "[epoch:10, iter:945] Loss: 0.122 | Acc: 96.373% \n",
      "[epoch:10, iter:946] Loss: 0.123 | Acc: 96.341% \n",
      "[epoch:10, iter:947] Loss: 0.121 | Acc: 96.386% \n",
      "[epoch:10, iter:948] Loss: 0.120 | Acc: 96.429% \n",
      "[epoch:10, iter:949] Loss: 0.119 | Acc: 96.471% \n",
      "[epoch:10, iter:950] Loss: 0.118 | Acc: 96.512% \n",
      "[epoch:10, iter:951] Loss: 0.117 | Acc: 96.552% \n",
      "[epoch:10, iter:952] Loss: 0.116 | Acc: 96.591% \n",
      "[epoch:10, iter:953] Loss: 0.115 | Acc: 96.629% \n",
      "[epoch:10, iter:954] Loss: 0.116 | Acc: 96.597% \n",
      "[epoch:10, iter:955] Loss: 0.115 | Acc: 96.635% \n",
      "[epoch:10, iter:956] Loss: 0.117 | Acc: 96.535% \n",
      "[epoch:10, iter:957] Loss: 0.119 | Acc: 96.505% \n",
      "[epoch:10, iter:958] Loss: 0.118 | Acc: 96.543% \n",
      "[epoch:10, iter:959] Loss: 0.121 | Acc: 96.513% \n",
      "[epoch:10, iter:960] Loss: 0.120 | Acc: 96.534% \n",
      "Waiting Test!\n",
      "测试分类准确率为：85.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 11\n",
      "[epoch:11, iter:961] Loss: 0.021 | Acc: 100.000% \n",
      "[epoch:11, iter:962] Loss: 0.024 | Acc: 100.000% \n",
      "[epoch:11, iter:963] Loss: 0.033 | Acc: 100.000% \n",
      "[epoch:11, iter:964] Loss: 0.041 | Acc: 100.000% \n",
      "[epoch:11, iter:965] Loss: 0.059 | Acc: 100.000% \n",
      "[epoch:11, iter:966] Loss: 0.050 | Acc: 100.000% \n",
      "[epoch:11, iter:967] Loss: 0.065 | Acc: 99.107% \n",
      "[epoch:11, iter:968] Loss: 0.061 | Acc: 99.219% \n",
      "[epoch:11, iter:969] Loss: 0.074 | Acc: 98.611% \n",
      "[epoch:11, iter:970] Loss: 0.079 | Acc: 98.750% \n",
      "[epoch:11, iter:971] Loss: 0.073 | Acc: 98.864% \n",
      "[epoch:11, iter:972] Loss: 0.080 | Acc: 98.438% \n",
      "[epoch:11, iter:973] Loss: 0.074 | Acc: 98.558% \n",
      "[epoch:11, iter:974] Loss: 0.069 | Acc: 98.661% \n",
      "[epoch:11, iter:975] Loss: 0.067 | Acc: 98.750% \n",
      "[epoch:11, iter:976] Loss: 0.064 | Acc: 98.828% \n",
      "[epoch:11, iter:977] Loss: 0.067 | Acc: 98.529% \n",
      "[epoch:11, iter:978] Loss: 0.064 | Acc: 98.611% \n",
      "[epoch:11, iter:979] Loss: 0.065 | Acc: 98.355% \n",
      "[epoch:11, iter:980] Loss: 0.063 | Acc: 98.438% \n",
      "[epoch:11, iter:981] Loss: 0.065 | Acc: 98.512% \n",
      "[epoch:11, iter:982] Loss: 0.063 | Acc: 98.580% \n",
      "[epoch:11, iter:983] Loss: 0.060 | Acc: 98.641% \n",
      "[epoch:11, iter:984] Loss: 0.059 | Acc: 98.698% \n",
      "[epoch:11, iter:985] Loss: 0.058 | Acc: 98.750% \n",
      "[epoch:11, iter:986] Loss: 0.065 | Acc: 98.317% \n",
      "[epoch:11, iter:987] Loss: 0.063 | Acc: 98.380% \n",
      "[epoch:11, iter:988] Loss: 0.062 | Acc: 98.438% \n",
      "[epoch:11, iter:989] Loss: 0.067 | Acc: 98.276% \n",
      "[epoch:11, iter:990] Loss: 0.070 | Acc: 98.125% \n",
      "[epoch:11, iter:991] Loss: 0.068 | Acc: 98.185% \n",
      "[epoch:11, iter:992] Loss: 0.071 | Acc: 98.047% \n",
      "[epoch:11, iter:993] Loss: 0.075 | Acc: 97.917% \n",
      "[epoch:11, iter:994] Loss: 0.073 | Acc: 97.978% \n",
      "[epoch:11, iter:995] Loss: 0.071 | Acc: 98.036% \n",
      "[epoch:11, iter:996] Loss: 0.073 | Acc: 97.917% \n",
      "[epoch:11, iter:997] Loss: 0.075 | Acc: 97.973% \n",
      "[epoch:11, iter:998] Loss: 0.073 | Acc: 98.026% \n",
      "[epoch:11, iter:999] Loss: 0.071 | Acc: 98.077% \n",
      "[epoch:11, iter:1000] Loss: 0.080 | Acc: 97.656% \n",
      "[epoch:11, iter:1001] Loss: 0.082 | Acc: 97.561% \n",
      "[epoch:11, iter:1002] Loss: 0.082 | Acc: 97.619% \n",
      "[epoch:11, iter:1003] Loss: 0.080 | Acc: 97.674% \n",
      "[epoch:11, iter:1004] Loss: 0.080 | Acc: 97.727% \n",
      "[epoch:11, iter:1005] Loss: 0.080 | Acc: 97.639% \n",
      "[epoch:11, iter:1006] Loss: 0.079 | Acc: 97.690% \n",
      "[epoch:11, iter:1007] Loss: 0.078 | Acc: 97.739% \n",
      "[epoch:11, iter:1008] Loss: 0.079 | Acc: 97.656% \n",
      "[epoch:11, iter:1009] Loss: 0.086 | Acc: 97.449% \n",
      "[epoch:11, iter:1010] Loss: 0.087 | Acc: 97.375% \n",
      "[epoch:11, iter:1011] Loss: 0.089 | Acc: 97.181% \n",
      "[epoch:11, iter:1012] Loss: 0.089 | Acc: 97.236% \n",
      "[epoch:11, iter:1013] Loss: 0.088 | Acc: 97.288% \n",
      "[epoch:11, iter:1014] Loss: 0.087 | Acc: 97.338% \n",
      "[epoch:11, iter:1015] Loss: 0.088 | Acc: 97.159% \n",
      "[epoch:11, iter:1016] Loss: 0.090 | Acc: 97.098% \n",
      "[epoch:11, iter:1017] Loss: 0.089 | Acc: 97.149% \n",
      "[epoch:11, iter:1018] Loss: 0.087 | Acc: 97.198% \n",
      "[epoch:11, iter:1019] Loss: 0.086 | Acc: 97.246% \n",
      "[epoch:11, iter:1020] Loss: 0.085 | Acc: 97.292% \n",
      "[epoch:11, iter:1021] Loss: 0.085 | Acc: 97.336% \n",
      "[epoch:11, iter:1022] Loss: 0.084 | Acc: 97.379% \n",
      "[epoch:11, iter:1023] Loss: 0.083 | Acc: 97.421% \n",
      "[epoch:11, iter:1024] Loss: 0.085 | Acc: 97.363% \n",
      "[epoch:11, iter:1025] Loss: 0.087 | Acc: 97.308% \n",
      "[epoch:11, iter:1026] Loss: 0.086 | Acc: 97.348% \n",
      "[epoch:11, iter:1027] Loss: 0.085 | Acc: 97.388% \n",
      "[epoch:11, iter:1028] Loss: 0.084 | Acc: 97.426% \n",
      "[epoch:11, iter:1029] Loss: 0.083 | Acc: 97.464% \n",
      "[epoch:11, iter:1030] Loss: 0.082 | Acc: 97.500% \n",
      "[epoch:11, iter:1031] Loss: 0.082 | Acc: 97.535% \n",
      "[epoch:11, iter:1032] Loss: 0.087 | Acc: 97.309% \n",
      "[epoch:11, iter:1033] Loss: 0.086 | Acc: 97.346% \n",
      "[epoch:11, iter:1034] Loss: 0.086 | Acc: 97.297% \n",
      "[epoch:11, iter:1035] Loss: 0.090 | Acc: 97.167% \n",
      "[epoch:11, iter:1036] Loss: 0.089 | Acc: 97.204% \n",
      "[epoch:11, iter:1037] Loss: 0.094 | Acc: 96.834% \n",
      "[epoch:11, iter:1038] Loss: 0.093 | Acc: 96.875% \n",
      "[epoch:11, iter:1039] Loss: 0.094 | Acc: 96.835% \n",
      "[epoch:11, iter:1040] Loss: 0.093 | Acc: 96.875% \n",
      "[epoch:11, iter:1041] Loss: 0.093 | Acc: 96.914% \n",
      "[epoch:11, iter:1042] Loss: 0.092 | Acc: 96.951% \n",
      "[epoch:11, iter:1043] Loss: 0.091 | Acc: 96.988% \n",
      "[epoch:11, iter:1044] Loss: 0.090 | Acc: 97.024% \n",
      "[epoch:11, iter:1045] Loss: 0.090 | Acc: 96.985% \n",
      "[epoch:11, iter:1046] Loss: 0.095 | Acc: 96.875% \n",
      "[epoch:11, iter:1047] Loss: 0.094 | Acc: 96.911% \n",
      "[epoch:11, iter:1048] Loss: 0.093 | Acc: 96.946% \n",
      "[epoch:11, iter:1049] Loss: 0.093 | Acc: 96.910% \n",
      "[epoch:11, iter:1050] Loss: 0.095 | Acc: 96.875% \n",
      "[epoch:11, iter:1051] Loss: 0.094 | Acc: 96.909% \n",
      "[epoch:11, iter:1052] Loss: 0.095 | Acc: 96.875% \n",
      "[epoch:11, iter:1053] Loss: 0.097 | Acc: 96.841% \n",
      "[epoch:11, iter:1054] Loss: 0.098 | Acc: 96.875% \n",
      "[epoch:11, iter:1055] Loss: 0.098 | Acc: 96.842% \n",
      "[epoch:11, iter:1056] Loss: 0.097 | Acc: 96.861% \n",
      "Waiting Test!\n",
      "测试分类准确率为：88.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 12\n",
      "[epoch:12, iter:1057] Loss: 0.156 | Acc: 93.750% \n",
      "[epoch:12, iter:1058] Loss: 0.128 | Acc: 93.750% \n",
      "[epoch:12, iter:1059] Loss: 0.090 | Acc: 95.833% \n",
      "[epoch:12, iter:1060] Loss: 0.202 | Acc: 92.188% \n",
      "[epoch:12, iter:1061] Loss: 0.170 | Acc: 93.750% \n",
      "[epoch:12, iter:1062] Loss: 0.148 | Acc: 94.792% \n",
      "[epoch:12, iter:1063] Loss: 0.129 | Acc: 95.536% \n",
      "[epoch:12, iter:1064] Loss: 0.121 | Acc: 96.094% \n",
      "[epoch:12, iter:1065] Loss: 0.108 | Acc: 96.528% \n",
      "[epoch:12, iter:1066] Loss: 0.099 | Acc: 96.875% \n",
      "[epoch:12, iter:1067] Loss: 0.110 | Acc: 96.023% \n",
      "[epoch:12, iter:1068] Loss: 0.103 | Acc: 96.354% \n",
      "[epoch:12, iter:1069] Loss: 0.104 | Acc: 96.154% \n",
      "[epoch:12, iter:1070] Loss: 0.099 | Acc: 96.429% \n",
      "[epoch:12, iter:1071] Loss: 0.094 | Acc: 96.667% \n",
      "[epoch:12, iter:1072] Loss: 0.091 | Acc: 96.875% \n",
      "[epoch:12, iter:1073] Loss: 0.105 | Acc: 96.324% \n",
      "[epoch:12, iter:1074] Loss: 0.119 | Acc: 95.833% \n",
      "[epoch:12, iter:1075] Loss: 0.114 | Acc: 96.053% \n",
      "[epoch:12, iter:1076] Loss: 0.116 | Acc: 95.938% \n",
      "[epoch:12, iter:1077] Loss: 0.112 | Acc: 96.131% \n",
      "[epoch:12, iter:1078] Loss: 0.111 | Acc: 96.023% \n",
      "[epoch:12, iter:1079] Loss: 0.107 | Acc: 96.196% \n",
      "[epoch:12, iter:1080] Loss: 0.103 | Acc: 96.354% \n",
      "[epoch:12, iter:1081] Loss: 0.100 | Acc: 96.500% \n",
      "[epoch:12, iter:1082] Loss: 0.097 | Acc: 96.635% \n",
      "[epoch:12, iter:1083] Loss: 0.103 | Acc: 96.296% \n",
      "[epoch:12, iter:1084] Loss: 0.107 | Acc: 96.205% \n",
      "[epoch:12, iter:1085] Loss: 0.104 | Acc: 96.336% \n",
      "[epoch:12, iter:1086] Loss: 0.101 | Acc: 96.458% \n",
      "[epoch:12, iter:1087] Loss: 0.100 | Acc: 96.573% \n",
      "[epoch:12, iter:1088] Loss: 0.097 | Acc: 96.680% \n",
      "[epoch:12, iter:1089] Loss: 0.096 | Acc: 96.780% \n",
      "[epoch:12, iter:1090] Loss: 0.100 | Acc: 96.507% \n",
      "[epoch:12, iter:1091] Loss: 0.099 | Acc: 96.607% \n",
      "[epoch:12, iter:1092] Loss: 0.097 | Acc: 96.701% \n",
      "[epoch:12, iter:1093] Loss: 0.107 | Acc: 96.284% \n",
      "[epoch:12, iter:1094] Loss: 0.104 | Acc: 96.382% \n",
      "[epoch:12, iter:1095] Loss: 0.102 | Acc: 96.474% \n",
      "[epoch:12, iter:1096] Loss: 0.103 | Acc: 96.406% \n",
      "[epoch:12, iter:1097] Loss: 0.102 | Acc: 96.494% \n",
      "[epoch:12, iter:1098] Loss: 0.100 | Acc: 96.577% \n",
      "[epoch:12, iter:1099] Loss: 0.098 | Acc: 96.657% \n",
      "[epoch:12, iter:1100] Loss: 0.108 | Acc: 96.307% \n",
      "[epoch:12, iter:1101] Loss: 0.115 | Acc: 96.250% \n",
      "[epoch:12, iter:1102] Loss: 0.118 | Acc: 96.060% \n",
      "[epoch:12, iter:1103] Loss: 0.117 | Acc: 96.144% \n",
      "[epoch:12, iter:1104] Loss: 0.115 | Acc: 96.224% \n",
      "[epoch:12, iter:1105] Loss: 0.113 | Acc: 96.301% \n",
      "[epoch:12, iter:1106] Loss: 0.111 | Acc: 96.375% \n",
      "[epoch:12, iter:1107] Loss: 0.110 | Acc: 96.446% \n",
      "[epoch:12, iter:1108] Loss: 0.108 | Acc: 96.514% \n",
      "[epoch:12, iter:1109] Loss: 0.120 | Acc: 95.873% \n",
      "[epoch:12, iter:1110] Loss: 0.122 | Acc: 95.833% \n",
      "[epoch:12, iter:1111] Loss: 0.124 | Acc: 95.682% \n",
      "[epoch:12, iter:1112] Loss: 0.122 | Acc: 95.759% \n",
      "[epoch:12, iter:1113] Loss: 0.121 | Acc: 95.833% \n",
      "[epoch:12, iter:1114] Loss: 0.125 | Acc: 95.690% \n",
      "[epoch:12, iter:1115] Loss: 0.124 | Acc: 95.763% \n",
      "[epoch:12, iter:1116] Loss: 0.123 | Acc: 95.833% \n",
      "[epoch:12, iter:1117] Loss: 0.123 | Acc: 95.799% \n",
      "[epoch:12, iter:1118] Loss: 0.123 | Acc: 95.867% \n",
      "[epoch:12, iter:1119] Loss: 0.121 | Acc: 95.933% \n",
      "[epoch:12, iter:1120] Loss: 0.120 | Acc: 95.996% \n",
      "[epoch:12, iter:1121] Loss: 0.119 | Acc: 96.058% \n",
      "[epoch:12, iter:1122] Loss: 0.122 | Acc: 96.023% \n",
      "[epoch:12, iter:1123] Loss: 0.128 | Acc: 95.896% \n",
      "[epoch:12, iter:1124] Loss: 0.126 | Acc: 95.956% \n",
      "[epoch:12, iter:1125] Loss: 0.129 | Acc: 95.924% \n",
      "[epoch:12, iter:1126] Loss: 0.127 | Acc: 95.982% \n",
      "[epoch:12, iter:1127] Loss: 0.129 | Acc: 95.863% \n",
      "[epoch:12, iter:1128] Loss: 0.127 | Acc: 95.920% \n",
      "[epoch:12, iter:1129] Loss: 0.128 | Acc: 95.890% \n",
      "[epoch:12, iter:1130] Loss: 0.126 | Acc: 95.946% \n",
      "[epoch:12, iter:1131] Loss: 0.125 | Acc: 96.000% \n",
      "[epoch:12, iter:1132] Loss: 0.124 | Acc: 96.053% \n",
      "[epoch:12, iter:1133] Loss: 0.122 | Acc: 96.104% \n",
      "[epoch:12, iter:1134] Loss: 0.121 | Acc: 96.154% \n",
      "[epoch:12, iter:1135] Loss: 0.120 | Acc: 96.203% \n",
      "[epoch:12, iter:1136] Loss: 0.119 | Acc: 96.250% \n",
      "[epoch:12, iter:1137] Loss: 0.123 | Acc: 96.219% \n",
      "[epoch:12, iter:1138] Loss: 0.121 | Acc: 96.265% \n",
      "[epoch:12, iter:1139] Loss: 0.122 | Acc: 96.235% \n",
      "[epoch:12, iter:1140] Loss: 0.120 | Acc: 96.280% \n",
      "[epoch:12, iter:1141] Loss: 0.119 | Acc: 96.324% \n",
      "[epoch:12, iter:1142] Loss: 0.118 | Acc: 96.366% \n",
      "[epoch:12, iter:1143] Loss: 0.117 | Acc: 96.408% \n",
      "[epoch:12, iter:1144] Loss: 0.117 | Acc: 96.449% \n",
      "[epoch:12, iter:1145] Loss: 0.117 | Acc: 96.348% \n",
      "[epoch:12, iter:1146] Loss: 0.120 | Acc: 96.250% \n",
      "[epoch:12, iter:1147] Loss: 0.120 | Acc: 96.223% \n",
      "[epoch:12, iter:1148] Loss: 0.119 | Acc: 96.264% \n",
      "[epoch:12, iter:1149] Loss: 0.118 | Acc: 96.237% \n",
      "[epoch:12, iter:1150] Loss: 0.117 | Acc: 96.277% \n",
      "[epoch:12, iter:1151] Loss: 0.120 | Acc: 96.184% \n",
      "[epoch:12, iter:1152] Loss: 0.119 | Acc: 96.207% \n",
      "Waiting Test!\n",
      "测试分类准确率为：88.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 13\n",
      "[epoch:13, iter:1153] Loss: 0.019 | Acc: 100.000% \n",
      "[epoch:13, iter:1154] Loss: 0.172 | Acc: 93.750% \n",
      "[epoch:13, iter:1155] Loss: 0.129 | Acc: 95.833% \n",
      "[epoch:13, iter:1156] Loss: 0.104 | Acc: 96.875% \n",
      "[epoch:13, iter:1157] Loss: 0.091 | Acc: 97.500% \n",
      "[epoch:13, iter:1158] Loss: 0.093 | Acc: 97.917% \n",
      "[epoch:13, iter:1159] Loss: 0.086 | Acc: 98.214% \n",
      "[epoch:13, iter:1160] Loss: 0.080 | Acc: 98.438% \n",
      "[epoch:13, iter:1161] Loss: 0.074 | Acc: 98.611% \n",
      "[epoch:13, iter:1162] Loss: 0.072 | Acc: 98.750% \n",
      "[epoch:13, iter:1163] Loss: 0.071 | Acc: 98.864% \n",
      "[epoch:13, iter:1164] Loss: 0.074 | Acc: 98.438% \n",
      "[epoch:13, iter:1165] Loss: 0.071 | Acc: 98.558% \n",
      "[epoch:13, iter:1166] Loss: 0.068 | Acc: 98.661% \n",
      "[epoch:13, iter:1167] Loss: 0.065 | Acc: 98.750% \n",
      "[epoch:13, iter:1168] Loss: 0.062 | Acc: 98.828% \n",
      "[epoch:13, iter:1169] Loss: 0.059 | Acc: 98.897% \n",
      "[epoch:13, iter:1170] Loss: 0.056 | Acc: 98.958% \n",
      "[epoch:13, iter:1171] Loss: 0.066 | Acc: 98.355% \n",
      "[epoch:13, iter:1172] Loss: 0.063 | Acc: 98.438% \n",
      "[epoch:13, iter:1173] Loss: 0.071 | Acc: 98.214% \n",
      "[epoch:13, iter:1174] Loss: 0.069 | Acc: 98.295% \n",
      "[epoch:13, iter:1175] Loss: 0.079 | Acc: 98.098% \n",
      "[epoch:13, iter:1176] Loss: 0.082 | Acc: 97.917% \n",
      "[epoch:13, iter:1177] Loss: 0.079 | Acc: 98.000% \n",
      "[epoch:13, iter:1178] Loss: 0.077 | Acc: 98.077% \n",
      "[epoch:13, iter:1179] Loss: 0.078 | Acc: 97.917% \n",
      "[epoch:13, iter:1180] Loss: 0.089 | Acc: 97.545% \n",
      "[epoch:13, iter:1181] Loss: 0.090 | Acc: 97.414% \n",
      "[epoch:13, iter:1182] Loss: 0.091 | Acc: 97.292% \n",
      "[epoch:13, iter:1183] Loss: 0.101 | Acc: 96.976% \n",
      "[epoch:13, iter:1184] Loss: 0.098 | Acc: 97.070% \n",
      "[epoch:13, iter:1185] Loss: 0.097 | Acc: 97.159% \n",
      "[epoch:13, iter:1186] Loss: 0.095 | Acc: 97.243% \n",
      "[epoch:13, iter:1187] Loss: 0.100 | Acc: 96.964% \n",
      "[epoch:13, iter:1188] Loss: 0.102 | Acc: 96.875% \n",
      "[epoch:13, iter:1189] Loss: 0.112 | Acc: 96.453% \n",
      "[epoch:13, iter:1190] Loss: 0.109 | Acc: 96.546% \n",
      "[epoch:13, iter:1191] Loss: 0.112 | Acc: 96.474% \n",
      "[epoch:13, iter:1192] Loss: 0.110 | Acc: 96.562% \n",
      "[epoch:13, iter:1193] Loss: 0.108 | Acc: 96.646% \n",
      "[epoch:13, iter:1194] Loss: 0.106 | Acc: 96.726% \n",
      "[epoch:13, iter:1195] Loss: 0.104 | Acc: 96.802% \n",
      "[epoch:13, iter:1196] Loss: 0.103 | Acc: 96.875% \n",
      "[epoch:13, iter:1197] Loss: 0.102 | Acc: 96.944% \n",
      "[epoch:13, iter:1198] Loss: 0.104 | Acc: 96.875% \n",
      "[epoch:13, iter:1199] Loss: 0.107 | Acc: 96.676% \n",
      "[epoch:13, iter:1200] Loss: 0.106 | Acc: 96.745% \n",
      "[epoch:13, iter:1201] Loss: 0.110 | Acc: 96.429% \n",
      "[epoch:13, iter:1202] Loss: 0.110 | Acc: 96.375% \n",
      "[epoch:13, iter:1203] Loss: 0.109 | Acc: 96.446% \n",
      "[epoch:13, iter:1204] Loss: 0.107 | Acc: 96.514% \n",
      "[epoch:13, iter:1205] Loss: 0.107 | Acc: 96.462% \n",
      "[epoch:13, iter:1206] Loss: 0.111 | Acc: 96.412% \n",
      "[epoch:13, iter:1207] Loss: 0.109 | Acc: 96.477% \n",
      "[epoch:13, iter:1208] Loss: 0.107 | Acc: 96.540% \n",
      "[epoch:13, iter:1209] Loss: 0.106 | Acc: 96.601% \n",
      "[epoch:13, iter:1210] Loss: 0.105 | Acc: 96.552% \n",
      "[epoch:13, iter:1211] Loss: 0.104 | Acc: 96.504% \n",
      "[epoch:13, iter:1212] Loss: 0.104 | Acc: 96.562% \n",
      "[epoch:13, iter:1213] Loss: 0.102 | Acc: 96.619% \n",
      "[epoch:13, iter:1214] Loss: 0.104 | Acc: 96.472% \n",
      "[epoch:13, iter:1215] Loss: 0.103 | Acc: 96.429% \n",
      "[epoch:13, iter:1216] Loss: 0.104 | Acc: 96.387% \n",
      "[epoch:13, iter:1217] Loss: 0.102 | Acc: 96.442% \n",
      "[epoch:13, iter:1218] Loss: 0.103 | Acc: 96.402% \n",
      "[epoch:13, iter:1219] Loss: 0.105 | Acc: 96.175% \n",
      "[epoch:13, iter:1220] Loss: 0.104 | Acc: 96.232% \n",
      "[epoch:13, iter:1221] Loss: 0.103 | Acc: 96.286% \n",
      "[epoch:13, iter:1222] Loss: 0.102 | Acc: 96.339% \n",
      "[epoch:13, iter:1223] Loss: 0.102 | Acc: 96.391% \n",
      "[epoch:13, iter:1224] Loss: 0.103 | Acc: 96.267% \n",
      "[epoch:13, iter:1225] Loss: 0.105 | Acc: 96.233% \n",
      "[epoch:13, iter:1226] Loss: 0.104 | Acc: 96.284% \n",
      "[epoch:13, iter:1227] Loss: 0.103 | Acc: 96.333% \n",
      "[epoch:13, iter:1228] Loss: 0.103 | Acc: 96.299% \n",
      "[epoch:13, iter:1229] Loss: 0.103 | Acc: 96.266% \n",
      "[epoch:13, iter:1230] Loss: 0.101 | Acc: 96.314% \n",
      "[epoch:13, iter:1231] Loss: 0.100 | Acc: 96.361% \n",
      "[epoch:13, iter:1232] Loss: 0.106 | Acc: 96.172% \n",
      "[epoch:13, iter:1233] Loss: 0.105 | Acc: 96.219% \n",
      "[epoch:13, iter:1234] Loss: 0.104 | Acc: 96.265% \n",
      "[epoch:13, iter:1235] Loss: 0.103 | Acc: 96.310% \n",
      "[epoch:13, iter:1236] Loss: 0.102 | Acc: 96.354% \n",
      "[epoch:13, iter:1237] Loss: 0.101 | Acc: 96.397% \n",
      "[epoch:13, iter:1238] Loss: 0.100 | Acc: 96.439% \n",
      "[epoch:13, iter:1239] Loss: 0.099 | Acc: 96.480% \n",
      "[epoch:13, iter:1240] Loss: 0.101 | Acc: 96.449% \n",
      "[epoch:13, iter:1241] Loss: 0.102 | Acc: 96.419% \n",
      "[epoch:13, iter:1242] Loss: 0.102 | Acc: 96.458% \n",
      "[epoch:13, iter:1243] Loss: 0.101 | Acc: 96.497% \n",
      "[epoch:13, iter:1244] Loss: 0.100 | Acc: 96.535% \n",
      "[epoch:13, iter:1245] Loss: 0.099 | Acc: 96.573% \n",
      "[epoch:13, iter:1246] Loss: 0.099 | Acc: 96.609% \n",
      "[epoch:13, iter:1247] Loss: 0.098 | Acc: 96.645% \n",
      "[epoch:13, iter:1248] Loss: 0.098 | Acc: 96.599% \n",
      "Waiting Test!\n",
      "测试分类准确率为：85.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 14\n",
      "[epoch:14, iter:1249] Loss: 0.303 | Acc: 87.500% \n",
      "[epoch:14, iter:1250] Loss: 0.184 | Acc: 90.625% \n",
      "[epoch:14, iter:1251] Loss: 0.149 | Acc: 93.750% \n",
      "[epoch:14, iter:1252] Loss: 0.129 | Acc: 93.750% \n",
      "[epoch:14, iter:1253] Loss: 0.130 | Acc: 92.500% \n",
      "[epoch:14, iter:1254] Loss: 0.117 | Acc: 93.750% \n",
      "[epoch:14, iter:1255] Loss: 0.103 | Acc: 94.643% \n",
      "[epoch:14, iter:1256] Loss: 0.091 | Acc: 95.312% \n",
      "[epoch:14, iter:1257] Loss: 0.083 | Acc: 95.833% \n",
      "[epoch:14, iter:1258] Loss: 0.077 | Acc: 96.250% \n",
      "[epoch:14, iter:1259] Loss: 0.105 | Acc: 94.886% \n",
      "[epoch:14, iter:1260] Loss: 0.097 | Acc: 95.312% \n",
      "[epoch:14, iter:1261] Loss: 0.091 | Acc: 95.673% \n",
      "[epoch:14, iter:1262] Loss: 0.087 | Acc: 95.982% \n",
      "[epoch:14, iter:1263] Loss: 0.081 | Acc: 96.250% \n",
      "[epoch:14, iter:1264] Loss: 0.085 | Acc: 96.094% \n",
      "[epoch:14, iter:1265] Loss: 0.081 | Acc: 96.324% \n",
      "[epoch:14, iter:1266] Loss: 0.077 | Acc: 96.528% \n",
      "[epoch:14, iter:1267] Loss: 0.073 | Acc: 96.711% \n",
      "[epoch:14, iter:1268] Loss: 0.070 | Acc: 96.875% \n",
      "[epoch:14, iter:1269] Loss: 0.084 | Acc: 96.429% \n",
      "[epoch:14, iter:1270] Loss: 0.082 | Acc: 96.591% \n",
      "[epoch:14, iter:1271] Loss: 0.079 | Acc: 96.739% \n",
      "[epoch:14, iter:1272] Loss: 0.086 | Acc: 96.615% \n",
      "[epoch:14, iter:1273] Loss: 0.084 | Acc: 96.750% \n",
      "[epoch:14, iter:1274] Loss: 0.081 | Acc: 96.875% \n",
      "[epoch:14, iter:1275] Loss: 0.078 | Acc: 96.991% \n",
      "[epoch:14, iter:1276] Loss: 0.075 | Acc: 97.098% \n",
      "[epoch:14, iter:1277] Loss: 0.073 | Acc: 97.198% \n",
      "[epoch:14, iter:1278] Loss: 0.071 | Acc: 97.292% \n",
      "[epoch:14, iter:1279] Loss: 0.070 | Acc: 97.379% \n",
      "[epoch:14, iter:1280] Loss: 0.082 | Acc: 96.289% \n",
      "[epoch:14, iter:1281] Loss: 0.081 | Acc: 96.212% \n",
      "[epoch:14, iter:1282] Loss: 0.079 | Acc: 96.324% \n",
      "[epoch:14, iter:1283] Loss: 0.079 | Acc: 96.429% \n",
      "[epoch:14, iter:1284] Loss: 0.103 | Acc: 95.312% \n",
      "[epoch:14, iter:1285] Loss: 0.104 | Acc: 95.270% \n",
      "[epoch:14, iter:1286] Loss: 0.106 | Acc: 95.230% \n",
      "[epoch:14, iter:1287] Loss: 0.106 | Acc: 95.192% \n",
      "[epoch:14, iter:1288] Loss: 0.107 | Acc: 95.156% \n",
      "[epoch:14, iter:1289] Loss: 0.106 | Acc: 95.274% \n",
      "[epoch:14, iter:1290] Loss: 0.103 | Acc: 95.387% \n",
      "[epoch:14, iter:1291] Loss: 0.102 | Acc: 95.494% \n",
      "[epoch:14, iter:1292] Loss: 0.100 | Acc: 95.597% \n",
      "[epoch:14, iter:1293] Loss: 0.100 | Acc: 95.556% \n",
      "[epoch:14, iter:1294] Loss: 0.107 | Acc: 95.516% \n",
      "[epoch:14, iter:1295] Loss: 0.106 | Acc: 95.612% \n",
      "[epoch:14, iter:1296] Loss: 0.105 | Acc: 95.703% \n",
      "[epoch:14, iter:1297] Loss: 0.104 | Acc: 95.791% \n",
      "[epoch:14, iter:1298] Loss: 0.103 | Acc: 95.875% \n",
      "[epoch:14, iter:1299] Loss: 0.102 | Acc: 95.956% \n",
      "[epoch:14, iter:1300] Loss: 0.100 | Acc: 96.034% \n",
      "[epoch:14, iter:1301] Loss: 0.103 | Acc: 95.873% \n",
      "[epoch:14, iter:1302] Loss: 0.104 | Acc: 95.833% \n",
      "[epoch:14, iter:1303] Loss: 0.104 | Acc: 95.682% \n",
      "[epoch:14, iter:1304] Loss: 0.103 | Acc: 95.759% \n",
      "[epoch:14, iter:1305] Loss: 0.102 | Acc: 95.833% \n",
      "[epoch:14, iter:1306] Loss: 0.100 | Acc: 95.905% \n",
      "[epoch:14, iter:1307] Loss: 0.100 | Acc: 95.975% \n",
      "[epoch:14, iter:1308] Loss: 0.099 | Acc: 96.042% \n",
      "[epoch:14, iter:1309] Loss: 0.097 | Acc: 96.107% \n",
      "[epoch:14, iter:1310] Loss: 0.096 | Acc: 96.169% \n",
      "[epoch:14, iter:1311] Loss: 0.094 | Acc: 96.230% \n",
      "[epoch:14, iter:1312] Loss: 0.094 | Acc: 96.191% \n",
      "[epoch:14, iter:1313] Loss: 0.096 | Acc: 96.154% \n",
      "[epoch:14, iter:1314] Loss: 0.100 | Acc: 95.928% \n",
      "[epoch:14, iter:1315] Loss: 0.100 | Acc: 95.896% \n",
      "[epoch:14, iter:1316] Loss: 0.103 | Acc: 95.864% \n",
      "[epoch:14, iter:1317] Loss: 0.106 | Acc: 95.833% \n",
      "[epoch:14, iter:1318] Loss: 0.106 | Acc: 95.804% \n",
      "[epoch:14, iter:1319] Loss: 0.110 | Acc: 95.599% \n",
      "[epoch:14, iter:1320] Loss: 0.112 | Acc: 95.573% \n",
      "[epoch:14, iter:1321] Loss: 0.119 | Acc: 95.377% \n",
      "[epoch:14, iter:1322] Loss: 0.118 | Acc: 95.439% \n",
      "[epoch:14, iter:1323] Loss: 0.118 | Acc: 95.417% \n",
      "[epoch:14, iter:1324] Loss: 0.117 | Acc: 95.477% \n",
      "[epoch:14, iter:1325] Loss: 0.115 | Acc: 95.536% \n",
      "[epoch:14, iter:1326] Loss: 0.117 | Acc: 95.513% \n",
      "[epoch:14, iter:1327] Loss: 0.117 | Acc: 95.491% \n",
      "[epoch:14, iter:1328] Loss: 0.116 | Acc: 95.547% \n",
      "[epoch:14, iter:1329] Loss: 0.120 | Acc: 95.448% \n",
      "[epoch:14, iter:1330] Loss: 0.126 | Acc: 95.274% \n",
      "[epoch:14, iter:1331] Loss: 0.126 | Acc: 95.331% \n",
      "[epoch:14, iter:1332] Loss: 0.126 | Acc: 95.312% \n",
      "[epoch:14, iter:1333] Loss: 0.126 | Acc: 95.294% \n",
      "[epoch:14, iter:1334] Loss: 0.126 | Acc: 95.349% \n",
      "[epoch:14, iter:1335] Loss: 0.125 | Acc: 95.402% \n",
      "[epoch:14, iter:1336] Loss: 0.126 | Acc: 95.312% \n",
      "[epoch:14, iter:1337] Loss: 0.125 | Acc: 95.365% \n",
      "[epoch:14, iter:1338] Loss: 0.124 | Acc: 95.417% \n",
      "[epoch:14, iter:1339] Loss: 0.126 | Acc: 95.330% \n",
      "[epoch:14, iter:1340] Loss: 0.125 | Acc: 95.380% \n",
      "[epoch:14, iter:1341] Loss: 0.128 | Acc: 95.228% \n",
      "[epoch:14, iter:1342] Loss: 0.128 | Acc: 95.213% \n",
      "[epoch:14, iter:1343] Loss: 0.128 | Acc: 95.197% \n",
      "[epoch:14, iter:1344] Loss: 0.129 | Acc: 95.160% \n",
      "Waiting Test!\n",
      "测试分类准确率为：87.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 15\n",
      "[epoch:15, iter:1345] Loss: 0.130 | Acc: 93.750% \n",
      "[epoch:15, iter:1346] Loss: 0.186 | Acc: 87.500% \n",
      "[epoch:15, iter:1347] Loss: 0.147 | Acc: 89.583% \n",
      "[epoch:15, iter:1348] Loss: 0.116 | Acc: 92.188% \n",
      "[epoch:15, iter:1349] Loss: 0.111 | Acc: 93.750% \n",
      "[epoch:15, iter:1350] Loss: 0.096 | Acc: 94.792% \n",
      "[epoch:15, iter:1351] Loss: 0.089 | Acc: 95.536% \n",
      "[epoch:15, iter:1352] Loss: 0.087 | Acc: 96.094% \n",
      "[epoch:15, iter:1353] Loss: 0.108 | Acc: 95.139% \n",
      "[epoch:15, iter:1354] Loss: 0.105 | Acc: 95.000% \n",
      "[epoch:15, iter:1355] Loss: 0.100 | Acc: 95.455% \n",
      "[epoch:15, iter:1356] Loss: 0.092 | Acc: 95.833% \n",
      "[epoch:15, iter:1357] Loss: 0.088 | Acc: 96.154% \n",
      "[epoch:15, iter:1358] Loss: 0.100 | Acc: 95.536% \n",
      "[epoch:15, iter:1359] Loss: 0.109 | Acc: 95.417% \n",
      "[epoch:15, iter:1360] Loss: 0.106 | Acc: 95.703% \n",
      "[epoch:15, iter:1361] Loss: 0.104 | Acc: 95.956% \n",
      "[epoch:15, iter:1362] Loss: 0.101 | Acc: 95.833% \n",
      "[epoch:15, iter:1363] Loss: 0.115 | Acc: 95.395% \n",
      "[epoch:15, iter:1364] Loss: 0.112 | Acc: 95.625% \n",
      "[epoch:15, iter:1365] Loss: 0.107 | Acc: 95.833% \n",
      "[epoch:15, iter:1366] Loss: 0.105 | Acc: 96.023% \n",
      "[epoch:15, iter:1367] Loss: 0.103 | Acc: 96.196% \n",
      "[epoch:15, iter:1368] Loss: 0.099 | Acc: 96.354% \n",
      "[epoch:15, iter:1369] Loss: 0.105 | Acc: 96.250% \n",
      "[epoch:15, iter:1370] Loss: 0.102 | Acc: 96.394% \n",
      "[epoch:15, iter:1371] Loss: 0.100 | Acc: 96.528% \n",
      "[epoch:15, iter:1372] Loss: 0.101 | Acc: 96.429% \n",
      "[epoch:15, iter:1373] Loss: 0.098 | Acc: 96.552% \n",
      "[epoch:15, iter:1374] Loss: 0.095 | Acc: 96.667% \n",
      "[epoch:15, iter:1375] Loss: 0.106 | Acc: 96.169% \n",
      "[epoch:15, iter:1376] Loss: 0.107 | Acc: 96.094% \n",
      "[epoch:15, iter:1377] Loss: 0.104 | Acc: 96.212% \n",
      "[epoch:15, iter:1378] Loss: 0.109 | Acc: 96.140% \n",
      "[epoch:15, iter:1379] Loss: 0.112 | Acc: 95.893% \n",
      "[epoch:15, iter:1380] Loss: 0.110 | Acc: 96.007% \n",
      "[epoch:15, iter:1381] Loss: 0.107 | Acc: 96.115% \n",
      "[epoch:15, iter:1382] Loss: 0.105 | Acc: 96.217% \n",
      "[epoch:15, iter:1383] Loss: 0.104 | Acc: 96.314% \n",
      "[epoch:15, iter:1384] Loss: 0.103 | Acc: 96.250% \n",
      "[epoch:15, iter:1385] Loss: 0.102 | Acc: 96.341% \n",
      "[epoch:15, iter:1386] Loss: 0.100 | Acc: 96.429% \n",
      "[epoch:15, iter:1387] Loss: 0.098 | Acc: 96.512% \n",
      "[epoch:15, iter:1388] Loss: 0.097 | Acc: 96.591% \n",
      "[epoch:15, iter:1389] Loss: 0.102 | Acc: 96.389% \n",
      "[epoch:15, iter:1390] Loss: 0.101 | Acc: 96.467% \n",
      "[epoch:15, iter:1391] Loss: 0.100 | Acc: 96.410% \n",
      "[epoch:15, iter:1392] Loss: 0.099 | Acc: 96.484% \n",
      "[epoch:15, iter:1393] Loss: 0.098 | Acc: 96.556% \n",
      "[epoch:15, iter:1394] Loss: 0.106 | Acc: 96.250% \n",
      "[epoch:15, iter:1395] Loss: 0.109 | Acc: 96.201% \n",
      "[epoch:15, iter:1396] Loss: 0.108 | Acc: 96.274% \n",
      "[epoch:15, iter:1397] Loss: 0.106 | Acc: 96.344% \n",
      "[epoch:15, iter:1398] Loss: 0.105 | Acc: 96.412% \n",
      "[epoch:15, iter:1399] Loss: 0.103 | Acc: 96.477% \n",
      "[epoch:15, iter:1400] Loss: 0.103 | Acc: 96.429% \n",
      "[epoch:15, iter:1401] Loss: 0.102 | Acc: 96.491% \n",
      "[epoch:15, iter:1402] Loss: 0.101 | Acc: 96.552% \n",
      "[epoch:15, iter:1403] Loss: 0.103 | Acc: 96.398% \n",
      "[epoch:15, iter:1404] Loss: 0.103 | Acc: 96.354% \n",
      "[epoch:15, iter:1405] Loss: 0.109 | Acc: 96.311% \n",
      "[epoch:15, iter:1406] Loss: 0.107 | Acc: 96.371% \n",
      "[epoch:15, iter:1407] Loss: 0.108 | Acc: 96.230% \n",
      "[epoch:15, iter:1408] Loss: 0.110 | Acc: 96.191% \n",
      "[epoch:15, iter:1409] Loss: 0.109 | Acc: 96.250% \n",
      "[epoch:15, iter:1410] Loss: 0.108 | Acc: 96.307% \n",
      "[epoch:15, iter:1411] Loss: 0.106 | Acc: 96.362% \n",
      "[epoch:15, iter:1412] Loss: 0.105 | Acc: 96.415% \n",
      "[epoch:15, iter:1413] Loss: 0.104 | Acc: 96.467% \n",
      "[epoch:15, iter:1414] Loss: 0.103 | Acc: 96.518% \n",
      "[epoch:15, iter:1415] Loss: 0.103 | Acc: 96.479% \n",
      "[epoch:15, iter:1416] Loss: 0.106 | Acc: 96.354% \n",
      "[epoch:15, iter:1417] Loss: 0.109 | Acc: 96.318% \n",
      "[epoch:15, iter:1418] Loss: 0.108 | Acc: 96.368% \n",
      "[epoch:15, iter:1419] Loss: 0.108 | Acc: 96.333% \n",
      "[epoch:15, iter:1420] Loss: 0.110 | Acc: 96.299% \n",
      "[epoch:15, iter:1421] Loss: 0.113 | Acc: 96.104% \n",
      "[epoch:15, iter:1422] Loss: 0.114 | Acc: 96.074% \n",
      "[epoch:15, iter:1423] Loss: 0.113 | Acc: 96.123% \n",
      "[epoch:15, iter:1424] Loss: 0.112 | Acc: 96.172% \n",
      "[epoch:15, iter:1425] Loss: 0.111 | Acc: 96.219% \n",
      "[epoch:15, iter:1426] Loss: 0.112 | Acc: 96.113% \n",
      "[epoch:15, iter:1427] Loss: 0.112 | Acc: 96.084% \n",
      "[epoch:15, iter:1428] Loss: 0.112 | Acc: 96.131% \n",
      "[epoch:15, iter:1429] Loss: 0.112 | Acc: 96.103% \n",
      "[epoch:15, iter:1430] Loss: 0.111 | Acc: 96.148% \n",
      "[epoch:15, iter:1431] Loss: 0.111 | Acc: 96.121% \n",
      "[epoch:15, iter:1432] Loss: 0.114 | Acc: 95.952% \n",
      "[epoch:15, iter:1433] Loss: 0.117 | Acc: 95.927% \n",
      "[epoch:15, iter:1434] Loss: 0.118 | Acc: 95.903% \n",
      "[epoch:15, iter:1435] Loss: 0.117 | Acc: 95.948% \n",
      "[epoch:15, iter:1436] Loss: 0.116 | Acc: 95.992% \n",
      "[epoch:15, iter:1437] Loss: 0.116 | Acc: 96.035% \n",
      "[epoch:15, iter:1438] Loss: 0.115 | Acc: 96.077% \n",
      "[epoch:15, iter:1439] Loss: 0.114 | Acc: 96.118% \n",
      "[epoch:15, iter:1440] Loss: 0.113 | Acc: 96.141% \n",
      "Waiting Test!\n",
      "测试分类准确率为：77.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 16\n",
      "[epoch:16, iter:1441] Loss: 0.083 | Acc: 100.000% \n",
      "[epoch:16, iter:1442] Loss: 0.086 | Acc: 96.875% \n",
      "[epoch:16, iter:1443] Loss: 0.108 | Acc: 93.750% \n",
      "[epoch:16, iter:1444] Loss: 0.083 | Acc: 95.312% \n",
      "[epoch:16, iter:1445] Loss: 0.071 | Acc: 96.250% \n",
      "[epoch:16, iter:1446] Loss: 0.061 | Acc: 96.875% \n",
      "[epoch:16, iter:1447] Loss: 0.070 | Acc: 96.429% \n",
      "[epoch:16, iter:1448] Loss: 0.065 | Acc: 96.875% \n",
      "[epoch:16, iter:1449] Loss: 0.060 | Acc: 97.222% \n",
      "[epoch:16, iter:1450] Loss: 0.056 | Acc: 97.500% \n",
      "[epoch:16, iter:1451] Loss: 0.061 | Acc: 97.159% \n",
      "[epoch:16, iter:1452] Loss: 0.056 | Acc: 97.396% \n",
      "[epoch:16, iter:1453] Loss: 0.082 | Acc: 96.635% \n",
      "[epoch:16, iter:1454] Loss: 0.077 | Acc: 96.875% \n",
      "[epoch:16, iter:1455] Loss: 0.072 | Acc: 97.083% \n",
      "[epoch:16, iter:1456] Loss: 0.071 | Acc: 97.266% \n",
      "[epoch:16, iter:1457] Loss: 0.079 | Acc: 97.059% \n",
      "[epoch:16, iter:1458] Loss: 0.078 | Acc: 96.875% \n",
      "[epoch:16, iter:1459] Loss: 0.074 | Acc: 97.039% \n",
      "[epoch:16, iter:1460] Loss: 0.079 | Acc: 96.875% \n",
      "[epoch:16, iter:1461] Loss: 0.091 | Acc: 96.726% \n",
      "[epoch:16, iter:1462] Loss: 0.091 | Acc: 96.875% \n",
      "[epoch:16, iter:1463] Loss: 0.088 | Acc: 97.011% \n",
      "[epoch:16, iter:1464] Loss: 0.084 | Acc: 97.135% \n",
      "[epoch:16, iter:1465] Loss: 0.084 | Acc: 97.000% \n",
      "[epoch:16, iter:1466] Loss: 0.081 | Acc: 97.115% \n",
      "[epoch:16, iter:1467] Loss: 0.078 | Acc: 97.222% \n",
      "[epoch:16, iter:1468] Loss: 0.078 | Acc: 97.098% \n",
      "[epoch:16, iter:1469] Loss: 0.076 | Acc: 97.198% \n",
      "[epoch:16, iter:1470] Loss: 0.082 | Acc: 96.875% \n",
      "[epoch:16, iter:1471] Loss: 0.080 | Acc: 96.976% \n",
      "[epoch:16, iter:1472] Loss: 0.077 | Acc: 97.070% \n",
      "[epoch:16, iter:1473] Loss: 0.085 | Acc: 96.970% \n",
      "[epoch:16, iter:1474] Loss: 0.084 | Acc: 97.059% \n",
      "[epoch:16, iter:1475] Loss: 0.083 | Acc: 96.964% \n",
      "[epoch:16, iter:1476] Loss: 0.084 | Acc: 96.875% \n",
      "[epoch:16, iter:1477] Loss: 0.090 | Acc: 96.791% \n",
      "[epoch:16, iter:1478] Loss: 0.092 | Acc: 96.711% \n",
      "[epoch:16, iter:1479] Loss: 0.091 | Acc: 96.795% \n",
      "[epoch:16, iter:1480] Loss: 0.090 | Acc: 96.875% \n",
      "[epoch:16, iter:1481] Loss: 0.090 | Acc: 96.799% \n",
      "[epoch:16, iter:1482] Loss: 0.089 | Acc: 96.875% \n",
      "[epoch:16, iter:1483] Loss: 0.088 | Acc: 96.948% \n",
      "[epoch:16, iter:1484] Loss: 0.088 | Acc: 96.875% \n",
      "[epoch:16, iter:1485] Loss: 0.089 | Acc: 96.806% \n",
      "[epoch:16, iter:1486] Loss: 0.087 | Acc: 96.875% \n",
      "[epoch:16, iter:1487] Loss: 0.086 | Acc: 96.941% \n",
      "[epoch:16, iter:1488] Loss: 0.087 | Acc: 97.005% \n",
      "[epoch:16, iter:1489] Loss: 0.088 | Acc: 96.939% \n",
      "[epoch:16, iter:1490] Loss: 0.086 | Acc: 97.000% \n",
      "[epoch:16, iter:1491] Loss: 0.085 | Acc: 97.059% \n",
      "[epoch:16, iter:1492] Loss: 0.085 | Acc: 96.995% \n",
      "[epoch:16, iter:1493] Loss: 0.084 | Acc: 97.052% \n",
      "[epoch:16, iter:1494] Loss: 0.084 | Acc: 96.991% \n",
      "[epoch:16, iter:1495] Loss: 0.083 | Acc: 97.045% \n",
      "[epoch:16, iter:1496] Loss: 0.082 | Acc: 97.098% \n",
      "[epoch:16, iter:1497] Loss: 0.080 | Acc: 97.149% \n",
      "[epoch:16, iter:1498] Loss: 0.084 | Acc: 96.983% \n",
      "[epoch:16, iter:1499] Loss: 0.088 | Acc: 96.822% \n",
      "[epoch:16, iter:1500] Loss: 0.088 | Acc: 96.771% \n",
      "[epoch:16, iter:1501] Loss: 0.087 | Acc: 96.824% \n",
      "[epoch:16, iter:1502] Loss: 0.087 | Acc: 96.774% \n",
      "[epoch:16, iter:1503] Loss: 0.086 | Acc: 96.825% \n",
      "[epoch:16, iter:1504] Loss: 0.085 | Acc: 96.875% \n",
      "[epoch:16, iter:1505] Loss: 0.084 | Acc: 96.923% \n",
      "[epoch:16, iter:1506] Loss: 0.084 | Acc: 96.875% \n",
      "[epoch:16, iter:1507] Loss: 0.085 | Acc: 96.828% \n",
      "[epoch:16, iter:1508] Loss: 0.084 | Acc: 96.875% \n",
      "[epoch:16, iter:1509] Loss: 0.083 | Acc: 96.920% \n",
      "[epoch:16, iter:1510] Loss: 0.082 | Acc: 96.964% \n",
      "[epoch:16, iter:1511] Loss: 0.088 | Acc: 96.831% \n",
      "[epoch:16, iter:1512] Loss: 0.088 | Acc: 96.875% \n",
      "[epoch:16, iter:1513] Loss: 0.087 | Acc: 96.918% \n",
      "[epoch:16, iter:1514] Loss: 0.086 | Acc: 96.959% \n",
      "[epoch:16, iter:1515] Loss: 0.085 | Acc: 97.000% \n",
      "[epoch:16, iter:1516] Loss: 0.085 | Acc: 97.039% \n",
      "[epoch:16, iter:1517] Loss: 0.084 | Acc: 97.078% \n",
      "[epoch:16, iter:1518] Loss: 0.083 | Acc: 97.115% \n",
      "[epoch:16, iter:1519] Loss: 0.085 | Acc: 96.915% \n",
      "[epoch:16, iter:1520] Loss: 0.084 | Acc: 96.953% \n",
      "[epoch:16, iter:1521] Loss: 0.084 | Acc: 96.914% \n",
      "[epoch:16, iter:1522] Loss: 0.085 | Acc: 96.799% \n",
      "[epoch:16, iter:1523] Loss: 0.086 | Acc: 96.762% \n",
      "[epoch:16, iter:1524] Loss: 0.087 | Acc: 96.726% \n",
      "[epoch:16, iter:1525] Loss: 0.086 | Acc: 96.765% \n",
      "[epoch:16, iter:1526] Loss: 0.085 | Acc: 96.802% \n",
      "[epoch:16, iter:1527] Loss: 0.085 | Acc: 96.767% \n",
      "[epoch:16, iter:1528] Loss: 0.084 | Acc: 96.804% \n",
      "[epoch:16, iter:1529] Loss: 0.084 | Acc: 96.840% \n",
      "[epoch:16, iter:1530] Loss: 0.083 | Acc: 96.875% \n",
      "[epoch:16, iter:1531] Loss: 0.083 | Acc: 96.909% \n",
      "[epoch:16, iter:1532] Loss: 0.082 | Acc: 96.943% \n",
      "[epoch:16, iter:1533] Loss: 0.082 | Acc: 96.976% \n",
      "[epoch:16, iter:1534] Loss: 0.081 | Acc: 97.008% \n",
      "[epoch:16, iter:1535] Loss: 0.083 | Acc: 96.842% \n",
      "[epoch:16, iter:1536] Loss: 0.082 | Acc: 96.861% \n",
      "Waiting Test!\n",
      "测试分类准确率为：87.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 17\n",
      "[epoch:17, iter:1537] Loss: 0.005 | Acc: 100.000% \n",
      "[epoch:17, iter:1538] Loss: 0.162 | Acc: 93.750% \n",
      "[epoch:17, iter:1539] Loss: 0.108 | Acc: 95.833% \n",
      "[epoch:17, iter:1540] Loss: 0.182 | Acc: 90.625% \n",
      "[epoch:17, iter:1541] Loss: 0.163 | Acc: 91.250% \n",
      "[epoch:17, iter:1542] Loss: 0.140 | Acc: 92.708% \n",
      "[epoch:17, iter:1543] Loss: 0.121 | Acc: 93.750% \n",
      "[epoch:17, iter:1544] Loss: 0.119 | Acc: 93.750% \n",
      "[epoch:17, iter:1545] Loss: 0.111 | Acc: 94.444% \n",
      "[epoch:17, iter:1546] Loss: 0.123 | Acc: 93.750% \n",
      "[epoch:17, iter:1547] Loss: 0.113 | Acc: 94.318% \n",
      "[epoch:17, iter:1548] Loss: 0.105 | Acc: 94.792% \n",
      "[epoch:17, iter:1549] Loss: 0.100 | Acc: 95.192% \n",
      "[epoch:17, iter:1550] Loss: 0.095 | Acc: 95.536% \n",
      "[epoch:17, iter:1551] Loss: 0.089 | Acc: 95.833% \n",
      "[epoch:17, iter:1552] Loss: 0.098 | Acc: 95.703% \n",
      "[epoch:17, iter:1553] Loss: 0.099 | Acc: 95.221% \n",
      "[epoch:17, iter:1554] Loss: 0.096 | Acc: 95.486% \n",
      "[epoch:17, iter:1555] Loss: 0.092 | Acc: 95.724% \n",
      "[epoch:17, iter:1556] Loss: 0.088 | Acc: 95.938% \n",
      "[epoch:17, iter:1557] Loss: 0.084 | Acc: 96.131% \n",
      "[epoch:17, iter:1558] Loss: 0.083 | Acc: 96.307% \n",
      "[epoch:17, iter:1559] Loss: 0.081 | Acc: 96.467% \n",
      "[epoch:17, iter:1560] Loss: 0.080 | Acc: 96.615% \n",
      "[epoch:17, iter:1561] Loss: 0.078 | Acc: 96.750% \n",
      "[epoch:17, iter:1562] Loss: 0.075 | Acc: 96.875% \n",
      "[epoch:17, iter:1563] Loss: 0.074 | Acc: 96.991% \n",
      "[epoch:17, iter:1564] Loss: 0.086 | Acc: 96.429% \n",
      "[epoch:17, iter:1565] Loss: 0.085 | Acc: 96.552% \n",
      "[epoch:17, iter:1566] Loss: 0.083 | Acc: 96.667% \n",
      "[epoch:17, iter:1567] Loss: 0.081 | Acc: 96.774% \n",
      "[epoch:17, iter:1568] Loss: 0.081 | Acc: 96.680% \n",
      "[epoch:17, iter:1569] Loss: 0.081 | Acc: 96.591% \n",
      "[epoch:17, iter:1570] Loss: 0.079 | Acc: 96.691% \n",
      "[epoch:17, iter:1571] Loss: 0.077 | Acc: 96.786% \n",
      "[epoch:17, iter:1572] Loss: 0.075 | Acc: 96.875% \n",
      "[epoch:17, iter:1573] Loss: 0.078 | Acc: 96.791% \n",
      "[epoch:17, iter:1574] Loss: 0.076 | Acc: 96.875% \n",
      "[epoch:17, iter:1575] Loss: 0.074 | Acc: 96.955% \n",
      "[epoch:17, iter:1576] Loss: 0.075 | Acc: 96.875% \n",
      "[epoch:17, iter:1577] Loss: 0.079 | Acc: 96.799% \n",
      "[epoch:17, iter:1578] Loss: 0.077 | Acc: 96.875% \n",
      "[epoch:17, iter:1579] Loss: 0.076 | Acc: 96.948% \n",
      "[epoch:17, iter:1580] Loss: 0.078 | Acc: 96.733% \n",
      "[epoch:17, iter:1581] Loss: 0.081 | Acc: 96.528% \n",
      "[epoch:17, iter:1582] Loss: 0.079 | Acc: 96.603% \n",
      "[epoch:17, iter:1583] Loss: 0.083 | Acc: 96.543% \n",
      "[epoch:17, iter:1584] Loss: 0.081 | Acc: 96.615% \n",
      "[epoch:17, iter:1585] Loss: 0.083 | Acc: 96.429% \n",
      "[epoch:17, iter:1586] Loss: 0.082 | Acc: 96.500% \n",
      "[epoch:17, iter:1587] Loss: 0.081 | Acc: 96.569% \n",
      "[epoch:17, iter:1588] Loss: 0.080 | Acc: 96.635% \n",
      "[epoch:17, iter:1589] Loss: 0.078 | Acc: 96.698% \n",
      "[epoch:17, iter:1590] Loss: 0.077 | Acc: 96.759% \n",
      "[epoch:17, iter:1591] Loss: 0.081 | Acc: 96.705% \n",
      "[epoch:17, iter:1592] Loss: 0.080 | Acc: 96.652% \n",
      "[epoch:17, iter:1593] Loss: 0.079 | Acc: 96.711% \n",
      "[epoch:17, iter:1594] Loss: 0.081 | Acc: 96.552% \n",
      "[epoch:17, iter:1595] Loss: 0.081 | Acc: 96.610% \n",
      "[epoch:17, iter:1596] Loss: 0.081 | Acc: 96.562% \n",
      "[epoch:17, iter:1597] Loss: 0.081 | Acc: 96.516% \n",
      "[epoch:17, iter:1598] Loss: 0.080 | Acc: 96.573% \n",
      "[epoch:17, iter:1599] Loss: 0.081 | Acc: 96.528% \n",
      "[epoch:17, iter:1600] Loss: 0.082 | Acc: 96.484% \n",
      "[epoch:17, iter:1601] Loss: 0.081 | Acc: 96.538% \n",
      "[epoch:17, iter:1602] Loss: 0.080 | Acc: 96.591% \n",
      "[epoch:17, iter:1603] Loss: 0.080 | Acc: 96.549% \n",
      "[epoch:17, iter:1604] Loss: 0.083 | Acc: 96.507% \n",
      "[epoch:17, iter:1605] Loss: 0.082 | Acc: 96.467% \n",
      "[epoch:17, iter:1606] Loss: 0.089 | Acc: 96.339% \n",
      "[epoch:17, iter:1607] Loss: 0.089 | Acc: 96.303% \n",
      "[epoch:17, iter:1608] Loss: 0.089 | Acc: 96.267% \n",
      "[epoch:17, iter:1609] Loss: 0.089 | Acc: 96.318% \n",
      "[epoch:17, iter:1610] Loss: 0.088 | Acc: 96.368% \n",
      "[epoch:17, iter:1611] Loss: 0.087 | Acc: 96.417% \n",
      "[epoch:17, iter:1612] Loss: 0.093 | Acc: 96.382% \n",
      "[epoch:17, iter:1613] Loss: 0.092 | Acc: 96.429% \n",
      "[epoch:17, iter:1614] Loss: 0.094 | Acc: 96.314% \n",
      "[epoch:17, iter:1615] Loss: 0.093 | Acc: 96.361% \n",
      "[epoch:17, iter:1616] Loss: 0.092 | Acc: 96.406% \n",
      "[epoch:17, iter:1617] Loss: 0.092 | Acc: 96.373% \n",
      "[epoch:17, iter:1618] Loss: 0.094 | Acc: 96.341% \n",
      "[epoch:17, iter:1619] Loss: 0.094 | Acc: 96.310% \n",
      "[epoch:17, iter:1620] Loss: 0.095 | Acc: 96.280% \n",
      "[epoch:17, iter:1621] Loss: 0.094 | Acc: 96.324% \n",
      "[epoch:17, iter:1622] Loss: 0.093 | Acc: 96.366% \n",
      "[epoch:17, iter:1623] Loss: 0.093 | Acc: 96.408% \n",
      "[epoch:17, iter:1624] Loss: 0.093 | Acc: 96.378% \n",
      "[epoch:17, iter:1625] Loss: 0.093 | Acc: 96.419% \n",
      "[epoch:17, iter:1626] Loss: 0.092 | Acc: 96.458% \n",
      "[epoch:17, iter:1627] Loss: 0.093 | Acc: 96.360% \n",
      "[epoch:17, iter:1628] Loss: 0.093 | Acc: 96.332% \n",
      "[epoch:17, iter:1629] Loss: 0.093 | Acc: 96.304% \n",
      "[epoch:17, iter:1630] Loss: 0.094 | Acc: 96.144% \n",
      "[epoch:17, iter:1631] Loss: 0.095 | Acc: 96.053% \n",
      "[epoch:17, iter:1632] Loss: 0.094 | Acc: 96.076% \n",
      "Waiting Test!\n",
      "测试分类准确率为：79.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 18\n",
      "[epoch:18, iter:1633] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:18, iter:1634] Loss: 0.030 | Acc: 100.000% \n",
      "[epoch:18, iter:1635] Loss: 0.033 | Acc: 100.000% \n",
      "[epoch:18, iter:1636] Loss: 0.098 | Acc: 96.875% \n",
      "[epoch:18, iter:1637] Loss: 0.109 | Acc: 96.250% \n",
      "[epoch:18, iter:1638] Loss: 0.112 | Acc: 96.875% \n",
      "[epoch:18, iter:1639] Loss: 0.098 | Acc: 97.321% \n",
      "[epoch:18, iter:1640] Loss: 0.090 | Acc: 97.656% \n",
      "[epoch:18, iter:1641] Loss: 0.082 | Acc: 97.917% \n",
      "[epoch:18, iter:1642] Loss: 0.099 | Acc: 97.500% \n",
      "[epoch:18, iter:1643] Loss: 0.090 | Acc: 97.727% \n",
      "[epoch:18, iter:1644] Loss: 0.101 | Acc: 97.396% \n",
      "[epoch:18, iter:1645] Loss: 0.094 | Acc: 97.596% \n",
      "[epoch:18, iter:1646] Loss: 0.092 | Acc: 97.321% \n",
      "[epoch:18, iter:1647] Loss: 0.089 | Acc: 97.500% \n",
      "[epoch:18, iter:1648] Loss: 0.084 | Acc: 97.656% \n",
      "[epoch:18, iter:1649] Loss: 0.089 | Acc: 97.059% \n",
      "[epoch:18, iter:1650] Loss: 0.084 | Acc: 97.222% \n",
      "[epoch:18, iter:1651] Loss: 0.082 | Acc: 97.368% \n",
      "[epoch:18, iter:1652] Loss: 0.078 | Acc: 97.500% \n",
      "[epoch:18, iter:1653] Loss: 0.078 | Acc: 97.619% \n",
      "[epoch:18, iter:1654] Loss: 0.081 | Acc: 97.443% \n",
      "[epoch:18, iter:1655] Loss: 0.078 | Acc: 97.554% \n",
      "[epoch:18, iter:1656] Loss: 0.077 | Acc: 97.396% \n",
      "[epoch:18, iter:1657] Loss: 0.081 | Acc: 97.250% \n",
      "[epoch:18, iter:1658] Loss: 0.085 | Acc: 96.875% \n",
      "[epoch:18, iter:1659] Loss: 0.085 | Acc: 96.759% \n",
      "[epoch:18, iter:1660] Loss: 0.095 | Acc: 96.205% \n",
      "[epoch:18, iter:1661] Loss: 0.092 | Acc: 96.336% \n",
      "[epoch:18, iter:1662] Loss: 0.089 | Acc: 96.458% \n",
      "[epoch:18, iter:1663] Loss: 0.087 | Acc: 96.573% \n",
      "[epoch:18, iter:1664] Loss: 0.090 | Acc: 96.484% \n",
      "[epoch:18, iter:1665] Loss: 0.088 | Acc: 96.591% \n",
      "[epoch:18, iter:1666] Loss: 0.086 | Acc: 96.691% \n",
      "[epoch:18, iter:1667] Loss: 0.085 | Acc: 96.607% \n",
      "[epoch:18, iter:1668] Loss: 0.083 | Acc: 96.701% \n",
      "[epoch:18, iter:1669] Loss: 0.081 | Acc: 96.791% \n",
      "[epoch:18, iter:1670] Loss: 0.080 | Acc: 96.875% \n",
      "[epoch:18, iter:1671] Loss: 0.082 | Acc: 96.795% \n",
      "[epoch:18, iter:1672] Loss: 0.081 | Acc: 96.875% \n",
      "[epoch:18, iter:1673] Loss: 0.079 | Acc: 96.951% \n",
      "[epoch:18, iter:1674] Loss: 0.078 | Acc: 97.024% \n",
      "[epoch:18, iter:1675] Loss: 0.077 | Acc: 97.093% \n",
      "[epoch:18, iter:1676] Loss: 0.076 | Acc: 97.159% \n",
      "[epoch:18, iter:1677] Loss: 0.076 | Acc: 97.083% \n",
      "[epoch:18, iter:1678] Loss: 0.076 | Acc: 97.147% \n",
      "[epoch:18, iter:1679] Loss: 0.075 | Acc: 97.207% \n",
      "[epoch:18, iter:1680] Loss: 0.074 | Acc: 97.266% \n",
      "[epoch:18, iter:1681] Loss: 0.075 | Acc: 97.321% \n",
      "[epoch:18, iter:1682] Loss: 0.074 | Acc: 97.250% \n",
      "[epoch:18, iter:1683] Loss: 0.073 | Acc: 97.304% \n",
      "[epoch:18, iter:1684] Loss: 0.075 | Acc: 97.236% \n",
      "[epoch:18, iter:1685] Loss: 0.074 | Acc: 97.288% \n",
      "[epoch:18, iter:1686] Loss: 0.076 | Acc: 96.991% \n",
      "[epoch:18, iter:1687] Loss: 0.075 | Acc: 97.045% \n",
      "[epoch:18, iter:1688] Loss: 0.079 | Acc: 96.987% \n",
      "[epoch:18, iter:1689] Loss: 0.080 | Acc: 96.930% \n",
      "[epoch:18, iter:1690] Loss: 0.082 | Acc: 96.767% \n",
      "[epoch:18, iter:1691] Loss: 0.080 | Acc: 96.822% \n",
      "[epoch:18, iter:1692] Loss: 0.079 | Acc: 96.875% \n",
      "[epoch:18, iter:1693] Loss: 0.081 | Acc: 96.824% \n",
      "[epoch:18, iter:1694] Loss: 0.090 | Acc: 96.472% \n",
      "[epoch:18, iter:1695] Loss: 0.089 | Acc: 96.528% \n",
      "[epoch:18, iter:1696] Loss: 0.088 | Acc: 96.582% \n",
      "[epoch:18, iter:1697] Loss: 0.086 | Acc: 96.635% \n",
      "[epoch:18, iter:1698] Loss: 0.085 | Acc: 96.686% \n",
      "[epoch:18, iter:1699] Loss: 0.084 | Acc: 96.735% \n",
      "[epoch:18, iter:1700] Loss: 0.086 | Acc: 96.599% \n",
      "[epoch:18, iter:1701] Loss: 0.085 | Acc: 96.649% \n",
      "[epoch:18, iter:1702] Loss: 0.085 | Acc: 96.607% \n",
      "[epoch:18, iter:1703] Loss: 0.084 | Acc: 96.655% \n",
      "[epoch:18, iter:1704] Loss: 0.084 | Acc: 96.701% \n",
      "[epoch:18, iter:1705] Loss: 0.083 | Acc: 96.747% \n",
      "[epoch:18, iter:1706] Loss: 0.082 | Acc: 96.706% \n",
      "[epoch:18, iter:1707] Loss: 0.083 | Acc: 96.667% \n",
      "[epoch:18, iter:1708] Loss: 0.086 | Acc: 96.546% \n",
      "[epoch:18, iter:1709] Loss: 0.085 | Acc: 96.591% \n",
      "[epoch:18, iter:1710] Loss: 0.084 | Acc: 96.635% \n",
      "[epoch:18, iter:1711] Loss: 0.084 | Acc: 96.677% \n",
      "[epoch:18, iter:1712] Loss: 0.083 | Acc: 96.719% \n",
      "[epoch:18, iter:1713] Loss: 0.086 | Acc: 96.682% \n",
      "[epoch:18, iter:1714] Loss: 0.089 | Acc: 96.646% \n",
      "[epoch:18, iter:1715] Loss: 0.090 | Acc: 96.536% \n",
      "[epoch:18, iter:1716] Loss: 0.093 | Acc: 96.429% \n",
      "[epoch:18, iter:1717] Loss: 0.092 | Acc: 96.471% \n",
      "[epoch:18, iter:1718] Loss: 0.092 | Acc: 96.439% \n",
      "[epoch:18, iter:1719] Loss: 0.094 | Acc: 96.336% \n",
      "[epoch:18, iter:1720] Loss: 0.093 | Acc: 96.378% \n",
      "[epoch:18, iter:1721] Loss: 0.092 | Acc: 96.419% \n",
      "[epoch:18, iter:1722] Loss: 0.092 | Acc: 96.458% \n",
      "[epoch:18, iter:1723] Loss: 0.091 | Acc: 96.497% \n",
      "[epoch:18, iter:1724] Loss: 0.094 | Acc: 96.399% \n",
      "[epoch:18, iter:1725] Loss: 0.095 | Acc: 96.371% \n",
      "[epoch:18, iter:1726] Loss: 0.097 | Acc: 96.210% \n",
      "[epoch:18, iter:1727] Loss: 0.096 | Acc: 96.250% \n",
      "[epoch:18, iter:1728] Loss: 0.099 | Acc: 96.207% \n",
      "Waiting Test!\n",
      "测试分类准确率为：76.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 19\n",
      "[epoch:19, iter:1729] Loss: 0.019 | Acc: 100.000% \n",
      "[epoch:19, iter:1730] Loss: 0.025 | Acc: 100.000% \n",
      "[epoch:19, iter:1731] Loss: 0.036 | Acc: 100.000% \n",
      "[epoch:19, iter:1732] Loss: 0.059 | Acc: 98.438% \n",
      "[epoch:19, iter:1733] Loss: 0.052 | Acc: 98.750% \n",
      "[epoch:19, iter:1734] Loss: 0.047 | Acc: 98.958% \n",
      "[epoch:19, iter:1735] Loss: 0.059 | Acc: 98.214% \n",
      "[epoch:19, iter:1736] Loss: 0.057 | Acc: 98.438% \n",
      "[epoch:19, iter:1737] Loss: 0.068 | Acc: 97.222% \n",
      "[epoch:19, iter:1738] Loss: 0.064 | Acc: 97.500% \n",
      "[epoch:19, iter:1739] Loss: 0.074 | Acc: 96.591% \n",
      "[epoch:19, iter:1740] Loss: 0.074 | Acc: 96.875% \n",
      "[epoch:19, iter:1741] Loss: 0.094 | Acc: 96.154% \n",
      "[epoch:19, iter:1742] Loss: 0.088 | Acc: 96.429% \n",
      "[epoch:19, iter:1743] Loss: 0.090 | Acc: 96.667% \n",
      "[epoch:19, iter:1744] Loss: 0.089 | Acc: 96.484% \n",
      "[epoch:19, iter:1745] Loss: 0.086 | Acc: 96.691% \n",
      "[epoch:19, iter:1746] Loss: 0.087 | Acc: 96.528% \n",
      "[epoch:19, iter:1747] Loss: 0.085 | Acc: 96.711% \n",
      "[epoch:19, iter:1748] Loss: 0.091 | Acc: 96.250% \n",
      "[epoch:19, iter:1749] Loss: 0.092 | Acc: 96.131% \n",
      "[epoch:19, iter:1750] Loss: 0.092 | Acc: 96.023% \n",
      "[epoch:19, iter:1751] Loss: 0.108 | Acc: 95.652% \n",
      "[epoch:19, iter:1752] Loss: 0.104 | Acc: 95.833% \n",
      "[epoch:19, iter:1753] Loss: 0.105 | Acc: 95.750% \n",
      "[epoch:19, iter:1754] Loss: 0.102 | Acc: 95.913% \n",
      "[epoch:19, iter:1755] Loss: 0.100 | Acc: 96.065% \n",
      "[epoch:19, iter:1756] Loss: 0.097 | Acc: 96.205% \n",
      "[epoch:19, iter:1757] Loss: 0.102 | Acc: 96.121% \n",
      "[epoch:19, iter:1758] Loss: 0.100 | Acc: 96.042% \n",
      "[epoch:19, iter:1759] Loss: 0.103 | Acc: 95.766% \n",
      "[epoch:19, iter:1760] Loss: 0.101 | Acc: 95.703% \n",
      "[epoch:19, iter:1761] Loss: 0.101 | Acc: 95.644% \n",
      "[epoch:19, iter:1762] Loss: 0.101 | Acc: 95.588% \n",
      "[epoch:19, iter:1763] Loss: 0.101 | Acc: 95.536% \n",
      "[epoch:19, iter:1764] Loss: 0.099 | Acc: 95.660% \n",
      "[epoch:19, iter:1765] Loss: 0.099 | Acc: 95.608% \n",
      "[epoch:19, iter:1766] Loss: 0.114 | Acc: 94.901% \n",
      "[epoch:19, iter:1767] Loss: 0.111 | Acc: 95.032% \n",
      "[epoch:19, iter:1768] Loss: 0.110 | Acc: 95.156% \n",
      "[epoch:19, iter:1769] Loss: 0.118 | Acc: 94.970% \n",
      "[epoch:19, iter:1770] Loss: 0.116 | Acc: 95.089% \n",
      "[epoch:19, iter:1771] Loss: 0.113 | Acc: 95.203% \n",
      "[epoch:19, iter:1772] Loss: 0.111 | Acc: 95.312% \n",
      "[epoch:19, iter:1773] Loss: 0.109 | Acc: 95.417% \n",
      "[epoch:19, iter:1774] Loss: 0.107 | Acc: 95.516% \n",
      "[epoch:19, iter:1775] Loss: 0.108 | Acc: 95.479% \n",
      "[epoch:19, iter:1776] Loss: 0.106 | Acc: 95.573% \n",
      "[epoch:19, iter:1777] Loss: 0.105 | Acc: 95.536% \n",
      "[epoch:19, iter:1778] Loss: 0.103 | Acc: 95.625% \n",
      "[epoch:19, iter:1779] Loss: 0.115 | Acc: 95.343% \n",
      "[epoch:19, iter:1780] Loss: 0.113 | Acc: 95.433% \n",
      "[epoch:19, iter:1781] Loss: 0.112 | Acc: 95.519% \n",
      "[epoch:19, iter:1782] Loss: 0.110 | Acc: 95.602% \n",
      "[epoch:19, iter:1783] Loss: 0.112 | Acc: 95.455% \n",
      "[epoch:19, iter:1784] Loss: 0.111 | Acc: 95.536% \n",
      "[epoch:19, iter:1785] Loss: 0.110 | Acc: 95.614% \n",
      "[epoch:19, iter:1786] Loss: 0.112 | Acc: 95.474% \n",
      "[epoch:19, iter:1787] Loss: 0.112 | Acc: 95.551% \n",
      "[epoch:19, iter:1788] Loss: 0.112 | Acc: 95.521% \n",
      "[epoch:19, iter:1789] Loss: 0.111 | Acc: 95.594% \n",
      "[epoch:19, iter:1790] Loss: 0.109 | Acc: 95.665% \n",
      "[epoch:19, iter:1791] Loss: 0.109 | Acc: 95.734% \n",
      "[epoch:19, iter:1792] Loss: 0.107 | Acc: 95.801% \n",
      "[epoch:19, iter:1793] Loss: 0.106 | Acc: 95.865% \n",
      "[epoch:19, iter:1794] Loss: 0.105 | Acc: 95.928% \n",
      "[epoch:19, iter:1795] Loss: 0.104 | Acc: 95.989% \n",
      "[epoch:19, iter:1796] Loss: 0.103 | Acc: 96.048% \n",
      "[epoch:19, iter:1797] Loss: 0.102 | Acc: 96.105% \n",
      "[epoch:19, iter:1798] Loss: 0.103 | Acc: 96.071% \n",
      "[epoch:19, iter:1799] Loss: 0.103 | Acc: 96.127% \n",
      "[epoch:19, iter:1800] Loss: 0.102 | Acc: 96.181% \n",
      "[epoch:19, iter:1801] Loss: 0.101 | Acc: 96.233% \n",
      "[epoch:19, iter:1802] Loss: 0.101 | Acc: 96.199% \n",
      "[epoch:19, iter:1803] Loss: 0.100 | Acc: 96.250% \n",
      "[epoch:19, iter:1804] Loss: 0.099 | Acc: 96.299% \n",
      "[epoch:19, iter:1805] Loss: 0.100 | Acc: 96.266% \n",
      "[epoch:19, iter:1806] Loss: 0.099 | Acc: 96.314% \n",
      "[epoch:19, iter:1807] Loss: 0.098 | Acc: 96.361% \n",
      "[epoch:19, iter:1808] Loss: 0.097 | Acc: 96.406% \n",
      "[epoch:19, iter:1809] Loss: 0.096 | Acc: 96.451% \n",
      "[epoch:19, iter:1810] Loss: 0.095 | Acc: 96.494% \n",
      "[epoch:19, iter:1811] Loss: 0.094 | Acc: 96.536% \n",
      "[epoch:19, iter:1812] Loss: 0.097 | Acc: 96.429% \n",
      "[epoch:19, iter:1813] Loss: 0.100 | Acc: 96.324% \n",
      "[epoch:19, iter:1814] Loss: 0.099 | Acc: 96.294% \n",
      "[epoch:19, iter:1815] Loss: 0.100 | Acc: 96.264% \n",
      "[epoch:19, iter:1816] Loss: 0.101 | Acc: 96.236% \n",
      "[epoch:19, iter:1817] Loss: 0.101 | Acc: 96.208% \n",
      "[epoch:19, iter:1818] Loss: 0.100 | Acc: 96.250% \n",
      "[epoch:19, iter:1819] Loss: 0.099 | Acc: 96.291% \n",
      "[epoch:19, iter:1820] Loss: 0.098 | Acc: 96.332% \n",
      "[epoch:19, iter:1821] Loss: 0.097 | Acc: 96.371% \n",
      "[epoch:19, iter:1822] Loss: 0.097 | Acc: 96.410% \n",
      "[epoch:19, iter:1823] Loss: 0.096 | Acc: 96.447% \n",
      "[epoch:19, iter:1824] Loss: 0.097 | Acc: 96.468% \n",
      "Waiting Test!\n",
      "测试分类准确率为：83.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 20\n",
      "[epoch:20, iter:1825] Loss: 0.043 | Acc: 100.000% \n",
      "[epoch:20, iter:1826] Loss: 0.192 | Acc: 93.750% \n",
      "[epoch:20, iter:1827] Loss: 0.219 | Acc: 93.750% \n",
      "[epoch:20, iter:1828] Loss: 0.196 | Acc: 92.188% \n",
      "[epoch:20, iter:1829] Loss: 0.158 | Acc: 93.750% \n",
      "[epoch:20, iter:1830] Loss: 0.140 | Acc: 93.750% \n",
      "[epoch:20, iter:1831] Loss: 0.126 | Acc: 94.643% \n",
      "[epoch:20, iter:1832] Loss: 0.110 | Acc: 95.312% \n",
      "[epoch:20, iter:1833] Loss: 0.099 | Acc: 95.833% \n",
      "[epoch:20, iter:1834] Loss: 0.133 | Acc: 94.375% \n",
      "[epoch:20, iter:1835] Loss: 0.145 | Acc: 94.318% \n",
      "[epoch:20, iter:1836] Loss: 0.136 | Acc: 94.792% \n",
      "[epoch:20, iter:1837] Loss: 0.133 | Acc: 94.712% \n",
      "[epoch:20, iter:1838] Loss: 0.126 | Acc: 95.089% \n",
      "[epoch:20, iter:1839] Loss: 0.134 | Acc: 95.000% \n",
      "[epoch:20, iter:1840] Loss: 0.129 | Acc: 95.312% \n",
      "[epoch:20, iter:1841] Loss: 0.124 | Acc: 95.588% \n",
      "[epoch:20, iter:1842] Loss: 0.119 | Acc: 95.833% \n",
      "[epoch:20, iter:1843] Loss: 0.120 | Acc: 95.395% \n",
      "[epoch:20, iter:1844] Loss: 0.116 | Acc: 95.625% \n",
      "[epoch:20, iter:1845] Loss: 0.111 | Acc: 95.833% \n",
      "[epoch:20, iter:1846] Loss: 0.108 | Acc: 96.023% \n",
      "[epoch:20, iter:1847] Loss: 0.104 | Acc: 96.196% \n",
      "[epoch:20, iter:1848] Loss: 0.100 | Acc: 96.354% \n",
      "[epoch:20, iter:1849] Loss: 0.097 | Acc: 96.500% \n",
      "[epoch:20, iter:1850] Loss: 0.097 | Acc: 96.635% \n",
      "[epoch:20, iter:1851] Loss: 0.103 | Acc: 96.528% \n",
      "[epoch:20, iter:1852] Loss: 0.100 | Acc: 96.652% \n",
      "[epoch:20, iter:1853] Loss: 0.098 | Acc: 96.767% \n",
      "[epoch:20, iter:1854] Loss: 0.095 | Acc: 96.875% \n",
      "[epoch:20, iter:1855] Loss: 0.092 | Acc: 96.976% \n",
      "[epoch:20, iter:1856] Loss: 0.090 | Acc: 97.070% \n",
      "[epoch:20, iter:1857] Loss: 0.087 | Acc: 97.159% \n",
      "[epoch:20, iter:1858] Loss: 0.085 | Acc: 97.243% \n",
      "[epoch:20, iter:1859] Loss: 0.083 | Acc: 97.321% \n",
      "[epoch:20, iter:1860] Loss: 0.081 | Acc: 97.396% \n",
      "[epoch:20, iter:1861] Loss: 0.084 | Acc: 97.128% \n",
      "[epoch:20, iter:1862] Loss: 0.082 | Acc: 97.204% \n",
      "[epoch:20, iter:1863] Loss: 0.080 | Acc: 97.276% \n",
      "[epoch:20, iter:1864] Loss: 0.079 | Acc: 97.344% \n",
      "[epoch:20, iter:1865] Loss: 0.077 | Acc: 97.409% \n",
      "[epoch:20, iter:1866] Loss: 0.078 | Acc: 97.321% \n",
      "[epoch:20, iter:1867] Loss: 0.076 | Acc: 97.384% \n",
      "[epoch:20, iter:1868] Loss: 0.079 | Acc: 97.159% \n",
      "[epoch:20, iter:1869] Loss: 0.077 | Acc: 97.222% \n",
      "[epoch:20, iter:1870] Loss: 0.079 | Acc: 97.147% \n",
      "[epoch:20, iter:1871] Loss: 0.078 | Acc: 97.207% \n",
      "[epoch:20, iter:1872] Loss: 0.081 | Acc: 97.135% \n",
      "[epoch:20, iter:1873] Loss: 0.080 | Acc: 97.194% \n",
      "[epoch:20, iter:1874] Loss: 0.081 | Acc: 97.125% \n",
      "[epoch:20, iter:1875] Loss: 0.080 | Acc: 97.181% \n",
      "[epoch:20, iter:1876] Loss: 0.082 | Acc: 96.995% \n",
      "[epoch:20, iter:1877] Loss: 0.081 | Acc: 97.052% \n",
      "[epoch:20, iter:1878] Loss: 0.080 | Acc: 97.106% \n",
      "[epoch:20, iter:1879] Loss: 0.079 | Acc: 97.159% \n",
      "[epoch:20, iter:1880] Loss: 0.079 | Acc: 97.098% \n",
      "[epoch:20, iter:1881] Loss: 0.077 | Acc: 97.149% \n",
      "[epoch:20, iter:1882] Loss: 0.076 | Acc: 97.198% \n",
      "[epoch:20, iter:1883] Loss: 0.075 | Acc: 97.246% \n",
      "[epoch:20, iter:1884] Loss: 0.076 | Acc: 97.188% \n",
      "[epoch:20, iter:1885] Loss: 0.077 | Acc: 97.131% \n",
      "[epoch:20, iter:1886] Loss: 0.077 | Acc: 97.077% \n",
      "[epoch:20, iter:1887] Loss: 0.076 | Acc: 97.123% \n",
      "[epoch:20, iter:1888] Loss: 0.075 | Acc: 97.168% \n",
      "[epoch:20, iter:1889] Loss: 0.074 | Acc: 97.212% \n",
      "[epoch:20, iter:1890] Loss: 0.077 | Acc: 97.159% \n",
      "[epoch:20, iter:1891] Loss: 0.078 | Acc: 97.108% \n",
      "[epoch:20, iter:1892] Loss: 0.077 | Acc: 97.151% \n",
      "[epoch:20, iter:1893] Loss: 0.080 | Acc: 97.101% \n",
      "[epoch:20, iter:1894] Loss: 0.079 | Acc: 97.143% \n",
      "[epoch:20, iter:1895] Loss: 0.078 | Acc: 97.183% \n",
      "[epoch:20, iter:1896] Loss: 0.077 | Acc: 97.222% \n",
      "[epoch:20, iter:1897] Loss: 0.078 | Acc: 97.175% \n",
      "[epoch:20, iter:1898] Loss: 0.078 | Acc: 97.213% \n",
      "[epoch:20, iter:1899] Loss: 0.079 | Acc: 97.167% \n",
      "[epoch:20, iter:1900] Loss: 0.078 | Acc: 97.204% \n",
      "[epoch:20, iter:1901] Loss: 0.078 | Acc: 97.159% \n",
      "[epoch:20, iter:1902] Loss: 0.077 | Acc: 97.196% \n",
      "[epoch:20, iter:1903] Loss: 0.077 | Acc: 97.231% \n",
      "[epoch:20, iter:1904] Loss: 0.078 | Acc: 97.188% \n",
      "[epoch:20, iter:1905] Loss: 0.077 | Acc: 97.222% \n",
      "[epoch:20, iter:1906] Loss: 0.077 | Acc: 97.256% \n",
      "[epoch:20, iter:1907] Loss: 0.076 | Acc: 97.289% \n",
      "[epoch:20, iter:1908] Loss: 0.075 | Acc: 97.321% \n",
      "[epoch:20, iter:1909] Loss: 0.076 | Acc: 97.279% \n",
      "[epoch:20, iter:1910] Loss: 0.076 | Acc: 97.238% \n",
      "[epoch:20, iter:1911] Loss: 0.075 | Acc: 97.270% \n",
      "[epoch:20, iter:1912] Loss: 0.076 | Acc: 97.230% \n",
      "[epoch:20, iter:1913] Loss: 0.076 | Acc: 97.261% \n",
      "[epoch:20, iter:1914] Loss: 0.076 | Acc: 97.292% \n",
      "[epoch:20, iter:1915] Loss: 0.076 | Acc: 97.253% \n",
      "[epoch:20, iter:1916] Loss: 0.081 | Acc: 97.011% \n",
      "[epoch:20, iter:1917] Loss: 0.081 | Acc: 96.976% \n",
      "[epoch:20, iter:1918] Loss: 0.082 | Acc: 96.875% \n",
      "[epoch:20, iter:1919] Loss: 0.081 | Acc: 96.908% \n",
      "[epoch:20, iter:1920] Loss: 0.081 | Acc: 96.926% \n",
      "Waiting Test!\n",
      "测试分类准确率为：87.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 21\n",
      "[epoch:21, iter:1921] Loss: 0.010 | Acc: 100.000% \n",
      "[epoch:21, iter:1922] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:21, iter:1923] Loss: 0.017 | Acc: 100.000% \n",
      "[epoch:21, iter:1924] Loss: 0.028 | Acc: 100.000% \n",
      "[epoch:21, iter:1925] Loss: 0.039 | Acc: 98.750% \n",
      "[epoch:21, iter:1926] Loss: 0.085 | Acc: 96.875% \n",
      "[epoch:21, iter:1927] Loss: 0.076 | Acc: 97.321% \n",
      "[epoch:21, iter:1928] Loss: 0.077 | Acc: 97.656% \n",
      "[epoch:21, iter:1929] Loss: 0.084 | Acc: 97.222% \n",
      "[epoch:21, iter:1930] Loss: 0.076 | Acc: 97.500% \n",
      "[epoch:21, iter:1931] Loss: 0.085 | Acc: 96.591% \n",
      "[epoch:21, iter:1932] Loss: 0.083 | Acc: 96.354% \n",
      "[epoch:21, iter:1933] Loss: 0.077 | Acc: 96.635% \n",
      "[epoch:21, iter:1934] Loss: 0.073 | Acc: 96.875% \n",
      "[epoch:21, iter:1935] Loss: 0.069 | Acc: 97.083% \n",
      "[epoch:21, iter:1936] Loss: 0.066 | Acc: 97.266% \n",
      "[epoch:21, iter:1937] Loss: 0.063 | Acc: 97.426% \n",
      "[epoch:21, iter:1938] Loss: 0.061 | Acc: 97.569% \n",
      "[epoch:21, iter:1939] Loss: 0.058 | Acc: 97.697% \n",
      "[epoch:21, iter:1940] Loss: 0.055 | Acc: 97.812% \n",
      "[epoch:21, iter:1941] Loss: 0.058 | Acc: 97.619% \n",
      "[epoch:21, iter:1942] Loss: 0.057 | Acc: 97.727% \n",
      "[epoch:21, iter:1943] Loss: 0.058 | Acc: 97.554% \n",
      "[epoch:21, iter:1944] Loss: 0.059 | Acc: 97.396% \n",
      "[epoch:21, iter:1945] Loss: 0.058 | Acc: 97.500% \n",
      "[epoch:21, iter:1946] Loss: 0.058 | Acc: 97.596% \n",
      "[epoch:21, iter:1947] Loss: 0.056 | Acc: 97.685% \n",
      "[epoch:21, iter:1948] Loss: 0.055 | Acc: 97.768% \n",
      "[epoch:21, iter:1949] Loss: 0.053 | Acc: 97.845% \n",
      "[epoch:21, iter:1950] Loss: 0.054 | Acc: 97.708% \n",
      "[epoch:21, iter:1951] Loss: 0.056 | Acc: 97.581% \n",
      "[epoch:21, iter:1952] Loss: 0.057 | Acc: 97.461% \n",
      "[epoch:21, iter:1953] Loss: 0.058 | Acc: 97.348% \n",
      "[epoch:21, iter:1954] Loss: 0.057 | Acc: 97.426% \n",
      "[epoch:21, iter:1955] Loss: 0.055 | Acc: 97.500% \n",
      "[epoch:21, iter:1956] Loss: 0.058 | Acc: 97.222% \n",
      "[epoch:21, iter:1957] Loss: 0.059 | Acc: 97.128% \n",
      "[epoch:21, iter:1958] Loss: 0.058 | Acc: 97.204% \n",
      "[epoch:21, iter:1959] Loss: 0.059 | Acc: 97.115% \n",
      "[epoch:21, iter:1960] Loss: 0.060 | Acc: 97.031% \n",
      "[epoch:21, iter:1961] Loss: 0.060 | Acc: 97.104% \n",
      "[epoch:21, iter:1962] Loss: 0.063 | Acc: 97.024% \n",
      "[epoch:21, iter:1963] Loss: 0.062 | Acc: 97.093% \n",
      "[epoch:21, iter:1964] Loss: 0.063 | Acc: 97.017% \n",
      "[epoch:21, iter:1965] Loss: 0.063 | Acc: 97.083% \n",
      "[epoch:21, iter:1966] Loss: 0.063 | Acc: 97.011% \n",
      "[epoch:21, iter:1967] Loss: 0.064 | Acc: 96.941% \n",
      "[epoch:21, iter:1968] Loss: 0.063 | Acc: 97.005% \n",
      "[epoch:21, iter:1969] Loss: 0.062 | Acc: 97.066% \n",
      "[epoch:21, iter:1970] Loss: 0.064 | Acc: 97.000% \n",
      "[epoch:21, iter:1971] Loss: 0.064 | Acc: 97.059% \n",
      "[epoch:21, iter:1972] Loss: 0.066 | Acc: 96.995% \n",
      "[epoch:21, iter:1973] Loss: 0.066 | Acc: 96.934% \n",
      "[epoch:21, iter:1974] Loss: 0.066 | Acc: 96.991% \n",
      "[epoch:21, iter:1975] Loss: 0.066 | Acc: 96.932% \n",
      "[epoch:21, iter:1976] Loss: 0.069 | Acc: 96.763% \n",
      "[epoch:21, iter:1977] Loss: 0.068 | Acc: 96.820% \n",
      "[epoch:21, iter:1978] Loss: 0.067 | Acc: 96.875% \n",
      "[epoch:21, iter:1979] Loss: 0.066 | Acc: 96.928% \n",
      "[epoch:21, iter:1980] Loss: 0.066 | Acc: 96.875% \n",
      "[epoch:21, iter:1981] Loss: 0.065 | Acc: 96.926% \n",
      "[epoch:21, iter:1982] Loss: 0.065 | Acc: 96.976% \n",
      "[epoch:21, iter:1983] Loss: 0.069 | Acc: 96.726% \n",
      "[epoch:21, iter:1984] Loss: 0.068 | Acc: 96.777% \n",
      "[epoch:21, iter:1985] Loss: 0.068 | Acc: 96.827% \n",
      "[epoch:21, iter:1986] Loss: 0.067 | Acc: 96.875% \n",
      "[epoch:21, iter:1987] Loss: 0.068 | Acc: 96.828% \n",
      "[epoch:21, iter:1988] Loss: 0.067 | Acc: 96.875% \n",
      "[epoch:21, iter:1989] Loss: 0.067 | Acc: 96.920% \n",
      "[epoch:21, iter:1990] Loss: 0.066 | Acc: 96.964% \n",
      "[epoch:21, iter:1991] Loss: 0.066 | Acc: 97.007% \n",
      "[epoch:21, iter:1992] Loss: 0.065 | Acc: 97.049% \n",
      "[epoch:21, iter:1993] Loss: 0.067 | Acc: 96.918% \n",
      "[epoch:21, iter:1994] Loss: 0.066 | Acc: 96.959% \n",
      "[epoch:21, iter:1995] Loss: 0.065 | Acc: 97.000% \n",
      "[epoch:21, iter:1996] Loss: 0.064 | Acc: 97.039% \n",
      "[epoch:21, iter:1997] Loss: 0.064 | Acc: 97.078% \n",
      "[epoch:21, iter:1998] Loss: 0.063 | Acc: 97.115% \n",
      "[epoch:21, iter:1999] Loss: 0.063 | Acc: 97.152% \n",
      "[epoch:21, iter:2000] Loss: 0.062 | Acc: 97.188% \n",
      "[epoch:21, iter:2001] Loss: 0.062 | Acc: 97.145% \n",
      "[epoch:21, iter:2002] Loss: 0.062 | Acc: 97.180% \n",
      "[epoch:21, iter:2003] Loss: 0.061 | Acc: 97.214% \n",
      "[epoch:21, iter:2004] Loss: 0.061 | Acc: 97.173% \n",
      "[epoch:21, iter:2005] Loss: 0.060 | Acc: 97.206% \n",
      "[epoch:21, iter:2006] Loss: 0.060 | Acc: 97.238% \n",
      "[epoch:21, iter:2007] Loss: 0.061 | Acc: 97.126% \n",
      "[epoch:21, iter:2008] Loss: 0.064 | Acc: 96.946% \n",
      "[epoch:21, iter:2009] Loss: 0.067 | Acc: 96.840% \n",
      "[epoch:21, iter:2010] Loss: 0.067 | Acc: 96.806% \n",
      "[epoch:21, iter:2011] Loss: 0.067 | Acc: 96.772% \n",
      "[epoch:21, iter:2012] Loss: 0.066 | Acc: 96.807% \n",
      "[epoch:21, iter:2013] Loss: 0.066 | Acc: 96.841% \n",
      "[epoch:21, iter:2014] Loss: 0.066 | Acc: 96.875% \n",
      "[epoch:21, iter:2015] Loss: 0.065 | Acc: 96.908% \n",
      "[epoch:21, iter:2016] Loss: 0.066 | Acc: 96.861% \n",
      "Waiting Test!\n",
      "测试分类准确率为：78.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 22\n",
      "[epoch:22, iter:2017] Loss: 0.006 | Acc: 100.000% \n",
      "[epoch:22, iter:2018] Loss: 0.038 | Acc: 96.875% \n",
      "[epoch:22, iter:2019] Loss: 0.026 | Acc: 97.917% \n",
      "[epoch:22, iter:2020] Loss: 0.029 | Acc: 98.438% \n",
      "[epoch:22, iter:2021] Loss: 0.026 | Acc: 98.750% \n",
      "[epoch:22, iter:2022] Loss: 0.030 | Acc: 98.958% \n",
      "[epoch:22, iter:2023] Loss: 0.027 | Acc: 99.107% \n",
      "[epoch:22, iter:2024] Loss: 0.057 | Acc: 98.438% \n",
      "[epoch:22, iter:2025] Loss: 0.063 | Acc: 97.917% \n",
      "[epoch:22, iter:2026] Loss: 0.065 | Acc: 97.500% \n",
      "[epoch:22, iter:2027] Loss: 0.065 | Acc: 97.727% \n",
      "[epoch:22, iter:2028] Loss: 0.101 | Acc: 96.875% \n",
      "[epoch:22, iter:2029] Loss: 0.100 | Acc: 96.635% \n",
      "[epoch:22, iter:2030] Loss: 0.094 | Acc: 96.875% \n",
      "[epoch:22, iter:2031] Loss: 0.099 | Acc: 96.667% \n",
      "[epoch:22, iter:2032] Loss: 0.095 | Acc: 96.875% \n",
      "[epoch:22, iter:2033] Loss: 0.090 | Acc: 97.059% \n",
      "[epoch:22, iter:2034] Loss: 0.086 | Acc: 97.222% \n",
      "[epoch:22, iter:2035] Loss: 0.081 | Acc: 97.368% \n",
      "[epoch:22, iter:2036] Loss: 0.097 | Acc: 96.875% \n",
      "[epoch:22, iter:2037] Loss: 0.098 | Acc: 96.726% \n",
      "[epoch:22, iter:2038] Loss: 0.094 | Acc: 96.875% \n",
      "[epoch:22, iter:2039] Loss: 0.093 | Acc: 97.011% \n",
      "[epoch:22, iter:2040] Loss: 0.090 | Acc: 97.135% \n",
      "[epoch:22, iter:2041] Loss: 0.090 | Acc: 97.250% \n",
      "[epoch:22, iter:2042] Loss: 0.091 | Acc: 97.115% \n",
      "[epoch:22, iter:2043] Loss: 0.088 | Acc: 97.222% \n",
      "[epoch:22, iter:2044] Loss: 0.085 | Acc: 97.321% \n",
      "[epoch:22, iter:2045] Loss: 0.085 | Acc: 97.198% \n",
      "[epoch:22, iter:2046] Loss: 0.091 | Acc: 96.875% \n",
      "[epoch:22, iter:2047] Loss: 0.088 | Acc: 96.976% \n",
      "[epoch:22, iter:2048] Loss: 0.090 | Acc: 96.875% \n",
      "[epoch:22, iter:2049] Loss: 0.090 | Acc: 96.780% \n",
      "[epoch:22, iter:2050] Loss: 0.088 | Acc: 96.875% \n",
      "[epoch:22, iter:2051] Loss: 0.085 | Acc: 96.964% \n",
      "[epoch:22, iter:2052] Loss: 0.083 | Acc: 97.049% \n",
      "[epoch:22, iter:2053] Loss: 0.082 | Acc: 97.128% \n",
      "[epoch:22, iter:2054] Loss: 0.081 | Acc: 97.204% \n",
      "[epoch:22, iter:2055] Loss: 0.079 | Acc: 97.276% \n",
      "[epoch:22, iter:2056] Loss: 0.078 | Acc: 97.344% \n",
      "[epoch:22, iter:2057] Loss: 0.078 | Acc: 97.256% \n",
      "[epoch:22, iter:2058] Loss: 0.076 | Acc: 97.321% \n",
      "[epoch:22, iter:2059] Loss: 0.077 | Acc: 97.238% \n",
      "[epoch:22, iter:2060] Loss: 0.077 | Acc: 97.159% \n",
      "[epoch:22, iter:2061] Loss: 0.076 | Acc: 97.222% \n",
      "[epoch:22, iter:2062] Loss: 0.074 | Acc: 97.283% \n",
      "[epoch:22, iter:2063] Loss: 0.073 | Acc: 97.340% \n",
      "[epoch:22, iter:2064] Loss: 0.072 | Acc: 97.396% \n",
      "[epoch:22, iter:2065] Loss: 0.071 | Acc: 97.449% \n",
      "[epoch:22, iter:2066] Loss: 0.070 | Acc: 97.500% \n",
      "[epoch:22, iter:2067] Loss: 0.069 | Acc: 97.549% \n",
      "[epoch:22, iter:2068] Loss: 0.068 | Acc: 97.596% \n",
      "[epoch:22, iter:2069] Loss: 0.066 | Acc: 97.642% \n",
      "[epoch:22, iter:2070] Loss: 0.069 | Acc: 97.569% \n",
      "[epoch:22, iter:2071] Loss: 0.068 | Acc: 97.614% \n",
      "[epoch:22, iter:2072] Loss: 0.068 | Acc: 97.656% \n",
      "[epoch:22, iter:2073] Loss: 0.069 | Acc: 97.588% \n",
      "[epoch:22, iter:2074] Loss: 0.068 | Acc: 97.629% \n",
      "[epoch:22, iter:2075] Loss: 0.067 | Acc: 97.669% \n",
      "[epoch:22, iter:2076] Loss: 0.066 | Acc: 97.708% \n",
      "[epoch:22, iter:2077] Loss: 0.065 | Acc: 97.746% \n",
      "[epoch:22, iter:2078] Loss: 0.065 | Acc: 97.782% \n",
      "[epoch:22, iter:2079] Loss: 0.074 | Acc: 97.619% \n",
      "[epoch:22, iter:2080] Loss: 0.075 | Acc: 97.559% \n",
      "[epoch:22, iter:2081] Loss: 0.074 | Acc: 97.596% \n",
      "[epoch:22, iter:2082] Loss: 0.073 | Acc: 97.633% \n",
      "[epoch:22, iter:2083] Loss: 0.077 | Acc: 97.481% \n",
      "[epoch:22, iter:2084] Loss: 0.076 | Acc: 97.518% \n",
      "[epoch:22, iter:2085] Loss: 0.077 | Acc: 97.464% \n",
      "[epoch:22, iter:2086] Loss: 0.076 | Acc: 97.500% \n",
      "[epoch:22, iter:2087] Loss: 0.075 | Acc: 97.535% \n",
      "[epoch:22, iter:2088] Loss: 0.074 | Acc: 97.569% \n",
      "[epoch:22, iter:2089] Loss: 0.077 | Acc: 97.432% \n",
      "[epoch:22, iter:2090] Loss: 0.076 | Acc: 97.466% \n",
      "[epoch:22, iter:2091] Loss: 0.076 | Acc: 97.500% \n",
      "[epoch:22, iter:2092] Loss: 0.076 | Acc: 97.451% \n",
      "[epoch:22, iter:2093] Loss: 0.076 | Acc: 97.484% \n",
      "[epoch:22, iter:2094] Loss: 0.075 | Acc: 97.516% \n",
      "[epoch:22, iter:2095] Loss: 0.074 | Acc: 97.547% \n",
      "[epoch:22, iter:2096] Loss: 0.074 | Acc: 97.578% \n",
      "[epoch:22, iter:2097] Loss: 0.073 | Acc: 97.608% \n",
      "[epoch:22, iter:2098] Loss: 0.074 | Acc: 97.561% \n",
      "[epoch:22, iter:2099] Loss: 0.078 | Acc: 97.515% \n",
      "[epoch:22, iter:2100] Loss: 0.077 | Acc: 97.545% \n",
      "[epoch:22, iter:2101] Loss: 0.077 | Acc: 97.574% \n",
      "[epoch:22, iter:2102] Loss: 0.077 | Acc: 97.529% \n",
      "[epoch:22, iter:2103] Loss: 0.076 | Acc: 97.557% \n",
      "[epoch:22, iter:2104] Loss: 0.075 | Acc: 97.585% \n",
      "[epoch:22, iter:2105] Loss: 0.075 | Acc: 97.612% \n",
      "[epoch:22, iter:2106] Loss: 0.074 | Acc: 97.639% \n",
      "[epoch:22, iter:2107] Loss: 0.078 | Acc: 97.527% \n",
      "[epoch:22, iter:2108] Loss: 0.077 | Acc: 97.554% \n",
      "[epoch:22, iter:2109] Loss: 0.077 | Acc: 97.513% \n",
      "[epoch:22, iter:2110] Loss: 0.077 | Acc: 97.540% \n",
      "[epoch:22, iter:2111] Loss: 0.077 | Acc: 97.500% \n",
      "[epoch:22, iter:2112] Loss: 0.077 | Acc: 97.515% \n",
      "Waiting Test!\n",
      "测试分类准确率为：81.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 23\n",
      "[epoch:23, iter:2113] Loss: 0.050 | Acc: 100.000% \n",
      "[epoch:23, iter:2114] Loss: 0.027 | Acc: 100.000% \n",
      "[epoch:23, iter:2115] Loss: 0.018 | Acc: 100.000% \n",
      "[epoch:23, iter:2116] Loss: 0.025 | Acc: 98.438% \n",
      "[epoch:23, iter:2117] Loss: 0.030 | Acc: 98.750% \n",
      "[epoch:23, iter:2118] Loss: 0.032 | Acc: 98.958% \n",
      "[epoch:23, iter:2119] Loss: 0.028 | Acc: 99.107% \n",
      "[epoch:23, iter:2120] Loss: 0.033 | Acc: 98.438% \n",
      "[epoch:23, iter:2121] Loss: 0.031 | Acc: 98.611% \n",
      "[epoch:23, iter:2122] Loss: 0.028 | Acc: 98.750% \n",
      "[epoch:23, iter:2123] Loss: 0.027 | Acc: 98.864% \n",
      "[epoch:23, iter:2124] Loss: 0.039 | Acc: 98.438% \n",
      "[epoch:23, iter:2125] Loss: 0.037 | Acc: 98.558% \n",
      "[epoch:23, iter:2126] Loss: 0.049 | Acc: 98.214% \n",
      "[epoch:23, iter:2127] Loss: 0.065 | Acc: 97.500% \n",
      "[epoch:23, iter:2128] Loss: 0.070 | Acc: 96.875% \n",
      "[epoch:23, iter:2129] Loss: 0.077 | Acc: 96.324% \n",
      "[epoch:23, iter:2130] Loss: 0.074 | Acc: 96.528% \n",
      "[epoch:23, iter:2131] Loss: 0.071 | Acc: 96.711% \n",
      "[epoch:23, iter:2132] Loss: 0.067 | Acc: 96.875% \n",
      "[epoch:23, iter:2133] Loss: 0.078 | Acc: 96.726% \n",
      "[epoch:23, iter:2134] Loss: 0.092 | Acc: 96.023% \n",
      "[epoch:23, iter:2135] Loss: 0.088 | Acc: 96.196% \n",
      "[epoch:23, iter:2136] Loss: 0.085 | Acc: 96.354% \n",
      "[epoch:23, iter:2137] Loss: 0.086 | Acc: 96.250% \n",
      "[epoch:23, iter:2138] Loss: 0.089 | Acc: 96.154% \n",
      "[epoch:23, iter:2139] Loss: 0.086 | Acc: 96.296% \n",
      "[epoch:23, iter:2140] Loss: 0.088 | Acc: 96.205% \n",
      "[epoch:23, iter:2141] Loss: 0.085 | Acc: 96.336% \n",
      "[epoch:23, iter:2142] Loss: 0.083 | Acc: 96.458% \n",
      "[epoch:23, iter:2143] Loss: 0.086 | Acc: 96.371% \n",
      "[epoch:23, iter:2144] Loss: 0.085 | Acc: 96.484% \n",
      "[epoch:23, iter:2145] Loss: 0.082 | Acc: 96.591% \n",
      "[epoch:23, iter:2146] Loss: 0.083 | Acc: 96.507% \n",
      "[epoch:23, iter:2147] Loss: 0.084 | Acc: 96.429% \n",
      "[epoch:23, iter:2148] Loss: 0.082 | Acc: 96.528% \n",
      "[epoch:23, iter:2149] Loss: 0.086 | Acc: 96.284% \n",
      "[epoch:23, iter:2150] Loss: 0.085 | Acc: 96.382% \n",
      "[epoch:23, iter:2151] Loss: 0.084 | Acc: 96.474% \n",
      "[epoch:23, iter:2152] Loss: 0.082 | Acc: 96.562% \n",
      "[epoch:23, iter:2153] Loss: 0.080 | Acc: 96.646% \n",
      "[epoch:23, iter:2154] Loss: 0.079 | Acc: 96.726% \n",
      "[epoch:23, iter:2155] Loss: 0.077 | Acc: 96.802% \n",
      "[epoch:23, iter:2156] Loss: 0.075 | Acc: 96.875% \n",
      "[epoch:23, iter:2157] Loss: 0.074 | Acc: 96.944% \n",
      "[epoch:23, iter:2158] Loss: 0.075 | Acc: 96.875% \n",
      "[epoch:23, iter:2159] Loss: 0.073 | Acc: 96.941% \n",
      "[epoch:23, iter:2160] Loss: 0.072 | Acc: 97.005% \n",
      "[epoch:23, iter:2161] Loss: 0.075 | Acc: 96.939% \n",
      "[epoch:23, iter:2162] Loss: 0.073 | Acc: 97.000% \n",
      "[epoch:23, iter:2163] Loss: 0.072 | Acc: 97.059% \n",
      "[epoch:23, iter:2164] Loss: 0.071 | Acc: 97.115% \n",
      "[epoch:23, iter:2165] Loss: 0.071 | Acc: 97.052% \n",
      "[epoch:23, iter:2166] Loss: 0.070 | Acc: 97.106% \n",
      "[epoch:23, iter:2167] Loss: 0.070 | Acc: 97.159% \n",
      "[epoch:23, iter:2168] Loss: 0.071 | Acc: 97.098% \n",
      "[epoch:23, iter:2169] Loss: 0.070 | Acc: 97.149% \n",
      "[epoch:23, iter:2170] Loss: 0.071 | Acc: 97.091% \n",
      "[epoch:23, iter:2171] Loss: 0.070 | Acc: 97.140% \n",
      "[epoch:23, iter:2172] Loss: 0.069 | Acc: 97.188% \n",
      "[epoch:23, iter:2173] Loss: 0.068 | Acc: 97.234% \n",
      "[epoch:23, iter:2174] Loss: 0.068 | Acc: 97.278% \n",
      "[epoch:23, iter:2175] Loss: 0.067 | Acc: 97.321% \n",
      "[epoch:23, iter:2176] Loss: 0.070 | Acc: 97.168% \n",
      "[epoch:23, iter:2177] Loss: 0.070 | Acc: 97.212% \n",
      "[epoch:23, iter:2178] Loss: 0.069 | Acc: 97.254% \n",
      "[epoch:23, iter:2179] Loss: 0.073 | Acc: 97.015% \n",
      "[epoch:23, iter:2180] Loss: 0.073 | Acc: 97.059% \n",
      "[epoch:23, iter:2181] Loss: 0.072 | Acc: 97.101% \n",
      "[epoch:23, iter:2182] Loss: 0.072 | Acc: 97.054% \n",
      "[epoch:23, iter:2183] Loss: 0.072 | Acc: 97.007% \n",
      "[epoch:23, iter:2184] Loss: 0.072 | Acc: 96.962% \n",
      "[epoch:23, iter:2185] Loss: 0.071 | Acc: 97.003% \n",
      "[epoch:23, iter:2186] Loss: 0.073 | Acc: 96.959% \n",
      "[epoch:23, iter:2187] Loss: 0.073 | Acc: 97.000% \n",
      "[epoch:23, iter:2188] Loss: 0.074 | Acc: 96.957% \n",
      "[epoch:23, iter:2189] Loss: 0.073 | Acc: 96.997% \n",
      "[epoch:23, iter:2190] Loss: 0.076 | Acc: 96.955% \n",
      "[epoch:23, iter:2191] Loss: 0.082 | Acc: 96.756% \n",
      "[epoch:23, iter:2192] Loss: 0.082 | Acc: 96.719% \n",
      "[epoch:23, iter:2193] Loss: 0.082 | Acc: 96.682% \n",
      "[epoch:23, iter:2194] Loss: 0.082 | Acc: 96.646% \n",
      "[epoch:23, iter:2195] Loss: 0.089 | Acc: 96.310% \n",
      "[epoch:23, iter:2196] Loss: 0.088 | Acc: 96.354% \n",
      "[epoch:23, iter:2197] Loss: 0.087 | Acc: 96.397% \n",
      "[epoch:23, iter:2198] Loss: 0.087 | Acc: 96.439% \n",
      "[epoch:23, iter:2199] Loss: 0.087 | Acc: 96.408% \n",
      "[epoch:23, iter:2200] Loss: 0.086 | Acc: 96.449% \n",
      "[epoch:23, iter:2201] Loss: 0.087 | Acc: 96.419% \n",
      "[epoch:23, iter:2202] Loss: 0.089 | Acc: 96.250% \n",
      "[epoch:23, iter:2203] Loss: 0.089 | Acc: 96.223% \n",
      "[epoch:23, iter:2204] Loss: 0.088 | Acc: 96.264% \n",
      "[epoch:23, iter:2205] Loss: 0.087 | Acc: 96.304% \n",
      "[epoch:23, iter:2206] Loss: 0.087 | Acc: 96.343% \n",
      "[epoch:23, iter:2207] Loss: 0.086 | Acc: 96.382% \n",
      "[epoch:23, iter:2208] Loss: 0.086 | Acc: 96.403% \n",
      "Waiting Test!\n",
      "测试分类准确率为：82.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 24\n",
      "[epoch:24, iter:2209] Loss: 0.018 | Acc: 100.000% \n",
      "[epoch:24, iter:2210] Loss: 0.036 | Acc: 100.000% \n",
      "[epoch:24, iter:2211] Loss: 0.062 | Acc: 97.917% \n",
      "[epoch:24, iter:2212] Loss: 0.052 | Acc: 98.438% \n",
      "[epoch:24, iter:2213] Loss: 0.056 | Acc: 98.750% \n",
      "[epoch:24, iter:2214] Loss: 0.047 | Acc: 98.958% \n",
      "[epoch:24, iter:2215] Loss: 0.042 | Acc: 99.107% \n",
      "[epoch:24, iter:2216] Loss: 0.051 | Acc: 98.438% \n",
      "[epoch:24, iter:2217] Loss: 0.046 | Acc: 98.611% \n",
      "[epoch:24, iter:2218] Loss: 0.043 | Acc: 98.750% \n",
      "[epoch:24, iter:2219] Loss: 0.040 | Acc: 98.864% \n",
      "[epoch:24, iter:2220] Loss: 0.042 | Acc: 98.958% \n",
      "[epoch:24, iter:2221] Loss: 0.039 | Acc: 99.038% \n",
      "[epoch:24, iter:2222] Loss: 0.041 | Acc: 98.661% \n",
      "[epoch:24, iter:2223] Loss: 0.040 | Acc: 98.750% \n",
      "[epoch:24, iter:2224] Loss: 0.055 | Acc: 98.438% \n",
      "[epoch:24, iter:2225] Loss: 0.059 | Acc: 98.529% \n",
      "[epoch:24, iter:2226] Loss: 0.056 | Acc: 98.611% \n",
      "[epoch:24, iter:2227] Loss: 0.054 | Acc: 98.684% \n",
      "[epoch:24, iter:2228] Loss: 0.052 | Acc: 98.750% \n",
      "[epoch:24, iter:2229] Loss: 0.050 | Acc: 98.810% \n",
      "[epoch:24, iter:2230] Loss: 0.048 | Acc: 98.864% \n",
      "[epoch:24, iter:2231] Loss: 0.054 | Acc: 98.641% \n",
      "[epoch:24, iter:2232] Loss: 0.053 | Acc: 98.698% \n",
      "[epoch:24, iter:2233] Loss: 0.054 | Acc: 98.750% \n",
      "[epoch:24, iter:2234] Loss: 0.060 | Acc: 98.317% \n",
      "[epoch:24, iter:2235] Loss: 0.061 | Acc: 98.148% \n",
      "[epoch:24, iter:2236] Loss: 0.083 | Acc: 97.098% \n",
      "[epoch:24, iter:2237] Loss: 0.083 | Acc: 97.198% \n",
      "[epoch:24, iter:2238] Loss: 0.083 | Acc: 97.083% \n",
      "[epoch:24, iter:2239] Loss: 0.081 | Acc: 97.177% \n",
      "[epoch:24, iter:2240] Loss: 0.079 | Acc: 97.266% \n",
      "[epoch:24, iter:2241] Loss: 0.079 | Acc: 97.159% \n",
      "[epoch:24, iter:2242] Loss: 0.078 | Acc: 97.243% \n",
      "[epoch:24, iter:2243] Loss: 0.075 | Acc: 97.321% \n",
      "[epoch:24, iter:2244] Loss: 0.074 | Acc: 97.396% \n",
      "[epoch:24, iter:2245] Loss: 0.073 | Acc: 97.466% \n",
      "[epoch:24, iter:2246] Loss: 0.077 | Acc: 97.368% \n",
      "[epoch:24, iter:2247] Loss: 0.075 | Acc: 97.436% \n",
      "[epoch:24, iter:2248] Loss: 0.089 | Acc: 97.188% \n",
      "[epoch:24, iter:2249] Loss: 0.087 | Acc: 97.256% \n",
      "[epoch:24, iter:2250] Loss: 0.086 | Acc: 97.321% \n",
      "[epoch:24, iter:2251] Loss: 0.088 | Acc: 97.093% \n",
      "[epoch:24, iter:2252] Loss: 0.087 | Acc: 97.159% \n",
      "[epoch:24, iter:2253] Loss: 0.086 | Acc: 97.083% \n",
      "[epoch:24, iter:2254] Loss: 0.085 | Acc: 97.147% \n",
      "[epoch:24, iter:2255] Loss: 0.083 | Acc: 97.207% \n",
      "[epoch:24, iter:2256] Loss: 0.085 | Acc: 97.135% \n",
      "[epoch:24, iter:2257] Loss: 0.083 | Acc: 97.194% \n",
      "[epoch:24, iter:2258] Loss: 0.084 | Acc: 97.125% \n",
      "[epoch:24, iter:2259] Loss: 0.083 | Acc: 97.181% \n",
      "[epoch:24, iter:2260] Loss: 0.083 | Acc: 97.236% \n",
      "[epoch:24, iter:2261] Loss: 0.084 | Acc: 97.052% \n",
      "[epoch:24, iter:2262] Loss: 0.085 | Acc: 96.991% \n",
      "[epoch:24, iter:2263] Loss: 0.084 | Acc: 97.045% \n",
      "[epoch:24, iter:2264] Loss: 0.083 | Acc: 97.098% \n",
      "[epoch:24, iter:2265] Loss: 0.086 | Acc: 96.930% \n",
      "[epoch:24, iter:2266] Loss: 0.084 | Acc: 96.983% \n",
      "[epoch:24, iter:2267] Loss: 0.083 | Acc: 97.034% \n",
      "[epoch:24, iter:2268] Loss: 0.082 | Acc: 97.083% \n",
      "[epoch:24, iter:2269] Loss: 0.081 | Acc: 97.131% \n",
      "[epoch:24, iter:2270] Loss: 0.080 | Acc: 97.177% \n",
      "[epoch:24, iter:2271] Loss: 0.079 | Acc: 97.222% \n",
      "[epoch:24, iter:2272] Loss: 0.079 | Acc: 97.266% \n",
      "[epoch:24, iter:2273] Loss: 0.078 | Acc: 97.308% \n",
      "[epoch:24, iter:2274] Loss: 0.077 | Acc: 97.348% \n",
      "[epoch:24, iter:2275] Loss: 0.076 | Acc: 97.388% \n",
      "[epoch:24, iter:2276] Loss: 0.075 | Acc: 97.426% \n",
      "[epoch:24, iter:2277] Loss: 0.075 | Acc: 97.373% \n",
      "[epoch:24, iter:2278] Loss: 0.074 | Acc: 97.411% \n",
      "[epoch:24, iter:2279] Loss: 0.073 | Acc: 97.447% \n",
      "[epoch:24, iter:2280] Loss: 0.072 | Acc: 97.483% \n",
      "[epoch:24, iter:2281] Loss: 0.074 | Acc: 97.432% \n",
      "[epoch:24, iter:2282] Loss: 0.075 | Acc: 97.382% \n",
      "[epoch:24, iter:2283] Loss: 0.078 | Acc: 97.167% \n",
      "[epoch:24, iter:2284] Loss: 0.078 | Acc: 97.204% \n",
      "[epoch:24, iter:2285] Loss: 0.077 | Acc: 97.240% \n",
      "[epoch:24, iter:2286] Loss: 0.076 | Acc: 97.276% \n",
      "[epoch:24, iter:2287] Loss: 0.077 | Acc: 97.231% \n",
      "[epoch:24, iter:2288] Loss: 0.078 | Acc: 97.188% \n",
      "[epoch:24, iter:2289] Loss: 0.079 | Acc: 97.145% \n",
      "[epoch:24, iter:2290] Loss: 0.079 | Acc: 97.104% \n",
      "[epoch:24, iter:2291] Loss: 0.078 | Acc: 97.139% \n",
      "[epoch:24, iter:2292] Loss: 0.079 | Acc: 97.098% \n",
      "[epoch:24, iter:2293] Loss: 0.080 | Acc: 97.059% \n",
      "[epoch:24, iter:2294] Loss: 0.080 | Acc: 97.020% \n",
      "[epoch:24, iter:2295] Loss: 0.079 | Acc: 97.055% \n",
      "[epoch:24, iter:2296] Loss: 0.080 | Acc: 97.017% \n",
      "[epoch:24, iter:2297] Loss: 0.082 | Acc: 96.910% \n",
      "[epoch:24, iter:2298] Loss: 0.082 | Acc: 96.944% \n",
      "[epoch:24, iter:2299] Loss: 0.081 | Acc: 96.978% \n",
      "[epoch:24, iter:2300] Loss: 0.080 | Acc: 97.011% \n",
      "[epoch:24, iter:2301] Loss: 0.083 | Acc: 96.841% \n",
      "[epoch:24, iter:2302] Loss: 0.083 | Acc: 96.875% \n",
      "[epoch:24, iter:2303] Loss: 0.082 | Acc: 96.908% \n",
      "[epoch:24, iter:2304] Loss: 0.085 | Acc: 96.795% \n",
      "Waiting Test!\n",
      "测试分类准确率为：80.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 25\n",
      "[epoch:25, iter:2305] Loss: 0.113 | Acc: 93.750% \n",
      "[epoch:25, iter:2306] Loss: 0.086 | Acc: 93.750% \n",
      "[epoch:25, iter:2307] Loss: 0.059 | Acc: 95.833% \n",
      "[epoch:25, iter:2308] Loss: 0.046 | Acc: 96.875% \n",
      "[epoch:25, iter:2309] Loss: 0.037 | Acc: 97.500% \n",
      "[epoch:25, iter:2310] Loss: 0.031 | Acc: 97.917% \n",
      "[epoch:25, iter:2311] Loss: 0.027 | Acc: 98.214% \n",
      "[epoch:25, iter:2312] Loss: 0.031 | Acc: 97.656% \n",
      "[epoch:25, iter:2313] Loss: 0.038 | Acc: 97.222% \n",
      "[epoch:25, iter:2314] Loss: 0.036 | Acc: 97.500% \n",
      "[epoch:25, iter:2315] Loss: 0.040 | Acc: 97.159% \n",
      "[epoch:25, iter:2316] Loss: 0.052 | Acc: 96.875% \n",
      "[epoch:25, iter:2317] Loss: 0.053 | Acc: 96.635% \n",
      "[epoch:25, iter:2318] Loss: 0.053 | Acc: 96.875% \n",
      "[epoch:25, iter:2319] Loss: 0.050 | Acc: 97.083% \n",
      "[epoch:25, iter:2320] Loss: 0.061 | Acc: 96.484% \n",
      "[epoch:25, iter:2321] Loss: 0.058 | Acc: 96.691% \n",
      "[epoch:25, iter:2322] Loss: 0.066 | Acc: 96.181% \n",
      "[epoch:25, iter:2323] Loss: 0.069 | Acc: 96.053% \n",
      "[epoch:25, iter:2324] Loss: 0.066 | Acc: 96.250% \n",
      "[epoch:25, iter:2325] Loss: 0.063 | Acc: 96.429% \n",
      "[epoch:25, iter:2326] Loss: 0.062 | Acc: 96.591% \n",
      "[epoch:25, iter:2327] Loss: 0.073 | Acc: 96.196% \n",
      "[epoch:25, iter:2328] Loss: 0.071 | Acc: 96.354% \n",
      "[epoch:25, iter:2329] Loss: 0.068 | Acc: 96.500% \n",
      "[epoch:25, iter:2330] Loss: 0.069 | Acc: 96.635% \n",
      "[epoch:25, iter:2331] Loss: 0.068 | Acc: 96.759% \n",
      "[epoch:25, iter:2332] Loss: 0.066 | Acc: 96.875% \n",
      "[epoch:25, iter:2333] Loss: 0.066 | Acc: 96.767% \n",
      "[epoch:25, iter:2334] Loss: 0.065 | Acc: 96.875% \n",
      "[epoch:25, iter:2335] Loss: 0.067 | Acc: 96.774% \n",
      "[epoch:25, iter:2336] Loss: 0.066 | Acc: 96.875% \n",
      "[epoch:25, iter:2337] Loss: 0.066 | Acc: 96.780% \n",
      "[epoch:25, iter:2338] Loss: 0.066 | Acc: 96.691% \n",
      "[epoch:25, iter:2339] Loss: 0.065 | Acc: 96.786% \n",
      "[epoch:25, iter:2340] Loss: 0.069 | Acc: 96.528% \n",
      "[epoch:25, iter:2341] Loss: 0.067 | Acc: 96.622% \n",
      "[epoch:25, iter:2342] Loss: 0.068 | Acc: 96.546% \n",
      "[epoch:25, iter:2343] Loss: 0.067 | Acc: 96.635% \n",
      "[epoch:25, iter:2344] Loss: 0.066 | Acc: 96.719% \n",
      "[epoch:25, iter:2345] Loss: 0.064 | Acc: 96.799% \n",
      "[epoch:25, iter:2346] Loss: 0.068 | Acc: 96.577% \n",
      "[epoch:25, iter:2347] Loss: 0.067 | Acc: 96.657% \n",
      "[epoch:25, iter:2348] Loss: 0.068 | Acc: 96.591% \n",
      "[epoch:25, iter:2349] Loss: 0.068 | Acc: 96.667% \n",
      "[epoch:25, iter:2350] Loss: 0.075 | Acc: 96.332% \n",
      "[epoch:25, iter:2351] Loss: 0.076 | Acc: 96.277% \n",
      "[epoch:25, iter:2352] Loss: 0.076 | Acc: 96.354% \n",
      "[epoch:25, iter:2353] Loss: 0.075 | Acc: 96.429% \n",
      "[epoch:25, iter:2354] Loss: 0.075 | Acc: 96.500% \n",
      "[epoch:25, iter:2355] Loss: 0.074 | Acc: 96.569% \n",
      "[epoch:25, iter:2356] Loss: 0.074 | Acc: 96.514% \n",
      "[epoch:25, iter:2357] Loss: 0.074 | Acc: 96.462% \n",
      "[epoch:25, iter:2358] Loss: 0.073 | Acc: 96.528% \n",
      "[epoch:25, iter:2359] Loss: 0.074 | Acc: 96.477% \n",
      "[epoch:25, iter:2360] Loss: 0.074 | Acc: 96.429% \n",
      "[epoch:25, iter:2361] Loss: 0.073 | Acc: 96.491% \n",
      "[epoch:25, iter:2362] Loss: 0.072 | Acc: 96.552% \n",
      "[epoch:25, iter:2363] Loss: 0.078 | Acc: 96.398% \n",
      "[epoch:25, iter:2364] Loss: 0.079 | Acc: 96.354% \n",
      "[epoch:25, iter:2365] Loss: 0.086 | Acc: 95.902% \n",
      "[epoch:25, iter:2366] Loss: 0.086 | Acc: 95.867% \n",
      "[epoch:25, iter:2367] Loss: 0.088 | Acc: 95.734% \n",
      "[epoch:25, iter:2368] Loss: 0.088 | Acc: 95.703% \n",
      "[epoch:25, iter:2369] Loss: 0.087 | Acc: 95.769% \n",
      "[epoch:25, iter:2370] Loss: 0.087 | Acc: 95.833% \n",
      "[epoch:25, iter:2371] Loss: 0.085 | Acc: 95.896% \n",
      "[epoch:25, iter:2372] Loss: 0.084 | Acc: 95.956% \n",
      "[epoch:25, iter:2373] Loss: 0.085 | Acc: 96.014% \n",
      "[epoch:25, iter:2374] Loss: 0.089 | Acc: 95.804% \n",
      "[epoch:25, iter:2375] Loss: 0.089 | Acc: 95.775% \n",
      "[epoch:25, iter:2376] Loss: 0.090 | Acc: 95.660% \n",
      "[epoch:25, iter:2377] Loss: 0.089 | Acc: 95.719% \n",
      "[epoch:25, iter:2378] Loss: 0.088 | Acc: 95.777% \n",
      "[epoch:25, iter:2379] Loss: 0.087 | Acc: 95.833% \n",
      "[epoch:25, iter:2380] Loss: 0.087 | Acc: 95.888% \n",
      "[epoch:25, iter:2381] Loss: 0.086 | Acc: 95.942% \n",
      "[epoch:25, iter:2382] Loss: 0.087 | Acc: 95.994% \n",
      "[epoch:25, iter:2383] Loss: 0.088 | Acc: 95.886% \n",
      "[epoch:25, iter:2384] Loss: 0.090 | Acc: 95.781% \n",
      "[epoch:25, iter:2385] Loss: 0.089 | Acc: 95.833% \n",
      "[epoch:25, iter:2386] Loss: 0.091 | Acc: 95.808% \n",
      "[epoch:25, iter:2387] Loss: 0.090 | Acc: 95.858% \n",
      "[epoch:25, iter:2388] Loss: 0.089 | Acc: 95.908% \n",
      "[epoch:25, iter:2389] Loss: 0.088 | Acc: 95.956% \n",
      "[epoch:25, iter:2390] Loss: 0.088 | Acc: 96.003% \n",
      "[epoch:25, iter:2391] Loss: 0.088 | Acc: 96.049% \n",
      "[epoch:25, iter:2392] Loss: 0.087 | Acc: 96.094% \n",
      "[epoch:25, iter:2393] Loss: 0.088 | Acc: 95.997% \n",
      "[epoch:25, iter:2394] Loss: 0.088 | Acc: 96.042% \n",
      "[epoch:25, iter:2395] Loss: 0.087 | Acc: 96.085% \n",
      "[epoch:25, iter:2396] Loss: 0.087 | Acc: 96.128% \n",
      "[epoch:25, iter:2397] Loss: 0.087 | Acc: 96.102% \n",
      "[epoch:25, iter:2398] Loss: 0.087 | Acc: 96.077% \n",
      "[epoch:25, iter:2399] Loss: 0.087 | Acc: 96.053% \n",
      "[epoch:25, iter:2400] Loss: 0.086 | Acc: 96.076% \n",
      "Waiting Test!\n",
      "测试分类准确率为：87.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 26\n",
      "[epoch:26, iter:2401] Loss: 0.012 | Acc: 100.000% \n",
      "[epoch:26, iter:2402] Loss: 0.055 | Acc: 96.875% \n",
      "[epoch:26, iter:2403] Loss: 0.039 | Acc: 97.917% \n",
      "[epoch:26, iter:2404] Loss: 0.040 | Acc: 98.438% \n",
      "[epoch:26, iter:2405] Loss: 0.035 | Acc: 98.750% \n",
      "[epoch:26, iter:2406] Loss: 0.037 | Acc: 98.958% \n",
      "[epoch:26, iter:2407] Loss: 0.033 | Acc: 99.107% \n",
      "[epoch:26, iter:2408] Loss: 0.029 | Acc: 99.219% \n",
      "[epoch:26, iter:2409] Loss: 0.056 | Acc: 97.917% \n",
      "[epoch:26, iter:2410] Loss: 0.053 | Acc: 98.125% \n",
      "[epoch:26, iter:2411] Loss: 0.049 | Acc: 98.295% \n",
      "[epoch:26, iter:2412] Loss: 0.047 | Acc: 98.438% \n",
      "[epoch:26, iter:2413] Loss: 0.046 | Acc: 98.558% \n",
      "[epoch:26, iter:2414] Loss: 0.052 | Acc: 98.214% \n",
      "[epoch:26, iter:2415] Loss: 0.049 | Acc: 98.333% \n",
      "[epoch:26, iter:2416] Loss: 0.046 | Acc: 98.438% \n",
      "[epoch:26, iter:2417] Loss: 0.058 | Acc: 97.794% \n",
      "[epoch:26, iter:2418] Loss: 0.056 | Acc: 97.917% \n",
      "[epoch:26, iter:2419] Loss: 0.054 | Acc: 98.026% \n",
      "[epoch:26, iter:2420] Loss: 0.051 | Acc: 98.125% \n",
      "[epoch:26, iter:2421] Loss: 0.064 | Acc: 97.917% \n",
      "[epoch:26, iter:2422] Loss: 0.073 | Acc: 97.443% \n",
      "[epoch:26, iter:2423] Loss: 0.070 | Acc: 97.554% \n",
      "[epoch:26, iter:2424] Loss: 0.071 | Acc: 97.656% \n",
      "[epoch:26, iter:2425] Loss: 0.068 | Acc: 97.750% \n",
      "[epoch:26, iter:2426] Loss: 0.066 | Acc: 97.837% \n",
      "[epoch:26, iter:2427] Loss: 0.063 | Acc: 97.917% \n",
      "[epoch:26, iter:2428] Loss: 0.074 | Acc: 97.768% \n",
      "[epoch:26, iter:2429] Loss: 0.076 | Acc: 97.414% \n",
      "[epoch:26, iter:2430] Loss: 0.074 | Acc: 97.500% \n",
      "[epoch:26, iter:2431] Loss: 0.073 | Acc: 97.581% \n",
      "[epoch:26, iter:2432] Loss: 0.071 | Acc: 97.656% \n",
      "[epoch:26, iter:2433] Loss: 0.069 | Acc: 97.727% \n",
      "[epoch:26, iter:2434] Loss: 0.068 | Acc: 97.794% \n",
      "[epoch:26, iter:2435] Loss: 0.067 | Acc: 97.857% \n",
      "[epoch:26, iter:2436] Loss: 0.071 | Acc: 97.743% \n",
      "[epoch:26, iter:2437] Loss: 0.070 | Acc: 97.804% \n",
      "[epoch:26, iter:2438] Loss: 0.068 | Acc: 97.862% \n",
      "[epoch:26, iter:2439] Loss: 0.069 | Acc: 97.917% \n",
      "[epoch:26, iter:2440] Loss: 0.068 | Acc: 97.969% \n",
      "[epoch:26, iter:2441] Loss: 0.066 | Acc: 98.018% \n",
      "[epoch:26, iter:2442] Loss: 0.069 | Acc: 97.917% \n",
      "[epoch:26, iter:2443] Loss: 0.069 | Acc: 97.965% \n",
      "[epoch:26, iter:2444] Loss: 0.069 | Acc: 97.869% \n",
      "[epoch:26, iter:2445] Loss: 0.068 | Acc: 97.917% \n",
      "[epoch:26, iter:2446] Loss: 0.070 | Acc: 97.826% \n",
      "[epoch:26, iter:2447] Loss: 0.068 | Acc: 97.872% \n",
      "[epoch:26, iter:2448] Loss: 0.067 | Acc: 97.917% \n",
      "[epoch:26, iter:2449] Loss: 0.067 | Acc: 97.959% \n",
      "[epoch:26, iter:2450] Loss: 0.071 | Acc: 97.750% \n",
      "[epoch:26, iter:2451] Loss: 0.069 | Acc: 97.794% \n",
      "[epoch:26, iter:2452] Loss: 0.068 | Acc: 97.837% \n",
      "[epoch:26, iter:2453] Loss: 0.067 | Acc: 97.877% \n",
      "[epoch:26, iter:2454] Loss: 0.066 | Acc: 97.917% \n",
      "[epoch:26, iter:2455] Loss: 0.065 | Acc: 97.955% \n",
      "[epoch:26, iter:2456] Loss: 0.065 | Acc: 97.991% \n",
      "[epoch:26, iter:2457] Loss: 0.064 | Acc: 98.026% \n",
      "[epoch:26, iter:2458] Loss: 0.065 | Acc: 97.953% \n",
      "[epoch:26, iter:2459] Loss: 0.064 | Acc: 97.987% \n",
      "[epoch:26, iter:2460] Loss: 0.063 | Acc: 98.021% \n",
      "[epoch:26, iter:2461] Loss: 0.072 | Acc: 97.541% \n",
      "[epoch:26, iter:2462] Loss: 0.073 | Acc: 97.480% \n",
      "[epoch:26, iter:2463] Loss: 0.073 | Acc: 97.421% \n",
      "[epoch:26, iter:2464] Loss: 0.073 | Acc: 97.363% \n",
      "[epoch:26, iter:2465] Loss: 0.072 | Acc: 97.404% \n",
      "[epoch:26, iter:2466] Loss: 0.071 | Acc: 97.443% \n",
      "[epoch:26, iter:2467] Loss: 0.076 | Acc: 97.108% \n",
      "[epoch:26, iter:2468] Loss: 0.076 | Acc: 97.151% \n",
      "[epoch:26, iter:2469] Loss: 0.075 | Acc: 97.192% \n",
      "[epoch:26, iter:2470] Loss: 0.074 | Acc: 97.232% \n",
      "[epoch:26, iter:2471] Loss: 0.073 | Acc: 97.271% \n",
      "[epoch:26, iter:2472] Loss: 0.073 | Acc: 97.309% \n",
      "[epoch:26, iter:2473] Loss: 0.072 | Acc: 97.346% \n",
      "[epoch:26, iter:2474] Loss: 0.071 | Acc: 97.382% \n",
      "[epoch:26, iter:2475] Loss: 0.073 | Acc: 97.333% \n",
      "[epoch:26, iter:2476] Loss: 0.072 | Acc: 97.368% \n",
      "[epoch:26, iter:2477] Loss: 0.071 | Acc: 97.403% \n",
      "[epoch:26, iter:2478] Loss: 0.072 | Acc: 97.356% \n",
      "[epoch:26, iter:2479] Loss: 0.072 | Acc: 97.310% \n",
      "[epoch:26, iter:2480] Loss: 0.073 | Acc: 97.266% \n",
      "[epoch:26, iter:2481] Loss: 0.072 | Acc: 97.299% \n",
      "[epoch:26, iter:2482] Loss: 0.071 | Acc: 97.332% \n",
      "[epoch:26, iter:2483] Loss: 0.071 | Acc: 97.364% \n",
      "[epoch:26, iter:2484] Loss: 0.070 | Acc: 97.321% \n",
      "[epoch:26, iter:2485] Loss: 0.070 | Acc: 97.353% \n",
      "[epoch:26, iter:2486] Loss: 0.070 | Acc: 97.311% \n",
      "[epoch:26, iter:2487] Loss: 0.069 | Acc: 97.342% \n",
      "[epoch:26, iter:2488] Loss: 0.070 | Acc: 97.301% \n",
      "[epoch:26, iter:2489] Loss: 0.069 | Acc: 97.331% \n",
      "[epoch:26, iter:2490] Loss: 0.070 | Acc: 97.292% \n",
      "[epoch:26, iter:2491] Loss: 0.070 | Acc: 97.321% \n",
      "[epoch:26, iter:2492] Loss: 0.069 | Acc: 97.351% \n",
      "[epoch:26, iter:2493] Loss: 0.070 | Acc: 97.312% \n",
      "[epoch:26, iter:2494] Loss: 0.071 | Acc: 97.207% \n",
      "[epoch:26, iter:2495] Loss: 0.070 | Acc: 97.237% \n",
      "[epoch:26, iter:2496] Loss: 0.072 | Acc: 97.188% \n",
      "Waiting Test!\n",
      "测试分类准确率为：82.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 27\n",
      "[epoch:27, iter:2497] Loss: 0.135 | Acc: 87.500% \n",
      "[epoch:27, iter:2498] Loss: 0.071 | Acc: 93.750% \n",
      "[epoch:27, iter:2499] Loss: 0.055 | Acc: 95.833% \n",
      "[epoch:27, iter:2500] Loss: 0.043 | Acc: 96.875% \n",
      "[epoch:27, iter:2501] Loss: 0.036 | Acc: 97.500% \n",
      "[epoch:27, iter:2502] Loss: 0.031 | Acc: 97.917% \n",
      "[epoch:27, iter:2503] Loss: 0.053 | Acc: 96.429% \n",
      "[epoch:27, iter:2504] Loss: 0.052 | Acc: 96.875% \n",
      "[epoch:27, iter:2505] Loss: 0.073 | Acc: 95.833% \n",
      "[epoch:27, iter:2506] Loss: 0.067 | Acc: 96.250% \n",
      "[epoch:27, iter:2507] Loss: 0.071 | Acc: 96.023% \n",
      "[epoch:27, iter:2508] Loss: 0.069 | Acc: 96.354% \n",
      "[epoch:27, iter:2509] Loss: 0.064 | Acc: 96.635% \n",
      "[epoch:27, iter:2510] Loss: 0.060 | Acc: 96.875% \n",
      "[epoch:27, iter:2511] Loss: 0.061 | Acc: 96.667% \n",
      "[epoch:27, iter:2512] Loss: 0.061 | Acc: 96.484% \n",
      "[epoch:27, iter:2513] Loss: 0.059 | Acc: 96.691% \n",
      "[epoch:27, iter:2514] Loss: 0.057 | Acc: 96.875% \n",
      "[epoch:27, iter:2515] Loss: 0.063 | Acc: 96.711% \n",
      "[epoch:27, iter:2516] Loss: 0.061 | Acc: 96.875% \n",
      "[epoch:27, iter:2517] Loss: 0.060 | Acc: 96.726% \n",
      "[epoch:27, iter:2518] Loss: 0.065 | Acc: 96.307% \n",
      "[epoch:27, iter:2519] Loss: 0.070 | Acc: 95.924% \n",
      "[epoch:27, iter:2520] Loss: 0.067 | Acc: 96.094% \n",
      "[epoch:27, iter:2521] Loss: 0.065 | Acc: 96.250% \n",
      "[epoch:27, iter:2522] Loss: 0.063 | Acc: 96.394% \n",
      "[epoch:27, iter:2523] Loss: 0.063 | Acc: 96.296% \n",
      "[epoch:27, iter:2524] Loss: 0.062 | Acc: 96.205% \n",
      "[epoch:27, iter:2525] Loss: 0.062 | Acc: 96.336% \n",
      "[epoch:27, iter:2526] Loss: 0.060 | Acc: 96.458% \n",
      "[epoch:27, iter:2527] Loss: 0.059 | Acc: 96.573% \n",
      "[epoch:27, iter:2528] Loss: 0.060 | Acc: 96.484% \n",
      "[epoch:27, iter:2529] Loss: 0.059 | Acc: 96.591% \n",
      "[epoch:27, iter:2530] Loss: 0.068 | Acc: 96.140% \n",
      "[epoch:27, iter:2531] Loss: 0.066 | Acc: 96.250% \n",
      "[epoch:27, iter:2532] Loss: 0.064 | Acc: 96.354% \n",
      "[epoch:27, iter:2533] Loss: 0.063 | Acc: 96.453% \n",
      "[epoch:27, iter:2534] Loss: 0.064 | Acc: 96.382% \n",
      "[epoch:27, iter:2535] Loss: 0.062 | Acc: 96.474% \n",
      "[epoch:27, iter:2536] Loss: 0.061 | Acc: 96.562% \n",
      "[epoch:27, iter:2537] Loss: 0.061 | Acc: 96.646% \n",
      "[epoch:27, iter:2538] Loss: 0.060 | Acc: 96.726% \n",
      "[epoch:27, iter:2539] Loss: 0.059 | Acc: 96.802% \n",
      "[epoch:27, iter:2540] Loss: 0.061 | Acc: 96.875% \n",
      "[epoch:27, iter:2541] Loss: 0.061 | Acc: 96.806% \n",
      "[epoch:27, iter:2542] Loss: 0.059 | Acc: 96.875% \n",
      "[epoch:27, iter:2543] Loss: 0.059 | Acc: 96.941% \n",
      "[epoch:27, iter:2544] Loss: 0.058 | Acc: 97.005% \n",
      "[epoch:27, iter:2545] Loss: 0.057 | Acc: 97.066% \n",
      "[epoch:27, iter:2546] Loss: 0.056 | Acc: 97.125% \n",
      "[epoch:27, iter:2547] Loss: 0.055 | Acc: 97.181% \n",
      "[epoch:27, iter:2548] Loss: 0.057 | Acc: 97.115% \n",
      "[epoch:27, iter:2549] Loss: 0.056 | Acc: 97.170% \n",
      "[epoch:27, iter:2550] Loss: 0.055 | Acc: 97.222% \n",
      "[epoch:27, iter:2551] Loss: 0.055 | Acc: 97.273% \n",
      "[epoch:27, iter:2552] Loss: 0.065 | Acc: 96.763% \n",
      "[epoch:27, iter:2553] Loss: 0.064 | Acc: 96.820% \n",
      "[epoch:27, iter:2554] Loss: 0.068 | Acc: 96.767% \n",
      "[epoch:27, iter:2555] Loss: 0.066 | Acc: 96.822% \n",
      "[epoch:27, iter:2556] Loss: 0.065 | Acc: 96.875% \n",
      "[epoch:27, iter:2557] Loss: 0.064 | Acc: 96.926% \n",
      "[epoch:27, iter:2558] Loss: 0.064 | Acc: 96.976% \n",
      "[epoch:27, iter:2559] Loss: 0.063 | Acc: 97.024% \n",
      "[epoch:27, iter:2560] Loss: 0.062 | Acc: 97.070% \n",
      "[epoch:27, iter:2561] Loss: 0.062 | Acc: 97.115% \n",
      "[epoch:27, iter:2562] Loss: 0.061 | Acc: 97.159% \n",
      "[epoch:27, iter:2563] Loss: 0.060 | Acc: 97.201% \n",
      "[epoch:27, iter:2564] Loss: 0.061 | Acc: 97.151% \n",
      "[epoch:27, iter:2565] Loss: 0.060 | Acc: 97.192% \n",
      "[epoch:27, iter:2566] Loss: 0.061 | Acc: 97.143% \n",
      "[epoch:27, iter:2567] Loss: 0.065 | Acc: 97.095% \n",
      "[epoch:27, iter:2568] Loss: 0.064 | Acc: 97.135% \n",
      "[epoch:27, iter:2569] Loss: 0.063 | Acc: 97.175% \n",
      "[epoch:27, iter:2570] Loss: 0.062 | Acc: 97.213% \n",
      "[epoch:27, iter:2571] Loss: 0.061 | Acc: 97.250% \n",
      "[epoch:27, iter:2572] Loss: 0.061 | Acc: 97.286% \n",
      "[epoch:27, iter:2573] Loss: 0.060 | Acc: 97.321% \n",
      "[epoch:27, iter:2574] Loss: 0.059 | Acc: 97.356% \n",
      "[epoch:27, iter:2575] Loss: 0.059 | Acc: 97.389% \n",
      "[epoch:27, iter:2576] Loss: 0.059 | Acc: 97.344% \n",
      "[epoch:27, iter:2577] Loss: 0.059 | Acc: 97.299% \n",
      "[epoch:27, iter:2578] Loss: 0.058 | Acc: 97.332% \n",
      "[epoch:27, iter:2579] Loss: 0.059 | Acc: 97.289% \n",
      "[epoch:27, iter:2580] Loss: 0.058 | Acc: 97.321% \n",
      "[epoch:27, iter:2581] Loss: 0.058 | Acc: 97.353% \n",
      "[epoch:27, iter:2582] Loss: 0.058 | Acc: 97.384% \n",
      "[epoch:27, iter:2583] Loss: 0.057 | Acc: 97.414% \n",
      "[epoch:27, iter:2584] Loss: 0.057 | Acc: 97.443% \n",
      "[epoch:27, iter:2585] Loss: 0.056 | Acc: 97.472% \n",
      "[epoch:27, iter:2586] Loss: 0.056 | Acc: 97.500% \n",
      "[epoch:27, iter:2587] Loss: 0.056 | Acc: 97.459% \n",
      "[epoch:27, iter:2588] Loss: 0.057 | Acc: 97.486% \n",
      "[epoch:27, iter:2589] Loss: 0.057 | Acc: 97.446% \n",
      "[epoch:27, iter:2590] Loss: 0.057 | Acc: 97.473% \n",
      "[epoch:27, iter:2591] Loss: 0.056 | Acc: 97.500% \n",
      "[epoch:27, iter:2592] Loss: 0.057 | Acc: 97.449% \n",
      "Waiting Test!\n",
      "测试分类准确率为：88.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 28\n",
      "[epoch:28, iter:2593] Loss: 0.055 | Acc: 93.750% \n",
      "[epoch:28, iter:2594] Loss: 0.034 | Acc: 96.875% \n",
      "[epoch:28, iter:2595] Loss: 0.040 | Acc: 95.833% \n",
      "[epoch:28, iter:2596] Loss: 0.055 | Acc: 95.312% \n",
      "[epoch:28, iter:2597] Loss: 0.044 | Acc: 96.250% \n",
      "[epoch:28, iter:2598] Loss: 0.090 | Acc: 95.833% \n",
      "[epoch:28, iter:2599] Loss: 0.078 | Acc: 96.429% \n",
      "[epoch:28, iter:2600] Loss: 0.069 | Acc: 96.875% \n",
      "[epoch:28, iter:2601] Loss: 0.062 | Acc: 97.222% \n",
      "[epoch:28, iter:2602] Loss: 0.060 | Acc: 97.500% \n",
      "[epoch:28, iter:2603] Loss: 0.057 | Acc: 97.727% \n",
      "[epoch:28, iter:2604] Loss: 0.053 | Acc: 97.917% \n",
      "[epoch:28, iter:2605] Loss: 0.055 | Acc: 97.596% \n",
      "[epoch:28, iter:2606] Loss: 0.051 | Acc: 97.768% \n",
      "[epoch:28, iter:2607] Loss: 0.056 | Acc: 97.500% \n",
      "[epoch:28, iter:2608] Loss: 0.053 | Acc: 97.656% \n",
      "[epoch:28, iter:2609] Loss: 0.051 | Acc: 97.794% \n",
      "[epoch:28, iter:2610] Loss: 0.053 | Acc: 97.569% \n",
      "[epoch:28, iter:2611] Loss: 0.055 | Acc: 97.368% \n",
      "[epoch:28, iter:2612] Loss: 0.055 | Acc: 97.188% \n",
      "[epoch:28, iter:2613] Loss: 0.052 | Acc: 97.321% \n",
      "[epoch:28, iter:2614] Loss: 0.060 | Acc: 97.159% \n",
      "[epoch:28, iter:2615] Loss: 0.058 | Acc: 97.283% \n",
      "[epoch:28, iter:2616] Loss: 0.056 | Acc: 97.396% \n",
      "[epoch:28, iter:2617] Loss: 0.058 | Acc: 97.250% \n",
      "[epoch:28, iter:2618] Loss: 0.058 | Acc: 97.356% \n",
      "[epoch:28, iter:2619] Loss: 0.056 | Acc: 97.454% \n",
      "[epoch:28, iter:2620] Loss: 0.060 | Acc: 97.321% \n",
      "[epoch:28, iter:2621] Loss: 0.058 | Acc: 97.414% \n",
      "[epoch:28, iter:2622] Loss: 0.057 | Acc: 97.500% \n",
      "[epoch:28, iter:2623] Loss: 0.058 | Acc: 97.379% \n",
      "[epoch:28, iter:2624] Loss: 0.059 | Acc: 97.461% \n",
      "[epoch:28, iter:2625] Loss: 0.057 | Acc: 97.538% \n",
      "[epoch:28, iter:2626] Loss: 0.060 | Acc: 97.426% \n",
      "[epoch:28, iter:2627] Loss: 0.059 | Acc: 97.500% \n",
      "[epoch:28, iter:2628] Loss: 0.065 | Acc: 97.396% \n",
      "[epoch:28, iter:2629] Loss: 0.068 | Acc: 97.128% \n",
      "[epoch:28, iter:2630] Loss: 0.067 | Acc: 97.204% \n",
      "[epoch:28, iter:2631] Loss: 0.067 | Acc: 97.276% \n",
      "[epoch:28, iter:2632] Loss: 0.066 | Acc: 97.344% \n",
      "[epoch:28, iter:2633] Loss: 0.071 | Acc: 97.104% \n",
      "[epoch:28, iter:2634] Loss: 0.069 | Acc: 97.173% \n",
      "[epoch:28, iter:2635] Loss: 0.068 | Acc: 97.238% \n",
      "[epoch:28, iter:2636] Loss: 0.071 | Acc: 97.159% \n",
      "[epoch:28, iter:2637] Loss: 0.070 | Acc: 97.222% \n",
      "[epoch:28, iter:2638] Loss: 0.069 | Acc: 97.283% \n",
      "[epoch:28, iter:2639] Loss: 0.068 | Acc: 97.340% \n",
      "[epoch:28, iter:2640] Loss: 0.067 | Acc: 97.396% \n",
      "[epoch:28, iter:2641] Loss: 0.066 | Acc: 97.449% \n",
      "[epoch:28, iter:2642] Loss: 0.065 | Acc: 97.500% \n",
      "[epoch:28, iter:2643] Loss: 0.064 | Acc: 97.549% \n",
      "[epoch:28, iter:2644] Loss: 0.063 | Acc: 97.596% \n",
      "[epoch:28, iter:2645] Loss: 0.064 | Acc: 97.524% \n",
      "[epoch:28, iter:2646] Loss: 0.063 | Acc: 97.569% \n",
      "[epoch:28, iter:2647] Loss: 0.062 | Acc: 97.614% \n",
      "[epoch:28, iter:2648] Loss: 0.062 | Acc: 97.545% \n",
      "[epoch:28, iter:2649] Loss: 0.061 | Acc: 97.588% \n",
      "[epoch:28, iter:2650] Loss: 0.060 | Acc: 97.629% \n",
      "[epoch:28, iter:2651] Loss: 0.059 | Acc: 97.669% \n",
      "[epoch:28, iter:2652] Loss: 0.059 | Acc: 97.708% \n",
      "[epoch:28, iter:2653] Loss: 0.058 | Acc: 97.746% \n",
      "[epoch:28, iter:2654] Loss: 0.057 | Acc: 97.782% \n",
      "[epoch:28, iter:2655] Loss: 0.057 | Acc: 97.817% \n",
      "[epoch:28, iter:2656] Loss: 0.057 | Acc: 97.754% \n",
      "[epoch:28, iter:2657] Loss: 0.060 | Acc: 97.596% \n",
      "[epoch:28, iter:2658] Loss: 0.062 | Acc: 97.443% \n",
      "[epoch:28, iter:2659] Loss: 0.061 | Acc: 97.481% \n",
      "[epoch:28, iter:2660] Loss: 0.060 | Acc: 97.518% \n",
      "[epoch:28, iter:2661] Loss: 0.060 | Acc: 97.554% \n",
      "[epoch:28, iter:2662] Loss: 0.060 | Acc: 97.589% \n",
      "[epoch:28, iter:2663] Loss: 0.059 | Acc: 97.623% \n",
      "[epoch:28, iter:2664] Loss: 0.059 | Acc: 97.569% \n",
      "[epoch:28, iter:2665] Loss: 0.059 | Acc: 97.517% \n",
      "[epoch:28, iter:2666] Loss: 0.058 | Acc: 97.551% \n",
      "[epoch:28, iter:2667] Loss: 0.059 | Acc: 97.500% \n",
      "[epoch:28, iter:2668] Loss: 0.060 | Acc: 97.451% \n",
      "[epoch:28, iter:2669] Loss: 0.059 | Acc: 97.484% \n",
      "[epoch:28, iter:2670] Loss: 0.062 | Acc: 97.356% \n",
      "[epoch:28, iter:2671] Loss: 0.062 | Acc: 97.389% \n",
      "[epoch:28, iter:2672] Loss: 0.062 | Acc: 97.422% \n",
      "[epoch:28, iter:2673] Loss: 0.063 | Acc: 97.299% \n",
      "[epoch:28, iter:2674] Loss: 0.064 | Acc: 97.256% \n",
      "[epoch:28, iter:2675] Loss: 0.063 | Acc: 97.289% \n",
      "[epoch:28, iter:2676] Loss: 0.063 | Acc: 97.321% \n",
      "[epoch:28, iter:2677] Loss: 0.064 | Acc: 97.279% \n",
      "[epoch:28, iter:2678] Loss: 0.064 | Acc: 97.238% \n",
      "[epoch:28, iter:2679] Loss: 0.064 | Acc: 97.270% \n",
      "[epoch:28, iter:2680] Loss: 0.065 | Acc: 97.230% \n",
      "[epoch:28, iter:2681] Loss: 0.064 | Acc: 97.261% \n",
      "[epoch:28, iter:2682] Loss: 0.065 | Acc: 97.222% \n",
      "[epoch:28, iter:2683] Loss: 0.066 | Acc: 97.184% \n",
      "[epoch:28, iter:2684] Loss: 0.065 | Acc: 97.215% \n",
      "[epoch:28, iter:2685] Loss: 0.064 | Acc: 97.245% \n",
      "[epoch:28, iter:2686] Loss: 0.064 | Acc: 97.274% \n",
      "[epoch:28, iter:2687] Loss: 0.063 | Acc: 97.303% \n",
      "[epoch:28, iter:2688] Loss: 0.062 | Acc: 97.319% \n",
      "Waiting Test!\n",
      "测试分类准确率为：83.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 29\n",
      "[epoch:29, iter:2689] Loss: 0.055 | Acc: 93.750% \n",
      "[epoch:29, iter:2690] Loss: 0.030 | Acc: 96.875% \n",
      "[epoch:29, iter:2691] Loss: 0.022 | Acc: 97.917% \n",
      "[epoch:29, iter:2692] Loss: 0.019 | Acc: 98.438% \n",
      "[epoch:29, iter:2693] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:29, iter:2694] Loss: 0.030 | Acc: 97.917% \n",
      "[epoch:29, iter:2695] Loss: 0.032 | Acc: 98.214% \n",
      "[epoch:29, iter:2696] Loss: 0.029 | Acc: 98.438% \n",
      "[epoch:29, iter:2697] Loss: 0.026 | Acc: 98.611% \n",
      "[epoch:29, iter:2698] Loss: 0.024 | Acc: 98.750% \n",
      "[epoch:29, iter:2699] Loss: 0.023 | Acc: 98.864% \n",
      "[epoch:29, iter:2700] Loss: 0.021 | Acc: 98.958% \n",
      "[epoch:29, iter:2701] Loss: 0.020 | Acc: 99.038% \n",
      "[epoch:29, iter:2702] Loss: 0.031 | Acc: 98.214% \n",
      "[epoch:29, iter:2703] Loss: 0.038 | Acc: 97.917% \n",
      "[epoch:29, iter:2704] Loss: 0.045 | Acc: 97.656% \n",
      "[epoch:29, iter:2705] Loss: 0.051 | Acc: 97.059% \n",
      "[epoch:29, iter:2706] Loss: 0.048 | Acc: 97.222% \n",
      "[epoch:29, iter:2707] Loss: 0.046 | Acc: 97.368% \n",
      "[epoch:29, iter:2708] Loss: 0.045 | Acc: 97.500% \n",
      "[epoch:29, iter:2709] Loss: 0.043 | Acc: 97.619% \n",
      "[epoch:29, iter:2710] Loss: 0.041 | Acc: 97.727% \n",
      "[epoch:29, iter:2711] Loss: 0.041 | Acc: 97.826% \n",
      "[epoch:29, iter:2712] Loss: 0.039 | Acc: 97.917% \n",
      "[epoch:29, iter:2713] Loss: 0.038 | Acc: 98.000% \n",
      "[epoch:29, iter:2714] Loss: 0.038 | Acc: 98.077% \n",
      "[epoch:29, iter:2715] Loss: 0.043 | Acc: 97.917% \n",
      "[epoch:29, iter:2716] Loss: 0.050 | Acc: 97.768% \n",
      "[epoch:29, iter:2717] Loss: 0.048 | Acc: 97.845% \n",
      "[epoch:29, iter:2718] Loss: 0.048 | Acc: 97.917% \n",
      "[epoch:29, iter:2719] Loss: 0.047 | Acc: 97.984% \n",
      "[epoch:29, iter:2720] Loss: 0.048 | Acc: 97.852% \n",
      "[epoch:29, iter:2721] Loss: 0.047 | Acc: 97.917% \n",
      "[epoch:29, iter:2722] Loss: 0.046 | Acc: 97.978% \n",
      "[epoch:29, iter:2723] Loss: 0.044 | Acc: 98.036% \n",
      "[epoch:29, iter:2724] Loss: 0.049 | Acc: 97.743% \n",
      "[epoch:29, iter:2725] Loss: 0.048 | Acc: 97.804% \n",
      "[epoch:29, iter:2726] Loss: 0.047 | Acc: 97.862% \n",
      "[epoch:29, iter:2727] Loss: 0.047 | Acc: 97.917% \n",
      "[epoch:29, iter:2728] Loss: 0.046 | Acc: 97.969% \n",
      "[epoch:29, iter:2729] Loss: 0.046 | Acc: 97.866% \n",
      "[epoch:29, iter:2730] Loss: 0.045 | Acc: 97.917% \n",
      "[epoch:29, iter:2731] Loss: 0.052 | Acc: 97.529% \n",
      "[epoch:29, iter:2732] Loss: 0.056 | Acc: 97.301% \n",
      "[epoch:29, iter:2733] Loss: 0.055 | Acc: 97.361% \n",
      "[epoch:29, iter:2734] Loss: 0.054 | Acc: 97.418% \n",
      "[epoch:29, iter:2735] Loss: 0.052 | Acc: 97.473% \n",
      "[epoch:29, iter:2736] Loss: 0.051 | Acc: 97.526% \n",
      "[epoch:29, iter:2737] Loss: 0.052 | Acc: 97.449% \n",
      "[epoch:29, iter:2738] Loss: 0.052 | Acc: 97.500% \n",
      "[epoch:29, iter:2739] Loss: 0.053 | Acc: 97.426% \n",
      "[epoch:29, iter:2740] Loss: 0.055 | Acc: 97.356% \n",
      "[epoch:29, iter:2741] Loss: 0.054 | Acc: 97.406% \n",
      "[epoch:29, iter:2742] Loss: 0.053 | Acc: 97.454% \n",
      "[epoch:29, iter:2743] Loss: 0.052 | Acc: 97.500% \n",
      "[epoch:29, iter:2744] Loss: 0.053 | Acc: 97.433% \n",
      "[epoch:29, iter:2745] Loss: 0.053 | Acc: 97.478% \n",
      "[epoch:29, iter:2746] Loss: 0.052 | Acc: 97.522% \n",
      "[epoch:29, iter:2747] Loss: 0.051 | Acc: 97.564% \n",
      "[epoch:29, iter:2748] Loss: 0.051 | Acc: 97.604% \n",
      "[epoch:29, iter:2749] Loss: 0.051 | Acc: 97.541% \n",
      "[epoch:29, iter:2750] Loss: 0.050 | Acc: 97.581% \n",
      "[epoch:29, iter:2751] Loss: 0.050 | Acc: 97.619% \n",
      "[epoch:29, iter:2752] Loss: 0.051 | Acc: 97.559% \n",
      "[epoch:29, iter:2753] Loss: 0.050 | Acc: 97.596% \n",
      "[epoch:29, iter:2754] Loss: 0.049 | Acc: 97.633% \n",
      "[epoch:29, iter:2755] Loss: 0.050 | Acc: 97.575% \n",
      "[epoch:29, iter:2756] Loss: 0.050 | Acc: 97.610% \n",
      "[epoch:29, iter:2757] Loss: 0.049 | Acc: 97.645% \n",
      "[epoch:29, iter:2758] Loss: 0.049 | Acc: 97.679% \n",
      "[epoch:29, iter:2759] Loss: 0.051 | Acc: 97.623% \n",
      "[epoch:29, iter:2760] Loss: 0.050 | Acc: 97.656% \n",
      "[epoch:29, iter:2761] Loss: 0.050 | Acc: 97.688% \n",
      "[epoch:29, iter:2762] Loss: 0.049 | Acc: 97.720% \n",
      "[epoch:29, iter:2763] Loss: 0.049 | Acc: 97.750% \n",
      "[epoch:29, iter:2764] Loss: 0.048 | Acc: 97.780% \n",
      "[epoch:29, iter:2765] Loss: 0.047 | Acc: 97.808% \n",
      "[epoch:29, iter:2766] Loss: 0.047 | Acc: 97.756% \n",
      "[epoch:29, iter:2767] Loss: 0.047 | Acc: 97.785% \n",
      "[epoch:29, iter:2768] Loss: 0.047 | Acc: 97.812% \n",
      "[epoch:29, iter:2769] Loss: 0.050 | Acc: 97.608% \n",
      "[epoch:29, iter:2770] Loss: 0.050 | Acc: 97.637% \n",
      "[epoch:29, iter:2771] Loss: 0.050 | Acc: 97.666% \n",
      "[epoch:29, iter:2772] Loss: 0.051 | Acc: 97.619% \n",
      "[epoch:29, iter:2773] Loss: 0.050 | Acc: 97.647% \n",
      "[epoch:29, iter:2774] Loss: 0.051 | Acc: 97.602% \n",
      "[epoch:29, iter:2775] Loss: 0.051 | Acc: 97.629% \n",
      "[epoch:29, iter:2776] Loss: 0.050 | Acc: 97.656% \n",
      "[epoch:29, iter:2777] Loss: 0.050 | Acc: 97.683% \n",
      "[epoch:29, iter:2778] Loss: 0.050 | Acc: 97.708% \n",
      "[epoch:29, iter:2779] Loss: 0.050 | Acc: 97.734% \n",
      "[epoch:29, iter:2780] Loss: 0.049 | Acc: 97.758% \n",
      "[epoch:29, iter:2781] Loss: 0.049 | Acc: 97.782% \n",
      "[epoch:29, iter:2782] Loss: 0.049 | Acc: 97.806% \n",
      "[epoch:29, iter:2783] Loss: 0.049 | Acc: 97.763% \n",
      "[epoch:29, iter:2784] Loss: 0.049 | Acc: 97.776% \n",
      "Waiting Test!\n",
      "测试分类准确率为：89.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 30\n",
      "[epoch:30, iter:2785] Loss: 0.260 | Acc: 93.750% \n",
      "[epoch:30, iter:2786] Loss: 0.209 | Acc: 93.750% \n",
      "[epoch:30, iter:2787] Loss: 0.152 | Acc: 95.833% \n",
      "[epoch:30, iter:2788] Loss: 0.124 | Acc: 96.875% \n",
      "[epoch:30, iter:2789] Loss: 0.108 | Acc: 97.500% \n",
      "[epoch:30, iter:2790] Loss: 0.092 | Acc: 97.917% \n",
      "[epoch:30, iter:2791] Loss: 0.079 | Acc: 98.214% \n",
      "[epoch:30, iter:2792] Loss: 0.070 | Acc: 98.438% \n",
      "[epoch:30, iter:2793] Loss: 0.062 | Acc: 98.611% \n",
      "[epoch:30, iter:2794] Loss: 0.056 | Acc: 98.750% \n",
      "[epoch:30, iter:2795] Loss: 0.065 | Acc: 98.295% \n",
      "[epoch:30, iter:2796] Loss: 0.079 | Acc: 97.396% \n",
      "[epoch:30, iter:2797] Loss: 0.080 | Acc: 97.115% \n",
      "[epoch:30, iter:2798] Loss: 0.075 | Acc: 97.321% \n",
      "[epoch:30, iter:2799] Loss: 0.072 | Acc: 97.500% \n",
      "[epoch:30, iter:2800] Loss: 0.075 | Acc: 96.875% \n",
      "[epoch:30, iter:2801] Loss: 0.071 | Acc: 97.059% \n",
      "[epoch:30, iter:2802] Loss: 0.069 | Acc: 97.222% \n",
      "[epoch:30, iter:2803] Loss: 0.066 | Acc: 97.368% \n",
      "[epoch:30, iter:2804] Loss: 0.068 | Acc: 97.188% \n",
      "[epoch:30, iter:2805] Loss: 0.069 | Acc: 97.024% \n",
      "[epoch:30, iter:2806] Loss: 0.074 | Acc: 96.875% \n",
      "[epoch:30, iter:2807] Loss: 0.080 | Acc: 96.467% \n",
      "[epoch:30, iter:2808] Loss: 0.077 | Acc: 96.615% \n",
      "[epoch:30, iter:2809] Loss: 0.075 | Acc: 96.750% \n",
      "[epoch:30, iter:2810] Loss: 0.072 | Acc: 96.875% \n",
      "[epoch:30, iter:2811] Loss: 0.070 | Acc: 96.991% \n",
      "[epoch:30, iter:2812] Loss: 0.068 | Acc: 97.098% \n",
      "[epoch:30, iter:2813] Loss: 0.066 | Acc: 97.198% \n",
      "[epoch:30, iter:2814] Loss: 0.064 | Acc: 97.292% \n",
      "[epoch:30, iter:2815] Loss: 0.062 | Acc: 97.379% \n",
      "[epoch:30, iter:2816] Loss: 0.060 | Acc: 97.461% \n",
      "[epoch:30, iter:2817] Loss: 0.067 | Acc: 96.780% \n",
      "[epoch:30, iter:2818] Loss: 0.066 | Acc: 96.875% \n",
      "[epoch:30, iter:2819] Loss: 0.066 | Acc: 96.786% \n",
      "[epoch:30, iter:2820] Loss: 0.073 | Acc: 96.354% \n",
      "[epoch:30, iter:2821] Loss: 0.072 | Acc: 96.453% \n",
      "[epoch:30, iter:2822] Loss: 0.070 | Acc: 96.546% \n",
      "[epoch:30, iter:2823] Loss: 0.069 | Acc: 96.635% \n",
      "[epoch:30, iter:2824] Loss: 0.071 | Acc: 96.562% \n",
      "[epoch:30, iter:2825] Loss: 0.074 | Acc: 96.494% \n",
      "[epoch:30, iter:2826] Loss: 0.072 | Acc: 96.577% \n",
      "[epoch:30, iter:2827] Loss: 0.071 | Acc: 96.657% \n",
      "[epoch:30, iter:2828] Loss: 0.070 | Acc: 96.733% \n",
      "[epoch:30, iter:2829] Loss: 0.070 | Acc: 96.806% \n",
      "[epoch:30, iter:2830] Loss: 0.069 | Acc: 96.875% \n",
      "[epoch:30, iter:2831] Loss: 0.069 | Acc: 96.809% \n",
      "[epoch:30, iter:2832] Loss: 0.067 | Acc: 96.875% \n",
      "[epoch:30, iter:2833] Loss: 0.067 | Acc: 96.939% \n",
      "[epoch:30, iter:2834] Loss: 0.070 | Acc: 96.875% \n",
      "[epoch:30, iter:2835] Loss: 0.073 | Acc: 96.691% \n",
      "[epoch:30, iter:2836] Loss: 0.072 | Acc: 96.755% \n",
      "[epoch:30, iter:2837] Loss: 0.070 | Acc: 96.816% \n",
      "[epoch:30, iter:2838] Loss: 0.069 | Acc: 96.875% \n",
      "[epoch:30, iter:2839] Loss: 0.069 | Acc: 96.932% \n",
      "[epoch:30, iter:2840] Loss: 0.068 | Acc: 96.987% \n",
      "[epoch:30, iter:2841] Loss: 0.067 | Acc: 97.039% \n",
      "[epoch:30, iter:2842] Loss: 0.066 | Acc: 97.091% \n",
      "[epoch:30, iter:2843] Loss: 0.066 | Acc: 97.034% \n",
      "[epoch:30, iter:2844] Loss: 0.066 | Acc: 96.979% \n",
      "[epoch:30, iter:2845] Loss: 0.069 | Acc: 96.824% \n",
      "[epoch:30, iter:2846] Loss: 0.070 | Acc: 96.774% \n",
      "[epoch:30, iter:2847] Loss: 0.070 | Acc: 96.726% \n",
      "[epoch:30, iter:2848] Loss: 0.070 | Acc: 96.777% \n",
      "[epoch:30, iter:2849] Loss: 0.071 | Acc: 96.731% \n",
      "[epoch:30, iter:2850] Loss: 0.070 | Acc: 96.780% \n",
      "[epoch:30, iter:2851] Loss: 0.069 | Acc: 96.828% \n",
      "[epoch:30, iter:2852] Loss: 0.069 | Acc: 96.783% \n",
      "[epoch:30, iter:2853] Loss: 0.068 | Acc: 96.830% \n",
      "[epoch:30, iter:2854] Loss: 0.069 | Acc: 96.786% \n",
      "[epoch:30, iter:2855] Loss: 0.068 | Acc: 96.831% \n",
      "[epoch:30, iter:2856] Loss: 0.071 | Acc: 96.701% \n",
      "[epoch:30, iter:2857] Loss: 0.071 | Acc: 96.661% \n",
      "[epoch:30, iter:2858] Loss: 0.072 | Acc: 96.622% \n",
      "[epoch:30, iter:2859] Loss: 0.074 | Acc: 96.500% \n",
      "[epoch:30, iter:2860] Loss: 0.074 | Acc: 96.546% \n",
      "[epoch:30, iter:2861] Loss: 0.073 | Acc: 96.591% \n",
      "[epoch:30, iter:2862] Loss: 0.073 | Acc: 96.635% \n",
      "[epoch:30, iter:2863] Loss: 0.073 | Acc: 96.598% \n",
      "[epoch:30, iter:2864] Loss: 0.072 | Acc: 96.641% \n",
      "[epoch:30, iter:2865] Loss: 0.072 | Acc: 96.605% \n",
      "[epoch:30, iter:2866] Loss: 0.071 | Acc: 96.646% \n",
      "[epoch:30, iter:2867] Loss: 0.071 | Acc: 96.687% \n",
      "[epoch:30, iter:2868] Loss: 0.075 | Acc: 96.577% \n",
      "[epoch:30, iter:2869] Loss: 0.075 | Acc: 96.544% \n",
      "[epoch:30, iter:2870] Loss: 0.074 | Acc: 96.584% \n",
      "[epoch:30, iter:2871] Loss: 0.073 | Acc: 96.624% \n",
      "[epoch:30, iter:2872] Loss: 0.074 | Acc: 96.591% \n",
      "[epoch:30, iter:2873] Loss: 0.074 | Acc: 96.559% \n",
      "[epoch:30, iter:2874] Loss: 0.074 | Acc: 96.597% \n",
      "[epoch:30, iter:2875] Loss: 0.075 | Acc: 96.497% \n",
      "[epoch:30, iter:2876] Loss: 0.075 | Acc: 96.535% \n",
      "[epoch:30, iter:2877] Loss: 0.075 | Acc: 96.505% \n",
      "[epoch:30, iter:2878] Loss: 0.075 | Acc: 96.476% \n",
      "[epoch:30, iter:2879] Loss: 0.074 | Acc: 96.513% \n",
      "[epoch:30, iter:2880] Loss: 0.074 | Acc: 96.534% \n",
      "Waiting Test!\n",
      "测试分类准确率为：81.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 31\n",
      "[epoch:31, iter:2881] Loss: 0.125 | Acc: 93.750% \n",
      "[epoch:31, iter:2882] Loss: 0.065 | Acc: 96.875% \n",
      "[epoch:31, iter:2883] Loss: 0.044 | Acc: 97.917% \n",
      "[epoch:31, iter:2884] Loss: 0.049 | Acc: 98.438% \n",
      "[epoch:31, iter:2885] Loss: 0.077 | Acc: 96.250% \n",
      "[epoch:31, iter:2886] Loss: 0.065 | Acc: 96.875% \n",
      "[epoch:31, iter:2887] Loss: 0.057 | Acc: 97.321% \n",
      "[epoch:31, iter:2888] Loss: 0.061 | Acc: 96.875% \n",
      "[epoch:31, iter:2889] Loss: 0.059 | Acc: 97.222% \n",
      "[epoch:31, iter:2890] Loss: 0.054 | Acc: 97.500% \n",
      "[epoch:31, iter:2891] Loss: 0.049 | Acc: 97.727% \n",
      "[epoch:31, iter:2892] Loss: 0.050 | Acc: 97.917% \n",
      "[epoch:31, iter:2893] Loss: 0.047 | Acc: 98.077% \n",
      "[epoch:31, iter:2894] Loss: 0.044 | Acc: 98.214% \n",
      "[epoch:31, iter:2895] Loss: 0.076 | Acc: 97.083% \n",
      "[epoch:31, iter:2896] Loss: 0.076 | Acc: 96.875% \n",
      "[epoch:31, iter:2897] Loss: 0.082 | Acc: 96.691% \n",
      "[epoch:31, iter:2898] Loss: 0.082 | Acc: 96.528% \n",
      "[epoch:31, iter:2899] Loss: 0.080 | Acc: 96.382% \n",
      "[epoch:31, iter:2900] Loss: 0.081 | Acc: 96.250% \n",
      "[epoch:31, iter:2901] Loss: 0.078 | Acc: 96.429% \n",
      "[epoch:31, iter:2902] Loss: 0.080 | Acc: 96.307% \n",
      "[epoch:31, iter:2903] Loss: 0.096 | Acc: 95.380% \n",
      "[epoch:31, iter:2904] Loss: 0.096 | Acc: 95.312% \n",
      "[epoch:31, iter:2905] Loss: 0.092 | Acc: 95.500% \n",
      "[epoch:31, iter:2906] Loss: 0.089 | Acc: 95.673% \n",
      "[epoch:31, iter:2907] Loss: 0.094 | Acc: 95.370% \n",
      "[epoch:31, iter:2908] Loss: 0.093 | Acc: 95.312% \n",
      "[epoch:31, iter:2909] Loss: 0.094 | Acc: 95.259% \n",
      "[epoch:31, iter:2910] Loss: 0.092 | Acc: 95.417% \n",
      "[epoch:31, iter:2911] Loss: 0.093 | Acc: 95.363% \n",
      "[epoch:31, iter:2912] Loss: 0.095 | Acc: 95.117% \n",
      "[epoch:31, iter:2913] Loss: 0.094 | Acc: 95.076% \n",
      "[epoch:31, iter:2914] Loss: 0.092 | Acc: 95.221% \n",
      "[epoch:31, iter:2915] Loss: 0.094 | Acc: 95.179% \n",
      "[epoch:31, iter:2916] Loss: 0.100 | Acc: 94.965% \n",
      "[epoch:31, iter:2917] Loss: 0.098 | Acc: 95.101% \n",
      "[epoch:31, iter:2918] Loss: 0.098 | Acc: 95.066% \n",
      "[epoch:31, iter:2919] Loss: 0.096 | Acc: 95.192% \n",
      "[epoch:31, iter:2920] Loss: 0.096 | Acc: 95.312% \n",
      "[epoch:31, iter:2921] Loss: 0.098 | Acc: 95.122% \n",
      "[epoch:31, iter:2922] Loss: 0.096 | Acc: 95.238% \n",
      "[epoch:31, iter:2923] Loss: 0.096 | Acc: 95.203% \n",
      "[epoch:31, iter:2924] Loss: 0.095 | Acc: 95.312% \n",
      "[epoch:31, iter:2925] Loss: 0.094 | Acc: 95.278% \n",
      "[epoch:31, iter:2926] Loss: 0.093 | Acc: 95.380% \n",
      "[epoch:31, iter:2927] Loss: 0.091 | Acc: 95.479% \n",
      "[epoch:31, iter:2928] Loss: 0.090 | Acc: 95.573% \n",
      "[epoch:31, iter:2929] Loss: 0.088 | Acc: 95.663% \n",
      "[epoch:31, iter:2930] Loss: 0.098 | Acc: 95.250% \n",
      "[epoch:31, iter:2931] Loss: 0.096 | Acc: 95.343% \n",
      "[epoch:31, iter:2932] Loss: 0.095 | Acc: 95.433% \n",
      "[epoch:31, iter:2933] Loss: 0.093 | Acc: 95.519% \n",
      "[epoch:31, iter:2934] Loss: 0.092 | Acc: 95.602% \n",
      "[epoch:31, iter:2935] Loss: 0.092 | Acc: 95.568% \n",
      "[epoch:31, iter:2936] Loss: 0.091 | Acc: 95.647% \n",
      "[epoch:31, iter:2937] Loss: 0.089 | Acc: 95.724% \n",
      "[epoch:31, iter:2938] Loss: 0.088 | Acc: 95.797% \n",
      "[epoch:31, iter:2939] Loss: 0.089 | Acc: 95.763% \n",
      "[epoch:31, iter:2940] Loss: 0.088 | Acc: 95.833% \n",
      "[epoch:31, iter:2941] Loss: 0.089 | Acc: 95.799% \n",
      "[epoch:31, iter:2942] Loss: 0.088 | Acc: 95.867% \n",
      "[epoch:31, iter:2943] Loss: 0.087 | Acc: 95.933% \n",
      "[epoch:31, iter:2944] Loss: 0.086 | Acc: 95.996% \n",
      "[epoch:31, iter:2945] Loss: 0.084 | Acc: 96.058% \n",
      "[epoch:31, iter:2946] Loss: 0.086 | Acc: 95.928% \n",
      "[epoch:31, iter:2947] Loss: 0.085 | Acc: 95.989% \n",
      "[epoch:31, iter:2948] Loss: 0.085 | Acc: 95.956% \n",
      "[epoch:31, iter:2949] Loss: 0.084 | Acc: 96.014% \n",
      "[epoch:31, iter:2950] Loss: 0.085 | Acc: 95.893% \n",
      "[epoch:31, iter:2951] Loss: 0.084 | Acc: 95.951% \n",
      "[epoch:31, iter:2952] Loss: 0.083 | Acc: 96.007% \n",
      "[epoch:31, iter:2953] Loss: 0.087 | Acc: 95.805% \n",
      "[epoch:31, iter:2954] Loss: 0.086 | Acc: 95.861% \n",
      "[epoch:31, iter:2955] Loss: 0.086 | Acc: 95.833% \n",
      "[epoch:31, iter:2956] Loss: 0.085 | Acc: 95.888% \n",
      "[epoch:31, iter:2957] Loss: 0.085 | Acc: 95.779% \n",
      "[epoch:31, iter:2958] Loss: 0.084 | Acc: 95.833% \n",
      "[epoch:31, iter:2959] Loss: 0.084 | Acc: 95.807% \n",
      "[epoch:31, iter:2960] Loss: 0.088 | Acc: 95.781% \n",
      "[epoch:31, iter:2961] Loss: 0.089 | Acc: 95.756% \n",
      "[epoch:31, iter:2962] Loss: 0.088 | Acc: 95.808% \n",
      "[epoch:31, iter:2963] Loss: 0.088 | Acc: 95.783% \n",
      "[epoch:31, iter:2964] Loss: 0.087 | Acc: 95.833% \n",
      "[epoch:31, iter:2965] Loss: 0.086 | Acc: 95.882% \n",
      "[epoch:31, iter:2966] Loss: 0.086 | Acc: 95.930% \n",
      "[epoch:31, iter:2967] Loss: 0.085 | Acc: 95.977% \n",
      "[epoch:31, iter:2968] Loss: 0.084 | Acc: 96.023% \n",
      "[epoch:31, iter:2969] Loss: 0.083 | Acc: 96.067% \n",
      "[epoch:31, iter:2970] Loss: 0.083 | Acc: 96.111% \n",
      "[epoch:31, iter:2971] Loss: 0.084 | Acc: 96.085% \n",
      "[epoch:31, iter:2972] Loss: 0.084 | Acc: 96.128% \n",
      "[epoch:31, iter:2973] Loss: 0.083 | Acc: 96.169% \n",
      "[epoch:31, iter:2974] Loss: 0.083 | Acc: 96.144% \n",
      "[epoch:31, iter:2975] Loss: 0.082 | Acc: 96.184% \n",
      "[epoch:31, iter:2976] Loss: 0.082 | Acc: 96.207% \n",
      "Waiting Test!\n",
      "测试分类准确率为：72.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 32\n",
      "[epoch:32, iter:2977] Loss: 0.239 | Acc: 87.500% \n",
      "[epoch:32, iter:2978] Loss: 0.281 | Acc: 84.375% \n",
      "[epoch:32, iter:2979] Loss: 0.205 | Acc: 87.500% \n",
      "[epoch:32, iter:2980] Loss: 0.155 | Acc: 90.625% \n",
      "[epoch:32, iter:2981] Loss: 0.133 | Acc: 92.500% \n",
      "[epoch:32, iter:2982] Loss: 0.111 | Acc: 93.750% \n",
      "[epoch:32, iter:2983] Loss: 0.098 | Acc: 94.643% \n",
      "[epoch:32, iter:2984] Loss: 0.145 | Acc: 92.969% \n",
      "[epoch:32, iter:2985] Loss: 0.167 | Acc: 91.667% \n",
      "[epoch:32, iter:2986] Loss: 0.168 | Acc: 91.875% \n",
      "[epoch:32, iter:2987] Loss: 0.168 | Acc: 91.477% \n",
      "[epoch:32, iter:2988] Loss: 0.156 | Acc: 92.188% \n",
      "[epoch:32, iter:2989] Loss: 0.147 | Acc: 92.788% \n",
      "[epoch:32, iter:2990] Loss: 0.138 | Acc: 93.304% \n",
      "[epoch:32, iter:2991] Loss: 0.142 | Acc: 93.333% \n",
      "[epoch:32, iter:2992] Loss: 0.146 | Acc: 93.359% \n",
      "[epoch:32, iter:2993] Loss: 0.138 | Acc: 93.750% \n",
      "[epoch:32, iter:2994] Loss: 0.131 | Acc: 94.097% \n",
      "[epoch:32, iter:2995] Loss: 0.127 | Acc: 94.079% \n",
      "[epoch:32, iter:2996] Loss: 0.125 | Acc: 94.062% \n",
      "[epoch:32, iter:2997] Loss: 0.122 | Acc: 94.048% \n",
      "[epoch:32, iter:2998] Loss: 0.120 | Acc: 94.034% \n",
      "[epoch:32, iter:2999] Loss: 0.117 | Acc: 94.022% \n",
      "[epoch:32, iter:3000] Loss: 0.114 | Acc: 94.271% \n",
      "[epoch:32, iter:3001] Loss: 0.111 | Acc: 94.500% \n",
      "[epoch:32, iter:3002] Loss: 0.107 | Acc: 94.712% \n",
      "[epoch:32, iter:3003] Loss: 0.106 | Acc: 94.676% \n",
      "[epoch:32, iter:3004] Loss: 0.105 | Acc: 94.866% \n",
      "[epoch:32, iter:3005] Loss: 0.107 | Acc: 94.828% \n",
      "[epoch:32, iter:3006] Loss: 0.104 | Acc: 95.000% \n",
      "[epoch:32, iter:3007] Loss: 0.101 | Acc: 95.161% \n",
      "[epoch:32, iter:3008] Loss: 0.098 | Acc: 95.312% \n",
      "[epoch:32, iter:3009] Loss: 0.102 | Acc: 95.265% \n",
      "[epoch:32, iter:3010] Loss: 0.099 | Acc: 95.404% \n",
      "[epoch:32, iter:3011] Loss: 0.096 | Acc: 95.536% \n",
      "[epoch:32, iter:3012] Loss: 0.102 | Acc: 95.139% \n",
      "[epoch:32, iter:3013] Loss: 0.103 | Acc: 95.101% \n",
      "[epoch:32, iter:3014] Loss: 0.101 | Acc: 95.230% \n",
      "[epoch:32, iter:3015] Loss: 0.099 | Acc: 95.353% \n",
      "[epoch:32, iter:3016] Loss: 0.097 | Acc: 95.469% \n",
      "[epoch:32, iter:3017] Loss: 0.097 | Acc: 95.427% \n",
      "[epoch:32, iter:3018] Loss: 0.095 | Acc: 95.536% \n",
      "[epoch:32, iter:3019] Loss: 0.110 | Acc: 94.913% \n",
      "[epoch:32, iter:3020] Loss: 0.107 | Acc: 95.028% \n",
      "[epoch:32, iter:3021] Loss: 0.106 | Acc: 95.139% \n",
      "[epoch:32, iter:3022] Loss: 0.109 | Acc: 94.973% \n",
      "[epoch:32, iter:3023] Loss: 0.107 | Acc: 95.080% \n",
      "[epoch:32, iter:3024] Loss: 0.105 | Acc: 95.182% \n",
      "[epoch:32, iter:3025] Loss: 0.106 | Acc: 95.281% \n",
      "[epoch:32, iter:3026] Loss: 0.104 | Acc: 95.375% \n",
      "[epoch:32, iter:3027] Loss: 0.102 | Acc: 95.466% \n",
      "[epoch:32, iter:3028] Loss: 0.102 | Acc: 95.433% \n",
      "[epoch:32, iter:3029] Loss: 0.101 | Acc: 95.401% \n",
      "[epoch:32, iter:3030] Loss: 0.100 | Acc: 95.486% \n",
      "[epoch:32, iter:3031] Loss: 0.098 | Acc: 95.568% \n",
      "[epoch:32, iter:3032] Loss: 0.097 | Acc: 95.647% \n",
      "[epoch:32, iter:3033] Loss: 0.095 | Acc: 95.724% \n",
      "[epoch:32, iter:3034] Loss: 0.094 | Acc: 95.797% \n",
      "[epoch:32, iter:3035] Loss: 0.095 | Acc: 95.763% \n",
      "[epoch:32, iter:3036] Loss: 0.094 | Acc: 95.833% \n",
      "[epoch:32, iter:3037] Loss: 0.093 | Acc: 95.902% \n",
      "[epoch:32, iter:3038] Loss: 0.093 | Acc: 95.867% \n",
      "[epoch:32, iter:3039] Loss: 0.093 | Acc: 95.933% \n",
      "[epoch:32, iter:3040] Loss: 0.092 | Acc: 95.996% \n",
      "[epoch:32, iter:3041] Loss: 0.093 | Acc: 95.962% \n",
      "[epoch:32, iter:3042] Loss: 0.095 | Acc: 95.928% \n",
      "[epoch:32, iter:3043] Loss: 0.095 | Acc: 95.896% \n",
      "[epoch:32, iter:3044] Loss: 0.094 | Acc: 95.956% \n",
      "[epoch:32, iter:3045] Loss: 0.093 | Acc: 96.014% \n",
      "[epoch:32, iter:3046] Loss: 0.092 | Acc: 96.071% \n",
      "[epoch:32, iter:3047] Loss: 0.092 | Acc: 95.951% \n",
      "[epoch:32, iter:3048] Loss: 0.091 | Acc: 96.007% \n",
      "[epoch:32, iter:3049] Loss: 0.090 | Acc: 96.062% \n",
      "[epoch:32, iter:3050] Loss: 0.093 | Acc: 95.946% \n",
      "[epoch:32, iter:3051] Loss: 0.092 | Acc: 96.000% \n",
      "[epoch:32, iter:3052] Loss: 0.091 | Acc: 96.053% \n",
      "[epoch:32, iter:3053] Loss: 0.090 | Acc: 96.023% \n",
      "[epoch:32, iter:3054] Loss: 0.089 | Acc: 96.074% \n",
      "[epoch:32, iter:3055] Loss: 0.088 | Acc: 96.123% \n",
      "[epoch:32, iter:3056] Loss: 0.087 | Acc: 96.172% \n",
      "[epoch:32, iter:3057] Loss: 0.087 | Acc: 96.219% \n",
      "[epoch:32, iter:3058] Loss: 0.086 | Acc: 96.265% \n",
      "[epoch:32, iter:3059] Loss: 0.085 | Acc: 96.310% \n",
      "[epoch:32, iter:3060] Loss: 0.084 | Acc: 96.354% \n",
      "[epoch:32, iter:3061] Loss: 0.086 | Acc: 96.250% \n",
      "[epoch:32, iter:3062] Loss: 0.085 | Acc: 96.294% \n",
      "[epoch:32, iter:3063] Loss: 0.084 | Acc: 96.336% \n",
      "[epoch:32, iter:3064] Loss: 0.085 | Acc: 96.307% \n",
      "[epoch:32, iter:3065] Loss: 0.084 | Acc: 96.348% \n",
      "[epoch:32, iter:3066] Loss: 0.084 | Acc: 96.389% \n",
      "[epoch:32, iter:3067] Loss: 0.083 | Acc: 96.429% \n",
      "[epoch:32, iter:3068] Loss: 0.083 | Acc: 96.467% \n",
      "[epoch:32, iter:3069] Loss: 0.083 | Acc: 96.438% \n",
      "[epoch:32, iter:3070] Loss: 0.082 | Acc: 96.476% \n",
      "[epoch:32, iter:3071] Loss: 0.081 | Acc: 96.513% \n",
      "[epoch:32, iter:3072] Loss: 0.080 | Acc: 96.534% \n",
      "Waiting Test!\n",
      "测试分类准确率为：77.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 33\n",
      "[epoch:33, iter:3073] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:33, iter:3074] Loss: 0.024 | Acc: 100.000% \n",
      "[epoch:33, iter:3075] Loss: 0.023 | Acc: 100.000% \n",
      "[epoch:33, iter:3076] Loss: 0.049 | Acc: 98.438% \n",
      "[epoch:33, iter:3077] Loss: 0.040 | Acc: 98.750% \n",
      "[epoch:33, iter:3078] Loss: 0.042 | Acc: 97.917% \n",
      "[epoch:33, iter:3079] Loss: 0.041 | Acc: 98.214% \n",
      "[epoch:33, iter:3080] Loss: 0.057 | Acc: 97.656% \n",
      "[epoch:33, iter:3081] Loss: 0.051 | Acc: 97.917% \n",
      "[epoch:33, iter:3082] Loss: 0.054 | Acc: 97.500% \n",
      "[epoch:33, iter:3083] Loss: 0.051 | Acc: 97.727% \n",
      "[epoch:33, iter:3084] Loss: 0.049 | Acc: 97.917% \n",
      "[epoch:33, iter:3085] Loss: 0.048 | Acc: 98.077% \n",
      "[epoch:33, iter:3086] Loss: 0.045 | Acc: 98.214% \n",
      "[epoch:33, iter:3087] Loss: 0.043 | Acc: 98.333% \n",
      "[epoch:33, iter:3088] Loss: 0.041 | Acc: 98.438% \n",
      "[epoch:33, iter:3089] Loss: 0.041 | Acc: 98.529% \n",
      "[epoch:33, iter:3090] Loss: 0.039 | Acc: 98.611% \n",
      "[epoch:33, iter:3091] Loss: 0.040 | Acc: 98.355% \n",
      "[epoch:33, iter:3092] Loss: 0.040 | Acc: 98.438% \n",
      "[epoch:33, iter:3093] Loss: 0.039 | Acc: 98.512% \n",
      "[epoch:33, iter:3094] Loss: 0.037 | Acc: 98.580% \n",
      "[epoch:33, iter:3095] Loss: 0.035 | Acc: 98.641% \n",
      "[epoch:33, iter:3096] Loss: 0.034 | Acc: 98.698% \n",
      "[epoch:33, iter:3097] Loss: 0.033 | Acc: 98.750% \n",
      "[epoch:33, iter:3098] Loss: 0.037 | Acc: 98.317% \n",
      "[epoch:33, iter:3099] Loss: 0.036 | Acc: 98.380% \n",
      "[epoch:33, iter:3100] Loss: 0.034 | Acc: 98.438% \n",
      "[epoch:33, iter:3101] Loss: 0.034 | Acc: 98.491% \n",
      "[epoch:33, iter:3102] Loss: 0.037 | Acc: 98.333% \n",
      "[epoch:33, iter:3103] Loss: 0.038 | Acc: 98.185% \n",
      "[epoch:33, iter:3104] Loss: 0.036 | Acc: 98.242% \n",
      "[epoch:33, iter:3105] Loss: 0.039 | Acc: 98.106% \n",
      "[epoch:33, iter:3106] Loss: 0.041 | Acc: 97.978% \n",
      "[epoch:33, iter:3107] Loss: 0.039 | Acc: 98.036% \n",
      "[epoch:33, iter:3108] Loss: 0.039 | Acc: 98.090% \n",
      "[epoch:33, iter:3109] Loss: 0.039 | Acc: 98.142% \n",
      "[epoch:33, iter:3110] Loss: 0.041 | Acc: 98.026% \n",
      "[epoch:33, iter:3111] Loss: 0.052 | Acc: 97.917% \n",
      "[epoch:33, iter:3112] Loss: 0.056 | Acc: 97.656% \n",
      "[epoch:33, iter:3113] Loss: 0.056 | Acc: 97.713% \n",
      "[epoch:33, iter:3114] Loss: 0.055 | Acc: 97.768% \n",
      "[epoch:33, iter:3115] Loss: 0.057 | Acc: 97.674% \n",
      "[epoch:33, iter:3116] Loss: 0.055 | Acc: 97.727% \n",
      "[epoch:33, iter:3117] Loss: 0.054 | Acc: 97.778% \n",
      "[epoch:33, iter:3118] Loss: 0.053 | Acc: 97.826% \n",
      "[epoch:33, iter:3119] Loss: 0.053 | Acc: 97.872% \n",
      "[epoch:33, iter:3120] Loss: 0.052 | Acc: 97.917% \n",
      "[epoch:33, iter:3121] Loss: 0.052 | Acc: 97.959% \n",
      "[epoch:33, iter:3122] Loss: 0.052 | Acc: 98.000% \n",
      "[epoch:33, iter:3123] Loss: 0.054 | Acc: 97.917% \n",
      "[epoch:33, iter:3124] Loss: 0.053 | Acc: 97.957% \n",
      "[epoch:33, iter:3125] Loss: 0.052 | Acc: 97.995% \n",
      "[epoch:33, iter:3126] Loss: 0.051 | Acc: 98.032% \n",
      "[epoch:33, iter:3127] Loss: 0.050 | Acc: 98.068% \n",
      "[epoch:33, iter:3128] Loss: 0.051 | Acc: 97.991% \n",
      "[epoch:33, iter:3129] Loss: 0.051 | Acc: 98.026% \n",
      "[epoch:33, iter:3130] Loss: 0.050 | Acc: 98.060% \n",
      "[epoch:33, iter:3131] Loss: 0.051 | Acc: 97.987% \n",
      "[epoch:33, iter:3132] Loss: 0.053 | Acc: 97.917% \n",
      "[epoch:33, iter:3133] Loss: 0.054 | Acc: 97.848% \n",
      "[epoch:33, iter:3134] Loss: 0.054 | Acc: 97.782% \n",
      "[epoch:33, iter:3135] Loss: 0.055 | Acc: 97.718% \n",
      "[epoch:33, iter:3136] Loss: 0.055 | Acc: 97.754% \n",
      "[epoch:33, iter:3137] Loss: 0.054 | Acc: 97.788% \n",
      "[epoch:33, iter:3138] Loss: 0.054 | Acc: 97.822% \n",
      "[epoch:33, iter:3139] Loss: 0.053 | Acc: 97.854% \n",
      "[epoch:33, iter:3140] Loss: 0.052 | Acc: 97.886% \n",
      "[epoch:33, iter:3141] Loss: 0.057 | Acc: 97.645% \n",
      "[epoch:33, iter:3142] Loss: 0.056 | Acc: 97.679% \n",
      "[epoch:33, iter:3143] Loss: 0.055 | Acc: 97.711% \n",
      "[epoch:33, iter:3144] Loss: 0.054 | Acc: 97.743% \n",
      "[epoch:33, iter:3145] Loss: 0.054 | Acc: 97.774% \n",
      "[epoch:33, iter:3146] Loss: 0.056 | Acc: 97.635% \n",
      "[epoch:33, iter:3147] Loss: 0.056 | Acc: 97.667% \n",
      "[epoch:33, iter:3148] Loss: 0.055 | Acc: 97.697% \n",
      "[epoch:33, iter:3149] Loss: 0.056 | Acc: 97.646% \n",
      "[epoch:33, iter:3150] Loss: 0.057 | Acc: 97.676% \n",
      "[epoch:33, iter:3151] Loss: 0.059 | Acc: 97.547% \n",
      "[epoch:33, iter:3152] Loss: 0.058 | Acc: 97.578% \n",
      "[epoch:33, iter:3153] Loss: 0.058 | Acc: 97.608% \n",
      "[epoch:33, iter:3154] Loss: 0.058 | Acc: 97.637% \n",
      "[epoch:33, iter:3155] Loss: 0.057 | Acc: 97.666% \n",
      "[epoch:33, iter:3156] Loss: 0.057 | Acc: 97.693% \n",
      "[epoch:33, iter:3157] Loss: 0.058 | Acc: 97.647% \n",
      "[epoch:33, iter:3158] Loss: 0.057 | Acc: 97.674% \n",
      "[epoch:33, iter:3159] Loss: 0.058 | Acc: 97.629% \n",
      "[epoch:33, iter:3160] Loss: 0.058 | Acc: 97.585% \n",
      "[epoch:33, iter:3161] Loss: 0.058 | Acc: 97.612% \n",
      "[epoch:33, iter:3162] Loss: 0.058 | Acc: 97.569% \n",
      "[epoch:33, iter:3163] Loss: 0.058 | Acc: 97.596% \n",
      "[epoch:33, iter:3164] Loss: 0.057 | Acc: 97.622% \n",
      "[epoch:33, iter:3165] Loss: 0.057 | Acc: 97.648% \n",
      "[epoch:33, iter:3166] Loss: 0.056 | Acc: 97.673% \n",
      "[epoch:33, iter:3167] Loss: 0.056 | Acc: 97.697% \n",
      "[epoch:33, iter:3168] Loss: 0.056 | Acc: 97.711% \n",
      "Waiting Test!\n",
      "测试分类准确率为：79.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 34\n",
      "[epoch:34, iter:3169] Loss: 0.007 | Acc: 100.000% \n",
      "[epoch:34, iter:3170] Loss: 0.053 | Acc: 96.875% \n",
      "[epoch:34, iter:3171] Loss: 0.040 | Acc: 97.917% \n",
      "[epoch:34, iter:3172] Loss: 0.065 | Acc: 96.875% \n",
      "[epoch:34, iter:3173] Loss: 0.052 | Acc: 97.500% \n",
      "[epoch:34, iter:3174] Loss: 0.044 | Acc: 97.917% \n",
      "[epoch:34, iter:3175] Loss: 0.041 | Acc: 98.214% \n",
      "[epoch:34, iter:3176] Loss: 0.036 | Acc: 98.438% \n",
      "[epoch:34, iter:3177] Loss: 0.036 | Acc: 98.611% \n",
      "[epoch:34, iter:3178] Loss: 0.032 | Acc: 98.750% \n",
      "[epoch:34, iter:3179] Loss: 0.035 | Acc: 98.295% \n",
      "[epoch:34, iter:3180] Loss: 0.035 | Acc: 98.438% \n",
      "[epoch:34, iter:3181] Loss: 0.035 | Acc: 98.558% \n",
      "[epoch:34, iter:3182] Loss: 0.035 | Acc: 98.661% \n",
      "[epoch:34, iter:3183] Loss: 0.041 | Acc: 98.333% \n",
      "[epoch:34, iter:3184] Loss: 0.042 | Acc: 98.047% \n",
      "[epoch:34, iter:3185] Loss: 0.043 | Acc: 98.162% \n",
      "[epoch:34, iter:3186] Loss: 0.051 | Acc: 97.917% \n",
      "[epoch:34, iter:3187] Loss: 0.052 | Acc: 97.697% \n",
      "[epoch:34, iter:3188] Loss: 0.052 | Acc: 97.812% \n",
      "[epoch:34, iter:3189] Loss: 0.061 | Acc: 97.024% \n",
      "[epoch:34, iter:3190] Loss: 0.059 | Acc: 97.159% \n",
      "[epoch:34, iter:3191] Loss: 0.062 | Acc: 97.011% \n",
      "[epoch:34, iter:3192] Loss: 0.060 | Acc: 97.135% \n",
      "[epoch:34, iter:3193] Loss: 0.061 | Acc: 97.000% \n",
      "[epoch:34, iter:3194] Loss: 0.059 | Acc: 97.115% \n",
      "[epoch:34, iter:3195] Loss: 0.063 | Acc: 96.991% \n",
      "[epoch:34, iter:3196] Loss: 0.067 | Acc: 96.875% \n",
      "[epoch:34, iter:3197] Loss: 0.067 | Acc: 96.767% \n",
      "[epoch:34, iter:3198] Loss: 0.065 | Acc: 96.875% \n",
      "[epoch:34, iter:3199] Loss: 0.063 | Acc: 96.976% \n",
      "[epoch:34, iter:3200] Loss: 0.061 | Acc: 97.070% \n",
      "[epoch:34, iter:3201] Loss: 0.062 | Acc: 96.970% \n",
      "[epoch:34, iter:3202] Loss: 0.061 | Acc: 97.059% \n",
      "[epoch:34, iter:3203] Loss: 0.065 | Acc: 96.964% \n",
      "[epoch:34, iter:3204] Loss: 0.065 | Acc: 96.875% \n",
      "[epoch:34, iter:3205] Loss: 0.063 | Acc: 96.959% \n",
      "[epoch:34, iter:3206] Loss: 0.062 | Acc: 97.039% \n",
      "[epoch:34, iter:3207] Loss: 0.062 | Acc: 96.955% \n",
      "[epoch:34, iter:3208] Loss: 0.060 | Acc: 97.031% \n",
      "[epoch:34, iter:3209] Loss: 0.060 | Acc: 96.951% \n",
      "[epoch:34, iter:3210] Loss: 0.062 | Acc: 96.875% \n",
      "[epoch:34, iter:3211] Loss: 0.062 | Acc: 96.802% \n",
      "[epoch:34, iter:3212] Loss: 0.061 | Acc: 96.875% \n",
      "[epoch:34, iter:3213] Loss: 0.065 | Acc: 96.667% \n",
      "[epoch:34, iter:3214] Loss: 0.063 | Acc: 96.739% \n",
      "[epoch:34, iter:3215] Loss: 0.066 | Acc: 96.676% \n",
      "[epoch:34, iter:3216] Loss: 0.064 | Acc: 96.745% \n",
      "[epoch:34, iter:3217] Loss: 0.066 | Acc: 96.556% \n",
      "[epoch:34, iter:3218] Loss: 0.065 | Acc: 96.625% \n",
      "[epoch:34, iter:3219] Loss: 0.064 | Acc: 96.691% \n",
      "[epoch:34, iter:3220] Loss: 0.062 | Acc: 96.755% \n",
      "[epoch:34, iter:3221] Loss: 0.062 | Acc: 96.816% \n",
      "[epoch:34, iter:3222] Loss: 0.061 | Acc: 96.875% \n",
      "[epoch:34, iter:3223] Loss: 0.064 | Acc: 96.818% \n",
      "[epoch:34, iter:3224] Loss: 0.063 | Acc: 96.875% \n",
      "[epoch:34, iter:3225] Loss: 0.063 | Acc: 96.820% \n",
      "[epoch:34, iter:3226] Loss: 0.062 | Acc: 96.875% \n",
      "[epoch:34, iter:3227] Loss: 0.065 | Acc: 96.822% \n",
      "[epoch:34, iter:3228] Loss: 0.064 | Acc: 96.875% \n",
      "[epoch:34, iter:3229] Loss: 0.064 | Acc: 96.824% \n",
      "[epoch:34, iter:3230] Loss: 0.066 | Acc: 96.673% \n",
      "[epoch:34, iter:3231] Loss: 0.066 | Acc: 96.726% \n",
      "[epoch:34, iter:3232] Loss: 0.065 | Acc: 96.777% \n",
      "[epoch:34, iter:3233] Loss: 0.065 | Acc: 96.731% \n",
      "[epoch:34, iter:3234] Loss: 0.064 | Acc: 96.780% \n",
      "[epoch:34, iter:3235] Loss: 0.064 | Acc: 96.828% \n",
      "[epoch:34, iter:3236] Loss: 0.064 | Acc: 96.875% \n",
      "[epoch:34, iter:3237] Loss: 0.063 | Acc: 96.920% \n",
      "[epoch:34, iter:3238] Loss: 0.063 | Acc: 96.964% \n",
      "[epoch:34, iter:3239] Loss: 0.062 | Acc: 97.007% \n",
      "[epoch:34, iter:3240] Loss: 0.062 | Acc: 97.049% \n",
      "[epoch:34, iter:3241] Loss: 0.062 | Acc: 97.003% \n",
      "[epoch:34, iter:3242] Loss: 0.063 | Acc: 96.959% \n",
      "[epoch:34, iter:3243] Loss: 0.062 | Acc: 97.000% \n",
      "[epoch:34, iter:3244] Loss: 0.062 | Acc: 96.957% \n",
      "[epoch:34, iter:3245] Loss: 0.061 | Acc: 96.997% \n",
      "[epoch:34, iter:3246] Loss: 0.061 | Acc: 97.035% \n",
      "[epoch:34, iter:3247] Loss: 0.060 | Acc: 97.073% \n",
      "[epoch:34, iter:3248] Loss: 0.062 | Acc: 97.031% \n",
      "[epoch:34, iter:3249] Loss: 0.061 | Acc: 97.068% \n",
      "[epoch:34, iter:3250] Loss: 0.061 | Acc: 97.104% \n",
      "[epoch:34, iter:3251] Loss: 0.064 | Acc: 96.988% \n",
      "[epoch:34, iter:3252] Loss: 0.063 | Acc: 97.024% \n",
      "[epoch:34, iter:3253] Loss: 0.067 | Acc: 96.912% \n",
      "[epoch:34, iter:3254] Loss: 0.066 | Acc: 96.948% \n",
      "[epoch:34, iter:3255] Loss: 0.066 | Acc: 96.911% \n",
      "[epoch:34, iter:3256] Loss: 0.065 | Acc: 96.946% \n",
      "[epoch:34, iter:3257] Loss: 0.066 | Acc: 96.910% \n",
      "[epoch:34, iter:3258] Loss: 0.066 | Acc: 96.944% \n",
      "[epoch:34, iter:3259] Loss: 0.066 | Acc: 96.978% \n",
      "[epoch:34, iter:3260] Loss: 0.065 | Acc: 97.011% \n",
      "[epoch:34, iter:3261] Loss: 0.065 | Acc: 96.976% \n",
      "[epoch:34, iter:3262] Loss: 0.065 | Acc: 97.008% \n",
      "[epoch:34, iter:3263] Loss: 0.064 | Acc: 97.039% \n",
      "[epoch:34, iter:3264] Loss: 0.064 | Acc: 97.057% \n",
      "Waiting Test!\n",
      "测试分类准确率为：76.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 35\n",
      "[epoch:35, iter:3265] Loss: 0.048 | Acc: 100.000% \n",
      "[epoch:35, iter:3266] Loss: 0.034 | Acc: 100.000% \n",
      "[epoch:35, iter:3267] Loss: 0.057 | Acc: 97.917% \n",
      "[epoch:35, iter:3268] Loss: 0.045 | Acc: 98.438% \n",
      "[epoch:35, iter:3269] Loss: 0.038 | Acc: 98.750% \n",
      "[epoch:35, iter:3270] Loss: 0.041 | Acc: 97.917% \n",
      "[epoch:35, iter:3271] Loss: 0.039 | Acc: 98.214% \n",
      "[epoch:35, iter:3272] Loss: 0.036 | Acc: 98.438% \n",
      "[epoch:35, iter:3273] Loss: 0.069 | Acc: 97.917% \n",
      "[epoch:35, iter:3274] Loss: 0.063 | Acc: 98.125% \n",
      "[epoch:35, iter:3275] Loss: 0.057 | Acc: 98.295% \n",
      "[epoch:35, iter:3276] Loss: 0.071 | Acc: 97.917% \n",
      "[epoch:35, iter:3277] Loss: 0.066 | Acc: 98.077% \n",
      "[epoch:35, iter:3278] Loss: 0.069 | Acc: 97.768% \n",
      "[epoch:35, iter:3279] Loss: 0.067 | Acc: 97.917% \n",
      "[epoch:35, iter:3280] Loss: 0.063 | Acc: 98.047% \n",
      "[epoch:35, iter:3281] Loss: 0.060 | Acc: 98.162% \n",
      "[epoch:35, iter:3282] Loss: 0.057 | Acc: 98.264% \n",
      "[epoch:35, iter:3283] Loss: 0.056 | Acc: 98.355% \n",
      "[epoch:35, iter:3284] Loss: 0.054 | Acc: 98.438% \n",
      "[epoch:35, iter:3285] Loss: 0.052 | Acc: 98.512% \n",
      "[epoch:35, iter:3286] Loss: 0.051 | Acc: 98.580% \n",
      "[epoch:35, iter:3287] Loss: 0.053 | Acc: 98.641% \n",
      "[epoch:35, iter:3288] Loss: 0.050 | Acc: 98.698% \n",
      "[epoch:35, iter:3289] Loss: 0.052 | Acc: 98.500% \n",
      "[epoch:35, iter:3290] Loss: 0.050 | Acc: 98.558% \n",
      "[epoch:35, iter:3291] Loss: 0.049 | Acc: 98.611% \n",
      "[epoch:35, iter:3292] Loss: 0.054 | Acc: 98.214% \n",
      "[epoch:35, iter:3293] Loss: 0.053 | Acc: 98.276% \n",
      "[epoch:35, iter:3294] Loss: 0.051 | Acc: 98.333% \n",
      "[epoch:35, iter:3295] Loss: 0.050 | Acc: 98.387% \n",
      "[epoch:35, iter:3296] Loss: 0.049 | Acc: 98.438% \n",
      "[epoch:35, iter:3297] Loss: 0.048 | Acc: 98.485% \n",
      "[epoch:35, iter:3298] Loss: 0.051 | Acc: 98.529% \n",
      "[epoch:35, iter:3299] Loss: 0.055 | Acc: 98.214% \n",
      "[epoch:35, iter:3300] Loss: 0.055 | Acc: 98.090% \n",
      "[epoch:35, iter:3301] Loss: 0.054 | Acc: 98.142% \n",
      "[epoch:35, iter:3302] Loss: 0.059 | Acc: 98.026% \n",
      "[epoch:35, iter:3303] Loss: 0.058 | Acc: 98.077% \n",
      "[epoch:35, iter:3304] Loss: 0.056 | Acc: 98.125% \n",
      "[epoch:35, iter:3305] Loss: 0.061 | Acc: 98.018% \n",
      "[epoch:35, iter:3306] Loss: 0.060 | Acc: 98.065% \n",
      "[epoch:35, iter:3307] Loss: 0.059 | Acc: 98.110% \n",
      "[epoch:35, iter:3308] Loss: 0.062 | Acc: 98.011% \n",
      "[epoch:35, iter:3309] Loss: 0.062 | Acc: 98.056% \n",
      "[epoch:35, iter:3310] Loss: 0.061 | Acc: 98.098% \n",
      "[epoch:35, iter:3311] Loss: 0.060 | Acc: 98.138% \n",
      "[epoch:35, iter:3312] Loss: 0.060 | Acc: 98.047% \n",
      "[epoch:35, iter:3313] Loss: 0.060 | Acc: 98.087% \n",
      "[epoch:35, iter:3314] Loss: 0.059 | Acc: 98.125% \n",
      "[epoch:35, iter:3315] Loss: 0.058 | Acc: 98.162% \n",
      "[epoch:35, iter:3316] Loss: 0.061 | Acc: 97.957% \n",
      "[epoch:35, iter:3317] Loss: 0.061 | Acc: 97.877% \n",
      "[epoch:35, iter:3318] Loss: 0.061 | Acc: 97.801% \n",
      "[epoch:35, iter:3319] Loss: 0.060 | Acc: 97.841% \n",
      "[epoch:35, iter:3320] Loss: 0.059 | Acc: 97.879% \n",
      "[epoch:35, iter:3321] Loss: 0.058 | Acc: 97.917% \n",
      "[epoch:35, iter:3322] Loss: 0.057 | Acc: 97.953% \n",
      "[epoch:35, iter:3323] Loss: 0.056 | Acc: 97.987% \n",
      "[epoch:35, iter:3324] Loss: 0.055 | Acc: 98.021% \n",
      "[epoch:35, iter:3325] Loss: 0.055 | Acc: 98.053% \n",
      "[epoch:35, iter:3326] Loss: 0.054 | Acc: 98.085% \n",
      "[epoch:35, iter:3327] Loss: 0.054 | Acc: 98.115% \n",
      "[epoch:35, iter:3328] Loss: 0.056 | Acc: 98.047% \n",
      "[epoch:35, iter:3329] Loss: 0.056 | Acc: 98.077% \n",
      "[epoch:35, iter:3330] Loss: 0.063 | Acc: 97.538% \n",
      "[epoch:35, iter:3331] Loss: 0.065 | Acc: 97.388% \n",
      "[epoch:35, iter:3332] Loss: 0.064 | Acc: 97.426% \n",
      "[epoch:35, iter:3333] Loss: 0.064 | Acc: 97.373% \n",
      "[epoch:35, iter:3334] Loss: 0.064 | Acc: 97.411% \n",
      "[epoch:35, iter:3335] Loss: 0.063 | Acc: 97.447% \n",
      "[epoch:35, iter:3336] Loss: 0.062 | Acc: 97.483% \n",
      "[epoch:35, iter:3337] Loss: 0.061 | Acc: 97.517% \n",
      "[epoch:35, iter:3338] Loss: 0.061 | Acc: 97.551% \n",
      "[epoch:35, iter:3339] Loss: 0.061 | Acc: 97.500% \n",
      "[epoch:35, iter:3340] Loss: 0.060 | Acc: 97.533% \n",
      "[epoch:35, iter:3341] Loss: 0.060 | Acc: 97.484% \n",
      "[epoch:35, iter:3342] Loss: 0.061 | Acc: 97.436% \n",
      "[epoch:35, iter:3343] Loss: 0.060 | Acc: 97.468% \n",
      "[epoch:35, iter:3344] Loss: 0.060 | Acc: 97.500% \n",
      "[epoch:35, iter:3345] Loss: 0.059 | Acc: 97.531% \n",
      "[epoch:35, iter:3346] Loss: 0.060 | Acc: 97.485% \n",
      "[epoch:35, iter:3347] Loss: 0.062 | Acc: 97.440% \n",
      "[epoch:35, iter:3348] Loss: 0.061 | Acc: 97.470% \n",
      "[epoch:35, iter:3349] Loss: 0.061 | Acc: 97.500% \n",
      "[epoch:35, iter:3350] Loss: 0.060 | Acc: 97.529% \n",
      "[epoch:35, iter:3351] Loss: 0.060 | Acc: 97.557% \n",
      "[epoch:35, iter:3352] Loss: 0.059 | Acc: 97.585% \n",
      "[epoch:35, iter:3353] Loss: 0.059 | Acc: 97.612% \n",
      "[epoch:35, iter:3354] Loss: 0.060 | Acc: 97.569% \n",
      "[epoch:35, iter:3355] Loss: 0.064 | Acc: 97.390% \n",
      "[epoch:35, iter:3356] Loss: 0.064 | Acc: 97.351% \n",
      "[epoch:35, iter:3357] Loss: 0.065 | Acc: 97.245% \n",
      "[epoch:35, iter:3358] Loss: 0.066 | Acc: 97.207% \n",
      "[epoch:35, iter:3359] Loss: 0.066 | Acc: 97.237% \n",
      "[epoch:35, iter:3360] Loss: 0.065 | Acc: 97.253% \n",
      "Waiting Test!\n",
      "测试分类准确率为：81.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 36\n",
      "[epoch:36, iter:3361] Loss: 0.015 | Acc: 100.000% \n",
      "[epoch:36, iter:3362] Loss: 0.138 | Acc: 93.750% \n",
      "[epoch:36, iter:3363] Loss: 0.094 | Acc: 95.833% \n",
      "[epoch:36, iter:3364] Loss: 0.084 | Acc: 96.875% \n",
      "[epoch:36, iter:3365] Loss: 0.071 | Acc: 97.500% \n",
      "[epoch:36, iter:3366] Loss: 0.063 | Acc: 97.917% \n",
      "[epoch:36, iter:3367] Loss: 0.056 | Acc: 98.214% \n",
      "[epoch:36, iter:3368] Loss: 0.050 | Acc: 98.438% \n",
      "[epoch:36, iter:3369] Loss: 0.045 | Acc: 98.611% \n",
      "[epoch:36, iter:3370] Loss: 0.041 | Acc: 98.750% \n",
      "[epoch:36, iter:3371] Loss: 0.053 | Acc: 98.295% \n",
      "[epoch:36, iter:3372] Loss: 0.049 | Acc: 98.438% \n",
      "[epoch:36, iter:3373] Loss: 0.046 | Acc: 98.558% \n",
      "[epoch:36, iter:3374] Loss: 0.047 | Acc: 98.661% \n",
      "[epoch:36, iter:3375] Loss: 0.045 | Acc: 98.750% \n",
      "[epoch:36, iter:3376] Loss: 0.044 | Acc: 98.828% \n",
      "[epoch:36, iter:3377] Loss: 0.043 | Acc: 98.897% \n",
      "[epoch:36, iter:3378] Loss: 0.048 | Acc: 98.264% \n",
      "[epoch:36, iter:3379] Loss: 0.046 | Acc: 98.355% \n",
      "[epoch:36, iter:3380] Loss: 0.044 | Acc: 98.438% \n",
      "[epoch:36, iter:3381] Loss: 0.060 | Acc: 97.619% \n",
      "[epoch:36, iter:3382] Loss: 0.086 | Acc: 97.443% \n",
      "[epoch:36, iter:3383] Loss: 0.083 | Acc: 97.554% \n",
      "[epoch:36, iter:3384] Loss: 0.081 | Acc: 97.656% \n",
      "[epoch:36, iter:3385] Loss: 0.078 | Acc: 97.750% \n",
      "[epoch:36, iter:3386] Loss: 0.078 | Acc: 97.837% \n",
      "[epoch:36, iter:3387] Loss: 0.081 | Acc: 97.685% \n",
      "[epoch:36, iter:3388] Loss: 0.082 | Acc: 97.545% \n",
      "[epoch:36, iter:3389] Loss: 0.081 | Acc: 97.629% \n",
      "[epoch:36, iter:3390] Loss: 0.084 | Acc: 97.292% \n",
      "[epoch:36, iter:3391] Loss: 0.082 | Acc: 97.177% \n",
      "[epoch:36, iter:3392] Loss: 0.080 | Acc: 97.266% \n",
      "[epoch:36, iter:3393] Loss: 0.079 | Acc: 97.159% \n",
      "[epoch:36, iter:3394] Loss: 0.077 | Acc: 97.243% \n",
      "[epoch:36, iter:3395] Loss: 0.076 | Acc: 97.321% \n",
      "[epoch:36, iter:3396] Loss: 0.074 | Acc: 97.396% \n",
      "[epoch:36, iter:3397] Loss: 0.074 | Acc: 97.297% \n",
      "[epoch:36, iter:3398] Loss: 0.072 | Acc: 97.368% \n",
      "[epoch:36, iter:3399] Loss: 0.071 | Acc: 97.436% \n",
      "[epoch:36, iter:3400] Loss: 0.069 | Acc: 97.500% \n",
      "[epoch:36, iter:3401] Loss: 0.068 | Acc: 97.561% \n",
      "[epoch:36, iter:3402] Loss: 0.068 | Acc: 97.470% \n",
      "[epoch:36, iter:3403] Loss: 0.067 | Acc: 97.529% \n",
      "[epoch:36, iter:3404] Loss: 0.068 | Acc: 97.443% \n",
      "[epoch:36, iter:3405] Loss: 0.067 | Acc: 97.500% \n",
      "[epoch:36, iter:3406] Loss: 0.069 | Acc: 97.554% \n",
      "[epoch:36, iter:3407] Loss: 0.069 | Acc: 97.473% \n",
      "[epoch:36, iter:3408] Loss: 0.068 | Acc: 97.396% \n",
      "[epoch:36, iter:3409] Loss: 0.068 | Acc: 97.449% \n",
      "[epoch:36, iter:3410] Loss: 0.066 | Acc: 97.500% \n",
      "[epoch:36, iter:3411] Loss: 0.066 | Acc: 97.549% \n",
      "[epoch:36, iter:3412] Loss: 0.066 | Acc: 97.476% \n",
      "[epoch:36, iter:3413] Loss: 0.064 | Acc: 97.524% \n",
      "[epoch:36, iter:3414] Loss: 0.065 | Acc: 97.454% \n",
      "[epoch:36, iter:3415] Loss: 0.064 | Acc: 97.500% \n",
      "[epoch:36, iter:3416] Loss: 0.062 | Acc: 97.545% \n",
      "[epoch:36, iter:3417] Loss: 0.061 | Acc: 97.588% \n",
      "[epoch:36, iter:3418] Loss: 0.064 | Acc: 97.414% \n",
      "[epoch:36, iter:3419] Loss: 0.067 | Acc: 97.352% \n",
      "[epoch:36, iter:3420] Loss: 0.066 | Acc: 97.396% \n",
      "[epoch:36, iter:3421] Loss: 0.065 | Acc: 97.439% \n",
      "[epoch:36, iter:3422] Loss: 0.065 | Acc: 97.480% \n",
      "[epoch:36, iter:3423] Loss: 0.064 | Acc: 97.520% \n",
      "[epoch:36, iter:3424] Loss: 0.067 | Acc: 97.363% \n",
      "[epoch:36, iter:3425] Loss: 0.067 | Acc: 97.212% \n",
      "[epoch:36, iter:3426] Loss: 0.067 | Acc: 97.254% \n",
      "[epoch:36, iter:3427] Loss: 0.066 | Acc: 97.295% \n",
      "[epoch:36, iter:3428] Loss: 0.065 | Acc: 97.335% \n",
      "[epoch:36, iter:3429] Loss: 0.068 | Acc: 97.192% \n",
      "[epoch:36, iter:3430] Loss: 0.071 | Acc: 97.054% \n",
      "[epoch:36, iter:3431] Loss: 0.071 | Acc: 97.007% \n",
      "[epoch:36, iter:3432] Loss: 0.070 | Acc: 97.049% \n",
      "[epoch:36, iter:3433] Loss: 0.070 | Acc: 97.003% \n",
      "[epoch:36, iter:3434] Loss: 0.069 | Acc: 97.044% \n",
      "[epoch:36, iter:3435] Loss: 0.068 | Acc: 97.083% \n",
      "[epoch:36, iter:3436] Loss: 0.077 | Acc: 96.711% \n",
      "[epoch:36, iter:3437] Loss: 0.078 | Acc: 96.591% \n",
      "[epoch:36, iter:3438] Loss: 0.079 | Acc: 96.554% \n",
      "[epoch:36, iter:3439] Loss: 0.078 | Acc: 96.598% \n",
      "[epoch:36, iter:3440] Loss: 0.079 | Acc: 96.562% \n",
      "[epoch:36, iter:3441] Loss: 0.079 | Acc: 96.528% \n",
      "[epoch:36, iter:3442] Loss: 0.080 | Acc: 96.418% \n",
      "[epoch:36, iter:3443] Loss: 0.080 | Acc: 96.461% \n",
      "[epoch:36, iter:3444] Loss: 0.079 | Acc: 96.503% \n",
      "[epoch:36, iter:3445] Loss: 0.080 | Acc: 96.544% \n",
      "[epoch:36, iter:3446] Loss: 0.079 | Acc: 96.584% \n",
      "[epoch:36, iter:3447] Loss: 0.083 | Acc: 96.408% \n",
      "[epoch:36, iter:3448] Loss: 0.083 | Acc: 96.378% \n",
      "[epoch:36, iter:3449] Loss: 0.083 | Acc: 96.419% \n",
      "[epoch:36, iter:3450] Loss: 0.082 | Acc: 96.458% \n",
      "[epoch:36, iter:3451] Loss: 0.082 | Acc: 96.497% \n",
      "[epoch:36, iter:3452] Loss: 0.081 | Acc: 96.535% \n",
      "[epoch:36, iter:3453] Loss: 0.081 | Acc: 96.505% \n",
      "[epoch:36, iter:3454] Loss: 0.080 | Acc: 96.543% \n",
      "[epoch:36, iter:3455] Loss: 0.079 | Acc: 96.579% \n",
      "[epoch:36, iter:3456] Loss: 0.087 | Acc: 96.403% \n",
      "Waiting Test!\n",
      "测试分类准确率为：77.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 37\n",
      "[epoch:37, iter:3457] Loss: 0.280 | Acc: 93.750% \n",
      "[epoch:37, iter:3458] Loss: 0.151 | Acc: 96.875% \n",
      "[epoch:37, iter:3459] Loss: 0.111 | Acc: 97.917% \n",
      "[epoch:37, iter:3460] Loss: 0.087 | Acc: 98.438% \n",
      "[epoch:37, iter:3461] Loss: 0.107 | Acc: 97.500% \n",
      "[epoch:37, iter:3462] Loss: 0.119 | Acc: 95.833% \n",
      "[epoch:37, iter:3463] Loss: 0.109 | Acc: 96.429% \n",
      "[epoch:37, iter:3464] Loss: 0.098 | Acc: 96.875% \n",
      "[epoch:37, iter:3465] Loss: 0.111 | Acc: 95.833% \n",
      "[epoch:37, iter:3466] Loss: 0.101 | Acc: 96.250% \n",
      "[epoch:37, iter:3467] Loss: 0.094 | Acc: 96.591% \n",
      "[epoch:37, iter:3468] Loss: 0.107 | Acc: 96.354% \n",
      "[epoch:37, iter:3469] Loss: 0.103 | Acc: 96.635% \n",
      "[epoch:37, iter:3470] Loss: 0.115 | Acc: 95.982% \n",
      "[epoch:37, iter:3471] Loss: 0.110 | Acc: 96.250% \n",
      "[epoch:37, iter:3472] Loss: 0.107 | Acc: 96.484% \n",
      "[epoch:37, iter:3473] Loss: 0.101 | Acc: 96.691% \n",
      "[epoch:37, iter:3474] Loss: 0.097 | Acc: 96.875% \n",
      "[epoch:37, iter:3475] Loss: 0.093 | Acc: 97.039% \n",
      "[epoch:37, iter:3476] Loss: 0.088 | Acc: 97.188% \n",
      "[epoch:37, iter:3477] Loss: 0.085 | Acc: 97.321% \n",
      "[epoch:37, iter:3478] Loss: 0.082 | Acc: 97.443% \n",
      "[epoch:37, iter:3479] Loss: 0.079 | Acc: 97.554% \n",
      "[epoch:37, iter:3480] Loss: 0.076 | Acc: 97.656% \n",
      "[epoch:37, iter:3481] Loss: 0.074 | Acc: 97.750% \n",
      "[epoch:37, iter:3482] Loss: 0.071 | Acc: 97.837% \n",
      "[epoch:37, iter:3483] Loss: 0.069 | Acc: 97.917% \n",
      "[epoch:37, iter:3484] Loss: 0.067 | Acc: 97.991% \n",
      "[epoch:37, iter:3485] Loss: 0.066 | Acc: 98.060% \n",
      "[epoch:37, iter:3486] Loss: 0.066 | Acc: 98.125% \n",
      "[epoch:37, iter:3487] Loss: 0.065 | Acc: 98.185% \n",
      "[epoch:37, iter:3488] Loss: 0.066 | Acc: 98.047% \n",
      "[epoch:37, iter:3489] Loss: 0.064 | Acc: 98.106% \n",
      "[epoch:37, iter:3490] Loss: 0.066 | Acc: 97.794% \n",
      "[epoch:37, iter:3491] Loss: 0.067 | Acc: 97.857% \n",
      "[epoch:37, iter:3492] Loss: 0.065 | Acc: 97.917% \n",
      "[epoch:37, iter:3493] Loss: 0.072 | Acc: 97.804% \n",
      "[epoch:37, iter:3494] Loss: 0.071 | Acc: 97.862% \n",
      "[epoch:37, iter:3495] Loss: 0.070 | Acc: 97.917% \n",
      "[epoch:37, iter:3496] Loss: 0.069 | Acc: 97.969% \n",
      "[epoch:37, iter:3497] Loss: 0.068 | Acc: 98.018% \n",
      "[epoch:37, iter:3498] Loss: 0.066 | Acc: 98.065% \n",
      "[epoch:37, iter:3499] Loss: 0.070 | Acc: 97.965% \n",
      "[epoch:37, iter:3500] Loss: 0.068 | Acc: 98.011% \n",
      "[epoch:37, iter:3501] Loss: 0.067 | Acc: 98.056% \n",
      "[epoch:37, iter:3502] Loss: 0.066 | Acc: 98.098% \n",
      "[epoch:37, iter:3503] Loss: 0.065 | Acc: 98.138% \n",
      "[epoch:37, iter:3504] Loss: 0.064 | Acc: 98.177% \n",
      "[epoch:37, iter:3505] Loss: 0.063 | Acc: 98.214% \n",
      "[epoch:37, iter:3506] Loss: 0.064 | Acc: 98.125% \n",
      "[epoch:37, iter:3507] Loss: 0.069 | Acc: 97.794% \n",
      "[epoch:37, iter:3508] Loss: 0.069 | Acc: 97.837% \n",
      "[epoch:37, iter:3509] Loss: 0.068 | Acc: 97.877% \n",
      "[epoch:37, iter:3510] Loss: 0.068 | Acc: 97.917% \n",
      "[epoch:37, iter:3511] Loss: 0.067 | Acc: 97.955% \n",
      "[epoch:37, iter:3512] Loss: 0.065 | Acc: 97.991% \n",
      "[epoch:37, iter:3513] Loss: 0.065 | Acc: 97.917% \n",
      "[epoch:37, iter:3514] Loss: 0.065 | Acc: 97.953% \n",
      "[epoch:37, iter:3515] Loss: 0.063 | Acc: 97.987% \n",
      "[epoch:37, iter:3516] Loss: 0.063 | Acc: 98.021% \n",
      "[epoch:37, iter:3517] Loss: 0.062 | Acc: 98.053% \n",
      "[epoch:37, iter:3518] Loss: 0.062 | Acc: 98.085% \n",
      "[epoch:37, iter:3519] Loss: 0.063 | Acc: 97.917% \n",
      "[epoch:37, iter:3520] Loss: 0.062 | Acc: 97.949% \n",
      "[epoch:37, iter:3521] Loss: 0.061 | Acc: 97.981% \n",
      "[epoch:37, iter:3522] Loss: 0.060 | Acc: 98.011% \n",
      "[epoch:37, iter:3523] Loss: 0.059 | Acc: 98.041% \n",
      "[epoch:37, iter:3524] Loss: 0.059 | Acc: 98.070% \n",
      "[epoch:37, iter:3525] Loss: 0.058 | Acc: 98.098% \n",
      "[epoch:37, iter:3526] Loss: 0.057 | Acc: 98.125% \n",
      "[epoch:37, iter:3527] Loss: 0.057 | Acc: 98.151% \n",
      "[epoch:37, iter:3528] Loss: 0.056 | Acc: 98.177% \n",
      "[epoch:37, iter:3529] Loss: 0.057 | Acc: 98.116% \n",
      "[epoch:37, iter:3530] Loss: 0.057 | Acc: 98.142% \n",
      "[epoch:37, iter:3531] Loss: 0.056 | Acc: 98.167% \n",
      "[epoch:37, iter:3532] Loss: 0.055 | Acc: 98.191% \n",
      "[epoch:37, iter:3533] Loss: 0.055 | Acc: 98.214% \n",
      "[epoch:37, iter:3534] Loss: 0.054 | Acc: 98.237% \n",
      "[epoch:37, iter:3535] Loss: 0.055 | Acc: 98.180% \n",
      "[epoch:37, iter:3536] Loss: 0.057 | Acc: 98.125% \n",
      "[epoch:37, iter:3537] Loss: 0.056 | Acc: 98.148% \n",
      "[epoch:37, iter:3538] Loss: 0.055 | Acc: 98.171% \n",
      "[epoch:37, iter:3539] Loss: 0.056 | Acc: 98.117% \n",
      "[epoch:37, iter:3540] Loss: 0.056 | Acc: 98.065% \n",
      "[epoch:37, iter:3541] Loss: 0.056 | Acc: 98.088% \n",
      "[epoch:37, iter:3542] Loss: 0.055 | Acc: 98.110% \n",
      "[epoch:37, iter:3543] Loss: 0.055 | Acc: 98.132% \n",
      "[epoch:37, iter:3544] Loss: 0.054 | Acc: 98.153% \n",
      "[epoch:37, iter:3545] Loss: 0.054 | Acc: 98.174% \n",
      "[epoch:37, iter:3546] Loss: 0.055 | Acc: 98.125% \n",
      "[epoch:37, iter:3547] Loss: 0.055 | Acc: 98.077% \n",
      "[epoch:37, iter:3548] Loss: 0.056 | Acc: 98.030% \n",
      "[epoch:37, iter:3549] Loss: 0.055 | Acc: 98.051% \n",
      "[epoch:37, iter:3550] Loss: 0.055 | Acc: 98.005% \n",
      "[epoch:37, iter:3551] Loss: 0.056 | Acc: 97.961% \n",
      "[epoch:37, iter:3552] Loss: 0.056 | Acc: 97.907% \n",
      "Waiting Test!\n",
      "测试分类准确率为：72.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 38\n",
      "[epoch:38, iter:3553] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:38, iter:3554] Loss: 0.003 | Acc: 100.000% \n",
      "[epoch:38, iter:3555] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:38, iter:3556] Loss: 0.054 | Acc: 96.875% \n",
      "[epoch:38, iter:3557] Loss: 0.043 | Acc: 97.500% \n",
      "[epoch:38, iter:3558] Loss: 0.039 | Acc: 97.917% \n",
      "[epoch:38, iter:3559] Loss: 0.045 | Acc: 97.321% \n",
      "[epoch:38, iter:3560] Loss: 0.049 | Acc: 96.875% \n",
      "[epoch:38, iter:3561] Loss: 0.044 | Acc: 97.222% \n",
      "[epoch:38, iter:3562] Loss: 0.040 | Acc: 97.500% \n",
      "[epoch:38, iter:3563] Loss: 0.036 | Acc: 97.727% \n",
      "[epoch:38, iter:3564] Loss: 0.033 | Acc: 97.917% \n",
      "[epoch:38, iter:3565] Loss: 0.033 | Acc: 98.077% \n",
      "[epoch:38, iter:3566] Loss: 0.031 | Acc: 98.214% \n",
      "[epoch:38, iter:3567] Loss: 0.029 | Acc: 98.333% \n",
      "[epoch:38, iter:3568] Loss: 0.027 | Acc: 98.438% \n",
      "[epoch:38, iter:3569] Loss: 0.034 | Acc: 98.162% \n",
      "[epoch:38, iter:3570] Loss: 0.032 | Acc: 98.264% \n",
      "[epoch:38, iter:3571] Loss: 0.031 | Acc: 98.355% \n",
      "[epoch:38, iter:3572] Loss: 0.030 | Acc: 98.438% \n",
      "[epoch:38, iter:3573] Loss: 0.029 | Acc: 98.512% \n",
      "[epoch:38, iter:3574] Loss: 0.028 | Acc: 98.580% \n",
      "[epoch:38, iter:3575] Loss: 0.027 | Acc: 98.641% \n",
      "[epoch:38, iter:3576] Loss: 0.027 | Acc: 98.698% \n",
      "[epoch:38, iter:3577] Loss: 0.026 | Acc: 98.750% \n",
      "[epoch:38, iter:3578] Loss: 0.026 | Acc: 98.798% \n",
      "[epoch:38, iter:3579] Loss: 0.029 | Acc: 98.611% \n",
      "[epoch:38, iter:3580] Loss: 0.028 | Acc: 98.661% \n",
      "[epoch:38, iter:3581] Loss: 0.027 | Acc: 98.707% \n",
      "[epoch:38, iter:3582] Loss: 0.027 | Acc: 98.750% \n",
      "[epoch:38, iter:3583] Loss: 0.026 | Acc: 98.790% \n",
      "[epoch:38, iter:3584] Loss: 0.029 | Acc: 98.633% \n",
      "[epoch:38, iter:3585] Loss: 0.032 | Acc: 98.485% \n",
      "[epoch:38, iter:3586] Loss: 0.031 | Acc: 98.529% \n",
      "[epoch:38, iter:3587] Loss: 0.033 | Acc: 98.571% \n",
      "[epoch:38, iter:3588] Loss: 0.034 | Acc: 98.438% \n",
      "[epoch:38, iter:3589] Loss: 0.034 | Acc: 98.480% \n",
      "[epoch:38, iter:3590] Loss: 0.033 | Acc: 98.520% \n",
      "[epoch:38, iter:3591] Loss: 0.036 | Acc: 98.077% \n",
      "[epoch:38, iter:3592] Loss: 0.036 | Acc: 98.125% \n",
      "[epoch:38, iter:3593] Loss: 0.036 | Acc: 98.018% \n",
      "[epoch:38, iter:3594] Loss: 0.036 | Acc: 98.065% \n",
      "[epoch:38, iter:3595] Loss: 0.036 | Acc: 98.110% \n",
      "[epoch:38, iter:3596] Loss: 0.035 | Acc: 98.153% \n",
      "[epoch:38, iter:3597] Loss: 0.034 | Acc: 98.194% \n",
      "[epoch:38, iter:3598] Loss: 0.034 | Acc: 98.234% \n",
      "[epoch:38, iter:3599] Loss: 0.033 | Acc: 98.271% \n",
      "[epoch:38, iter:3600] Loss: 0.033 | Acc: 98.307% \n",
      "[epoch:38, iter:3601] Loss: 0.033 | Acc: 98.342% \n",
      "[epoch:38, iter:3602] Loss: 0.032 | Acc: 98.375% \n",
      "[epoch:38, iter:3603] Loss: 0.033 | Acc: 98.407% \n",
      "[epoch:38, iter:3604] Loss: 0.032 | Acc: 98.438% \n",
      "[epoch:38, iter:3605] Loss: 0.032 | Acc: 98.467% \n",
      "[epoch:38, iter:3606] Loss: 0.031 | Acc: 98.495% \n",
      "[epoch:38, iter:3607] Loss: 0.033 | Acc: 98.409% \n",
      "[epoch:38, iter:3608] Loss: 0.033 | Acc: 98.438% \n",
      "[epoch:38, iter:3609] Loss: 0.033 | Acc: 98.465% \n",
      "[epoch:38, iter:3610] Loss: 0.032 | Acc: 98.491% \n",
      "[epoch:38, iter:3611] Loss: 0.032 | Acc: 98.517% \n",
      "[epoch:38, iter:3612] Loss: 0.031 | Acc: 98.542% \n",
      "[epoch:38, iter:3613] Loss: 0.032 | Acc: 98.463% \n",
      "[epoch:38, iter:3614] Loss: 0.033 | Acc: 98.387% \n",
      "[epoch:38, iter:3615] Loss: 0.033 | Acc: 98.413% \n",
      "[epoch:38, iter:3616] Loss: 0.034 | Acc: 98.340% \n",
      "[epoch:38, iter:3617] Loss: 0.034 | Acc: 98.365% \n",
      "[epoch:38, iter:3618] Loss: 0.033 | Acc: 98.390% \n",
      "[epoch:38, iter:3619] Loss: 0.033 | Acc: 98.414% \n",
      "[epoch:38, iter:3620] Loss: 0.033 | Acc: 98.438% \n",
      "[epoch:38, iter:3621] Loss: 0.034 | Acc: 98.370% \n",
      "[epoch:38, iter:3622] Loss: 0.036 | Acc: 98.304% \n",
      "[epoch:38, iter:3623] Loss: 0.036 | Acc: 98.327% \n",
      "[epoch:38, iter:3624] Loss: 0.036 | Acc: 98.264% \n",
      "[epoch:38, iter:3625] Loss: 0.036 | Acc: 98.288% \n",
      "[epoch:38, iter:3626] Loss: 0.036 | Acc: 98.311% \n",
      "[epoch:38, iter:3627] Loss: 0.036 | Acc: 98.333% \n",
      "[epoch:38, iter:3628] Loss: 0.035 | Acc: 98.355% \n",
      "[epoch:38, iter:3629] Loss: 0.035 | Acc: 98.377% \n",
      "[epoch:38, iter:3630] Loss: 0.035 | Acc: 98.397% \n",
      "[epoch:38, iter:3631] Loss: 0.035 | Acc: 98.339% \n",
      "[epoch:38, iter:3632] Loss: 0.036 | Acc: 98.281% \n",
      "[epoch:38, iter:3633] Loss: 0.038 | Acc: 98.071% \n",
      "[epoch:38, iter:3634] Loss: 0.038 | Acc: 98.018% \n",
      "[epoch:38, iter:3635] Loss: 0.038 | Acc: 98.042% \n",
      "[epoch:38, iter:3636] Loss: 0.039 | Acc: 97.991% \n",
      "[epoch:38, iter:3637] Loss: 0.039 | Acc: 97.941% \n",
      "[epoch:38, iter:3638] Loss: 0.039 | Acc: 97.965% \n",
      "[epoch:38, iter:3639] Loss: 0.040 | Acc: 97.917% \n",
      "[epoch:38, iter:3640] Loss: 0.041 | Acc: 97.798% \n",
      "[epoch:38, iter:3641] Loss: 0.041 | Acc: 97.823% \n",
      "[epoch:38, iter:3642] Loss: 0.041 | Acc: 97.778% \n",
      "[epoch:38, iter:3643] Loss: 0.041 | Acc: 97.802% \n",
      "[epoch:38, iter:3644] Loss: 0.040 | Acc: 97.826% \n",
      "[epoch:38, iter:3645] Loss: 0.040 | Acc: 97.849% \n",
      "[epoch:38, iter:3646] Loss: 0.040 | Acc: 97.872% \n",
      "[epoch:38, iter:3647] Loss: 0.040 | Acc: 97.895% \n",
      "[epoch:38, iter:3648] Loss: 0.039 | Acc: 97.907% \n",
      "Waiting Test!\n",
      "测试分类准确率为：75.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 39\n",
      "[epoch:39, iter:3649] Loss: 0.054 | Acc: 100.000% \n",
      "[epoch:39, iter:3650] Loss: 0.097 | Acc: 96.875% \n",
      "[epoch:39, iter:3651] Loss: 0.069 | Acc: 97.917% \n",
      "[epoch:39, iter:3652] Loss: 0.052 | Acc: 98.438% \n",
      "[epoch:39, iter:3653] Loss: 0.042 | Acc: 98.750% \n",
      "[epoch:39, iter:3654] Loss: 0.035 | Acc: 98.958% \n",
      "[epoch:39, iter:3655] Loss: 0.035 | Acc: 99.107% \n",
      "[epoch:39, iter:3656] Loss: 0.032 | Acc: 99.219% \n",
      "[epoch:39, iter:3657] Loss: 0.029 | Acc: 99.306% \n",
      "[epoch:39, iter:3658] Loss: 0.031 | Acc: 99.375% \n",
      "[epoch:39, iter:3659] Loss: 0.028 | Acc: 99.432% \n",
      "[epoch:39, iter:3660] Loss: 0.026 | Acc: 99.479% \n",
      "[epoch:39, iter:3661] Loss: 0.024 | Acc: 99.519% \n",
      "[epoch:39, iter:3662] Loss: 0.049 | Acc: 98.661% \n",
      "[epoch:39, iter:3663] Loss: 0.048 | Acc: 98.750% \n",
      "[epoch:39, iter:3664] Loss: 0.046 | Acc: 98.828% \n",
      "[epoch:39, iter:3665] Loss: 0.057 | Acc: 98.162% \n",
      "[epoch:39, iter:3666] Loss: 0.056 | Acc: 98.264% \n",
      "[epoch:39, iter:3667] Loss: 0.061 | Acc: 97.697% \n",
      "[epoch:39, iter:3668] Loss: 0.058 | Acc: 97.812% \n",
      "[epoch:39, iter:3669] Loss: 0.056 | Acc: 97.917% \n",
      "[epoch:39, iter:3670] Loss: 0.057 | Acc: 97.727% \n",
      "[epoch:39, iter:3671] Loss: 0.054 | Acc: 97.826% \n",
      "[epoch:39, iter:3672] Loss: 0.052 | Acc: 97.917% \n",
      "[epoch:39, iter:3673] Loss: 0.068 | Acc: 97.250% \n",
      "[epoch:39, iter:3674] Loss: 0.066 | Acc: 97.356% \n",
      "[epoch:39, iter:3675] Loss: 0.065 | Acc: 97.222% \n",
      "[epoch:39, iter:3676] Loss: 0.066 | Acc: 97.098% \n",
      "[epoch:39, iter:3677] Loss: 0.064 | Acc: 97.198% \n",
      "[epoch:39, iter:3678] Loss: 0.063 | Acc: 97.083% \n",
      "[epoch:39, iter:3679] Loss: 0.061 | Acc: 97.177% \n",
      "[epoch:39, iter:3680] Loss: 0.060 | Acc: 97.266% \n",
      "[epoch:39, iter:3681] Loss: 0.064 | Acc: 97.159% \n",
      "[epoch:39, iter:3682] Loss: 0.062 | Acc: 97.243% \n",
      "[epoch:39, iter:3683] Loss: 0.061 | Acc: 97.143% \n",
      "[epoch:39, iter:3684] Loss: 0.060 | Acc: 97.222% \n",
      "[epoch:39, iter:3685] Loss: 0.060 | Acc: 97.128% \n",
      "[epoch:39, iter:3686] Loss: 0.059 | Acc: 97.204% \n",
      "[epoch:39, iter:3687] Loss: 0.057 | Acc: 97.276% \n",
      "[epoch:39, iter:3688] Loss: 0.056 | Acc: 97.344% \n",
      "[epoch:39, iter:3689] Loss: 0.056 | Acc: 97.256% \n",
      "[epoch:39, iter:3690] Loss: 0.067 | Acc: 96.726% \n",
      "[epoch:39, iter:3691] Loss: 0.066 | Acc: 96.802% \n",
      "[epoch:39, iter:3692] Loss: 0.066 | Acc: 96.875% \n",
      "[epoch:39, iter:3693] Loss: 0.064 | Acc: 96.944% \n",
      "[epoch:39, iter:3694] Loss: 0.065 | Acc: 96.875% \n",
      "[epoch:39, iter:3695] Loss: 0.064 | Acc: 96.941% \n",
      "[epoch:39, iter:3696] Loss: 0.065 | Acc: 96.875% \n",
      "[epoch:39, iter:3697] Loss: 0.064 | Acc: 96.939% \n",
      "[epoch:39, iter:3698] Loss: 0.063 | Acc: 97.000% \n",
      "[epoch:39, iter:3699] Loss: 0.062 | Acc: 97.059% \n",
      "[epoch:39, iter:3700] Loss: 0.061 | Acc: 97.115% \n",
      "[epoch:39, iter:3701] Loss: 0.063 | Acc: 97.052% \n",
      "[epoch:39, iter:3702] Loss: 0.062 | Acc: 97.106% \n",
      "[epoch:39, iter:3703] Loss: 0.061 | Acc: 97.159% \n",
      "[epoch:39, iter:3704] Loss: 0.060 | Acc: 97.210% \n",
      "[epoch:39, iter:3705] Loss: 0.060 | Acc: 97.149% \n",
      "[epoch:39, iter:3706] Loss: 0.061 | Acc: 97.091% \n",
      "[epoch:39, iter:3707] Loss: 0.063 | Acc: 96.928% \n",
      "[epoch:39, iter:3708] Loss: 0.063 | Acc: 96.979% \n",
      "[epoch:39, iter:3709] Loss: 0.062 | Acc: 97.029% \n",
      "[epoch:39, iter:3710] Loss: 0.061 | Acc: 97.077% \n",
      "[epoch:39, iter:3711] Loss: 0.060 | Acc: 97.123% \n",
      "[epoch:39, iter:3712] Loss: 0.060 | Acc: 97.070% \n",
      "[epoch:39, iter:3713] Loss: 0.061 | Acc: 97.019% \n",
      "[epoch:39, iter:3714] Loss: 0.061 | Acc: 97.064% \n",
      "[epoch:39, iter:3715] Loss: 0.060 | Acc: 97.108% \n",
      "[epoch:39, iter:3716] Loss: 0.059 | Acc: 97.151% \n",
      "[epoch:39, iter:3717] Loss: 0.062 | Acc: 97.011% \n",
      "[epoch:39, iter:3718] Loss: 0.061 | Acc: 97.054% \n",
      "[epoch:39, iter:3719] Loss: 0.061 | Acc: 97.007% \n",
      "[epoch:39, iter:3720] Loss: 0.061 | Acc: 97.049% \n",
      "[epoch:39, iter:3721] Loss: 0.060 | Acc: 97.089% \n",
      "[epoch:39, iter:3722] Loss: 0.059 | Acc: 97.128% \n",
      "[epoch:39, iter:3723] Loss: 0.063 | Acc: 96.917% \n",
      "[epoch:39, iter:3724] Loss: 0.062 | Acc: 96.957% \n",
      "[epoch:39, iter:3725] Loss: 0.062 | Acc: 96.997% \n",
      "[epoch:39, iter:3726] Loss: 0.061 | Acc: 97.035% \n",
      "[epoch:39, iter:3727] Loss: 0.061 | Acc: 96.994% \n",
      "[epoch:39, iter:3728] Loss: 0.061 | Acc: 97.031% \n",
      "[epoch:39, iter:3729] Loss: 0.063 | Acc: 96.836% \n",
      "[epoch:39, iter:3730] Loss: 0.062 | Acc: 96.875% \n",
      "[epoch:39, iter:3731] Loss: 0.062 | Acc: 96.913% \n",
      "[epoch:39, iter:3732] Loss: 0.061 | Acc: 96.949% \n",
      "[epoch:39, iter:3733] Loss: 0.061 | Acc: 96.985% \n",
      "[epoch:39, iter:3734] Loss: 0.061 | Acc: 96.875% \n",
      "[epoch:39, iter:3735] Loss: 0.061 | Acc: 96.911% \n",
      "[epoch:39, iter:3736] Loss: 0.061 | Acc: 96.946% \n",
      "[epoch:39, iter:3737] Loss: 0.061 | Acc: 96.980% \n",
      "[epoch:39, iter:3738] Loss: 0.061 | Acc: 97.014% \n",
      "[epoch:39, iter:3739] Loss: 0.061 | Acc: 96.978% \n",
      "[epoch:39, iter:3740] Loss: 0.061 | Acc: 97.011% \n",
      "[epoch:39, iter:3741] Loss: 0.060 | Acc: 97.043% \n",
      "[epoch:39, iter:3742] Loss: 0.060 | Acc: 97.074% \n",
      "[epoch:39, iter:3743] Loss: 0.059 | Acc: 97.105% \n",
      "[epoch:39, iter:3744] Loss: 0.059 | Acc: 97.122% \n",
      "Waiting Test!\n",
      "测试分类准确率为：84.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 40\n",
      "[epoch:40, iter:3745] Loss: 0.003 | Acc: 100.000% \n",
      "[epoch:40, iter:3746] Loss: 0.088 | Acc: 96.875% \n",
      "[epoch:40, iter:3747] Loss: 0.059 | Acc: 97.917% \n",
      "[epoch:40, iter:3748] Loss: 0.055 | Acc: 98.438% \n",
      "[epoch:40, iter:3749] Loss: 0.045 | Acc: 98.750% \n",
      "[epoch:40, iter:3750] Loss: 0.038 | Acc: 98.958% \n",
      "[epoch:40, iter:3751] Loss: 0.039 | Acc: 99.107% \n",
      "[epoch:40, iter:3752] Loss: 0.041 | Acc: 98.438% \n",
      "[epoch:40, iter:3753] Loss: 0.037 | Acc: 98.611% \n",
      "[epoch:40, iter:3754] Loss: 0.035 | Acc: 98.750% \n",
      "[epoch:40, iter:3755] Loss: 0.032 | Acc: 98.864% \n",
      "[epoch:40, iter:3756] Loss: 0.032 | Acc: 98.958% \n",
      "[epoch:40, iter:3757] Loss: 0.029 | Acc: 99.038% \n",
      "[epoch:40, iter:3758] Loss: 0.027 | Acc: 99.107% \n",
      "[epoch:40, iter:3759] Loss: 0.027 | Acc: 99.167% \n",
      "[epoch:40, iter:3760] Loss: 0.025 | Acc: 99.219% \n",
      "[epoch:40, iter:3761] Loss: 0.024 | Acc: 99.265% \n",
      "[epoch:40, iter:3762] Loss: 0.023 | Acc: 99.306% \n",
      "[epoch:40, iter:3763] Loss: 0.022 | Acc: 99.342% \n",
      "[epoch:40, iter:3764] Loss: 0.021 | Acc: 99.375% \n",
      "[epoch:40, iter:3765] Loss: 0.027 | Acc: 98.810% \n",
      "[epoch:40, iter:3766] Loss: 0.027 | Acc: 98.864% \n",
      "[epoch:40, iter:3767] Loss: 0.029 | Acc: 98.913% \n",
      "[epoch:40, iter:3768] Loss: 0.028 | Acc: 98.958% \n",
      "[epoch:40, iter:3769] Loss: 0.027 | Acc: 99.000% \n",
      "[epoch:40, iter:3770] Loss: 0.027 | Acc: 99.038% \n",
      "[epoch:40, iter:3771] Loss: 0.027 | Acc: 99.074% \n",
      "[epoch:40, iter:3772] Loss: 0.027 | Acc: 99.107% \n",
      "[epoch:40, iter:3773] Loss: 0.026 | Acc: 99.138% \n",
      "[epoch:40, iter:3774] Loss: 0.025 | Acc: 99.167% \n",
      "[epoch:40, iter:3775] Loss: 0.024 | Acc: 99.194% \n",
      "[epoch:40, iter:3776] Loss: 0.023 | Acc: 99.219% \n",
      "[epoch:40, iter:3777] Loss: 0.026 | Acc: 99.053% \n",
      "[epoch:40, iter:3778] Loss: 0.028 | Acc: 98.897% \n",
      "[epoch:40, iter:3779] Loss: 0.027 | Acc: 98.929% \n",
      "[epoch:40, iter:3780] Loss: 0.026 | Acc: 98.958% \n",
      "[epoch:40, iter:3781] Loss: 0.026 | Acc: 98.986% \n",
      "[epoch:40, iter:3782] Loss: 0.030 | Acc: 98.849% \n",
      "[epoch:40, iter:3783] Loss: 0.030 | Acc: 98.718% \n",
      "[epoch:40, iter:3784] Loss: 0.030 | Acc: 98.750% \n",
      "[epoch:40, iter:3785] Loss: 0.033 | Acc: 98.476% \n",
      "[epoch:40, iter:3786] Loss: 0.036 | Acc: 98.363% \n",
      "[epoch:40, iter:3787] Loss: 0.036 | Acc: 98.256% \n",
      "[epoch:40, iter:3788] Loss: 0.036 | Acc: 98.295% \n",
      "[epoch:40, iter:3789] Loss: 0.036 | Acc: 98.333% \n",
      "[epoch:40, iter:3790] Loss: 0.035 | Acc: 98.370% \n",
      "[epoch:40, iter:3791] Loss: 0.035 | Acc: 98.404% \n",
      "[epoch:40, iter:3792] Loss: 0.034 | Acc: 98.438% \n",
      "[epoch:40, iter:3793] Loss: 0.035 | Acc: 98.342% \n",
      "[epoch:40, iter:3794] Loss: 0.034 | Acc: 98.375% \n",
      "[epoch:40, iter:3795] Loss: 0.036 | Acc: 98.284% \n",
      "[epoch:40, iter:3796] Loss: 0.036 | Acc: 98.317% \n",
      "[epoch:40, iter:3797] Loss: 0.035 | Acc: 98.349% \n",
      "[epoch:40, iter:3798] Loss: 0.035 | Acc: 98.380% \n",
      "[epoch:40, iter:3799] Loss: 0.035 | Acc: 98.295% \n",
      "[epoch:40, iter:3800] Loss: 0.035 | Acc: 98.326% \n",
      "[epoch:40, iter:3801] Loss: 0.035 | Acc: 98.355% \n",
      "[epoch:40, iter:3802] Loss: 0.034 | Acc: 98.384% \n",
      "[epoch:40, iter:3803] Loss: 0.036 | Acc: 98.305% \n",
      "[epoch:40, iter:3804] Loss: 0.036 | Acc: 98.229% \n",
      "[epoch:40, iter:3805] Loss: 0.037 | Acc: 98.156% \n",
      "[epoch:40, iter:3806] Loss: 0.036 | Acc: 98.185% \n",
      "[epoch:40, iter:3807] Loss: 0.037 | Acc: 98.115% \n",
      "[epoch:40, iter:3808] Loss: 0.037 | Acc: 98.145% \n",
      "[epoch:40, iter:3809] Loss: 0.036 | Acc: 98.173% \n",
      "[epoch:40, iter:3810] Loss: 0.036 | Acc: 98.201% \n",
      "[epoch:40, iter:3811] Loss: 0.035 | Acc: 98.228% \n",
      "[epoch:40, iter:3812] Loss: 0.035 | Acc: 98.254% \n",
      "[epoch:40, iter:3813] Loss: 0.036 | Acc: 98.188% \n",
      "[epoch:40, iter:3814] Loss: 0.036 | Acc: 98.214% \n",
      "[epoch:40, iter:3815] Loss: 0.036 | Acc: 98.239% \n",
      "[epoch:40, iter:3816] Loss: 0.035 | Acc: 98.264% \n",
      "[epoch:40, iter:3817] Loss: 0.036 | Acc: 98.202% \n",
      "[epoch:40, iter:3818] Loss: 0.036 | Acc: 98.226% \n",
      "[epoch:40, iter:3819] Loss: 0.035 | Acc: 98.250% \n",
      "[epoch:40, iter:3820] Loss: 0.035 | Acc: 98.273% \n",
      "[epoch:40, iter:3821] Loss: 0.035 | Acc: 98.295% \n",
      "[epoch:40, iter:3822] Loss: 0.034 | Acc: 98.317% \n",
      "[epoch:40, iter:3823] Loss: 0.038 | Acc: 98.180% \n",
      "[epoch:40, iter:3824] Loss: 0.037 | Acc: 98.203% \n",
      "[epoch:40, iter:3825] Loss: 0.038 | Acc: 98.148% \n",
      "[epoch:40, iter:3826] Loss: 0.037 | Acc: 98.171% \n",
      "[epoch:40, iter:3827] Loss: 0.037 | Acc: 98.193% \n",
      "[epoch:40, iter:3828] Loss: 0.037 | Acc: 98.214% \n",
      "[epoch:40, iter:3829] Loss: 0.041 | Acc: 98.015% \n",
      "[epoch:40, iter:3830] Loss: 0.041 | Acc: 97.965% \n",
      "[epoch:40, iter:3831] Loss: 0.041 | Acc: 97.989% \n",
      "[epoch:40, iter:3832] Loss: 0.041 | Acc: 98.011% \n",
      "[epoch:40, iter:3833] Loss: 0.041 | Acc: 98.034% \n",
      "[epoch:40, iter:3834] Loss: 0.040 | Acc: 98.056% \n",
      "[epoch:40, iter:3835] Loss: 0.040 | Acc: 98.077% \n",
      "[epoch:40, iter:3836] Loss: 0.039 | Acc: 98.098% \n",
      "[epoch:40, iter:3837] Loss: 0.040 | Acc: 98.118% \n",
      "[epoch:40, iter:3838] Loss: 0.040 | Acc: 98.138% \n",
      "[epoch:40, iter:3839] Loss: 0.040 | Acc: 98.092% \n",
      "[epoch:40, iter:3840] Loss: 0.040 | Acc: 98.103% \n",
      "Waiting Test!\n",
      "测试分类准确率为：79.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 41\n",
      "[epoch:41, iter:3841] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:41, iter:3842] Loss: 0.129 | Acc: 96.875% \n",
      "[epoch:41, iter:3843] Loss: 0.101 | Acc: 97.917% \n",
      "[epoch:41, iter:3844] Loss: 0.083 | Acc: 98.438% \n",
      "[epoch:41, iter:3845] Loss: 0.071 | Acc: 98.750% \n",
      "[epoch:41, iter:3846] Loss: 0.062 | Acc: 98.958% \n",
      "[epoch:41, iter:3847] Loss: 0.055 | Acc: 99.107% \n",
      "[epoch:41, iter:3848] Loss: 0.049 | Acc: 99.219% \n",
      "[epoch:41, iter:3849] Loss: 0.055 | Acc: 97.917% \n",
      "[epoch:41, iter:3850] Loss: 0.050 | Acc: 98.125% \n",
      "[epoch:41, iter:3851] Loss: 0.046 | Acc: 98.295% \n",
      "[epoch:41, iter:3852] Loss: 0.044 | Acc: 98.438% \n",
      "[epoch:41, iter:3853] Loss: 0.049 | Acc: 98.077% \n",
      "[epoch:41, iter:3854] Loss: 0.049 | Acc: 98.214% \n",
      "[epoch:41, iter:3855] Loss: 0.050 | Acc: 97.917% \n",
      "[epoch:41, iter:3856] Loss: 0.048 | Acc: 98.047% \n",
      "[epoch:41, iter:3857] Loss: 0.052 | Acc: 97.426% \n",
      "[epoch:41, iter:3858] Loss: 0.049 | Acc: 97.569% \n",
      "[epoch:41, iter:3859] Loss: 0.048 | Acc: 97.697% \n",
      "[epoch:41, iter:3860] Loss: 0.048 | Acc: 97.500% \n",
      "[epoch:41, iter:3861] Loss: 0.047 | Acc: 97.619% \n",
      "[epoch:41, iter:3862] Loss: 0.045 | Acc: 97.727% \n",
      "[epoch:41, iter:3863] Loss: 0.044 | Acc: 97.826% \n",
      "[epoch:41, iter:3864] Loss: 0.043 | Acc: 97.917% \n",
      "[epoch:41, iter:3865] Loss: 0.043 | Acc: 98.000% \n",
      "[epoch:41, iter:3866] Loss: 0.045 | Acc: 97.837% \n",
      "[epoch:41, iter:3867] Loss: 0.045 | Acc: 97.917% \n",
      "[epoch:41, iter:3868] Loss: 0.043 | Acc: 97.991% \n",
      "[epoch:41, iter:3869] Loss: 0.042 | Acc: 98.060% \n",
      "[epoch:41, iter:3870] Loss: 0.045 | Acc: 97.917% \n",
      "[epoch:41, iter:3871] Loss: 0.043 | Acc: 97.984% \n",
      "[epoch:41, iter:3872] Loss: 0.042 | Acc: 98.047% \n",
      "[epoch:41, iter:3873] Loss: 0.041 | Acc: 98.106% \n",
      "[epoch:41, iter:3874] Loss: 0.040 | Acc: 98.162% \n",
      "[epoch:41, iter:3875] Loss: 0.039 | Acc: 98.214% \n",
      "[epoch:41, iter:3876] Loss: 0.038 | Acc: 98.264% \n",
      "[epoch:41, iter:3877] Loss: 0.039 | Acc: 98.142% \n",
      "[epoch:41, iter:3878] Loss: 0.039 | Acc: 98.191% \n",
      "[epoch:41, iter:3879] Loss: 0.038 | Acc: 98.237% \n",
      "[epoch:41, iter:3880] Loss: 0.037 | Acc: 98.281% \n",
      "[epoch:41, iter:3881] Loss: 0.048 | Acc: 98.018% \n",
      "[epoch:41, iter:3882] Loss: 0.050 | Acc: 97.917% \n",
      "[epoch:41, iter:3883] Loss: 0.049 | Acc: 97.965% \n",
      "[epoch:41, iter:3884] Loss: 0.048 | Acc: 98.011% \n",
      "[epoch:41, iter:3885] Loss: 0.047 | Acc: 98.056% \n",
      "[epoch:41, iter:3886] Loss: 0.049 | Acc: 97.962% \n",
      "[epoch:41, iter:3887] Loss: 0.048 | Acc: 98.005% \n",
      "[epoch:41, iter:3888] Loss: 0.049 | Acc: 97.917% \n",
      "[epoch:41, iter:3889] Loss: 0.048 | Acc: 97.959% \n",
      "[epoch:41, iter:3890] Loss: 0.056 | Acc: 97.625% \n",
      "[epoch:41, iter:3891] Loss: 0.055 | Acc: 97.672% \n",
      "[epoch:41, iter:3892] Loss: 0.054 | Acc: 97.716% \n",
      "[epoch:41, iter:3893] Loss: 0.054 | Acc: 97.759% \n",
      "[epoch:41, iter:3894] Loss: 0.053 | Acc: 97.801% \n",
      "[epoch:41, iter:3895] Loss: 0.052 | Acc: 97.841% \n",
      "[epoch:41, iter:3896] Loss: 0.051 | Acc: 97.879% \n",
      "[epoch:41, iter:3897] Loss: 0.050 | Acc: 97.917% \n",
      "[epoch:41, iter:3898] Loss: 0.050 | Acc: 97.953% \n",
      "[epoch:41, iter:3899] Loss: 0.050 | Acc: 97.987% \n",
      "[epoch:41, iter:3900] Loss: 0.052 | Acc: 97.917% \n",
      "[epoch:41, iter:3901] Loss: 0.051 | Acc: 97.951% \n",
      "[epoch:41, iter:3902] Loss: 0.050 | Acc: 97.984% \n",
      "[epoch:41, iter:3903] Loss: 0.049 | Acc: 98.016% \n",
      "[epoch:41, iter:3904] Loss: 0.049 | Acc: 98.047% \n",
      "[epoch:41, iter:3905] Loss: 0.048 | Acc: 98.077% \n",
      "[epoch:41, iter:3906] Loss: 0.048 | Acc: 98.106% \n",
      "[epoch:41, iter:3907] Loss: 0.047 | Acc: 98.134% \n",
      "[epoch:41, iter:3908] Loss: 0.048 | Acc: 98.070% \n",
      "[epoch:41, iter:3909] Loss: 0.048 | Acc: 98.007% \n",
      "[epoch:41, iter:3910] Loss: 0.048 | Acc: 98.036% \n",
      "[epoch:41, iter:3911] Loss: 0.047 | Acc: 98.063% \n",
      "[epoch:41, iter:3912] Loss: 0.047 | Acc: 98.090% \n",
      "[epoch:41, iter:3913] Loss: 0.047 | Acc: 98.116% \n",
      "[epoch:41, iter:3914] Loss: 0.046 | Acc: 98.142% \n",
      "[epoch:41, iter:3915] Loss: 0.046 | Acc: 98.167% \n",
      "[epoch:41, iter:3916] Loss: 0.047 | Acc: 98.109% \n",
      "[epoch:41, iter:3917] Loss: 0.046 | Acc: 98.133% \n",
      "[epoch:41, iter:3918] Loss: 0.046 | Acc: 98.157% \n",
      "[epoch:41, iter:3919] Loss: 0.045 | Acc: 98.180% \n",
      "[epoch:41, iter:3920] Loss: 0.047 | Acc: 98.125% \n",
      "[epoch:41, iter:3921] Loss: 0.046 | Acc: 98.148% \n",
      "[epoch:41, iter:3922] Loss: 0.046 | Acc: 98.171% \n",
      "[epoch:41, iter:3923] Loss: 0.046 | Acc: 98.193% \n",
      "[epoch:41, iter:3924] Loss: 0.048 | Acc: 97.991% \n",
      "[epoch:41, iter:3925] Loss: 0.048 | Acc: 98.015% \n",
      "[epoch:41, iter:3926] Loss: 0.047 | Acc: 98.038% \n",
      "[epoch:41, iter:3927] Loss: 0.047 | Acc: 98.060% \n",
      "[epoch:41, iter:3928] Loss: 0.046 | Acc: 98.082% \n",
      "[epoch:41, iter:3929] Loss: 0.050 | Acc: 97.963% \n",
      "[epoch:41, iter:3930] Loss: 0.050 | Acc: 97.917% \n",
      "[epoch:41, iter:3931] Loss: 0.049 | Acc: 97.940% \n",
      "[epoch:41, iter:3932] Loss: 0.049 | Acc: 97.962% \n",
      "[epoch:41, iter:3933] Loss: 0.049 | Acc: 97.917% \n",
      "[epoch:41, iter:3934] Loss: 0.048 | Acc: 97.939% \n",
      "[epoch:41, iter:3935] Loss: 0.054 | Acc: 97.763% \n",
      "[epoch:41, iter:3936] Loss: 0.059 | Acc: 97.646% \n",
      "Waiting Test!\n",
      "测试分类准确率为：72.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 42\n",
      "[epoch:42, iter:3937] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:42, iter:3938] Loss: 0.016 | Acc: 100.000% \n",
      "[epoch:42, iter:3939] Loss: 0.034 | Acc: 97.917% \n",
      "[epoch:42, iter:3940] Loss: 0.027 | Acc: 98.438% \n",
      "[epoch:42, iter:3941] Loss: 0.041 | Acc: 97.500% \n",
      "[epoch:42, iter:3942] Loss: 0.042 | Acc: 96.875% \n",
      "[epoch:42, iter:3943] Loss: 0.049 | Acc: 96.429% \n",
      "[epoch:42, iter:3944] Loss: 0.055 | Acc: 96.875% \n",
      "[epoch:42, iter:3945] Loss: 0.064 | Acc: 96.528% \n",
      "[epoch:42, iter:3946] Loss: 0.059 | Acc: 96.875% \n",
      "[epoch:42, iter:3947] Loss: 0.063 | Acc: 97.159% \n",
      "[epoch:42, iter:3948] Loss: 0.062 | Acc: 97.396% \n",
      "[epoch:42, iter:3949] Loss: 0.059 | Acc: 97.596% \n",
      "[epoch:42, iter:3950] Loss: 0.055 | Acc: 97.768% \n",
      "[epoch:42, iter:3951] Loss: 0.073 | Acc: 97.083% \n",
      "[epoch:42, iter:3952] Loss: 0.072 | Acc: 97.266% \n",
      "[epoch:42, iter:3953] Loss: 0.068 | Acc: 97.426% \n",
      "[epoch:42, iter:3954] Loss: 0.069 | Acc: 97.222% \n",
      "[epoch:42, iter:3955] Loss: 0.072 | Acc: 97.039% \n",
      "[epoch:42, iter:3956] Loss: 0.078 | Acc: 96.875% \n",
      "[epoch:42, iter:3957] Loss: 0.075 | Acc: 97.024% \n",
      "[epoch:42, iter:3958] Loss: 0.082 | Acc: 96.591% \n",
      "[epoch:42, iter:3959] Loss: 0.081 | Acc: 96.739% \n",
      "[epoch:42, iter:3960] Loss: 0.078 | Acc: 96.875% \n",
      "[epoch:42, iter:3961] Loss: 0.076 | Acc: 97.000% \n",
      "[epoch:42, iter:3962] Loss: 0.074 | Acc: 97.115% \n",
      "[epoch:42, iter:3963] Loss: 0.072 | Acc: 97.222% \n",
      "[epoch:42, iter:3964] Loss: 0.072 | Acc: 97.098% \n",
      "[epoch:42, iter:3965] Loss: 0.071 | Acc: 96.983% \n",
      "[epoch:42, iter:3966] Loss: 0.073 | Acc: 96.875% \n",
      "[epoch:42, iter:3967] Loss: 0.072 | Acc: 96.976% \n",
      "[epoch:42, iter:3968] Loss: 0.070 | Acc: 97.070% \n",
      "[epoch:42, iter:3969] Loss: 0.069 | Acc: 97.159% \n",
      "[epoch:42, iter:3970] Loss: 0.068 | Acc: 97.243% \n",
      "[epoch:42, iter:3971] Loss: 0.068 | Acc: 97.143% \n",
      "[epoch:42, iter:3972] Loss: 0.068 | Acc: 97.049% \n",
      "[epoch:42, iter:3973] Loss: 0.066 | Acc: 97.128% \n",
      "[epoch:42, iter:3974] Loss: 0.068 | Acc: 97.039% \n",
      "[epoch:42, iter:3975] Loss: 0.077 | Acc: 96.635% \n",
      "[epoch:42, iter:3976] Loss: 0.076 | Acc: 96.719% \n",
      "[epoch:42, iter:3977] Loss: 0.078 | Acc: 96.646% \n",
      "[epoch:42, iter:3978] Loss: 0.077 | Acc: 96.726% \n",
      "[epoch:42, iter:3979] Loss: 0.075 | Acc: 96.802% \n",
      "[epoch:42, iter:3980] Loss: 0.073 | Acc: 96.875% \n",
      "[epoch:42, iter:3981] Loss: 0.072 | Acc: 96.944% \n",
      "[epoch:42, iter:3982] Loss: 0.070 | Acc: 97.011% \n",
      "[epoch:42, iter:3983] Loss: 0.069 | Acc: 97.074% \n",
      "[epoch:42, iter:3984] Loss: 0.069 | Acc: 97.005% \n",
      "[epoch:42, iter:3985] Loss: 0.069 | Acc: 97.066% \n",
      "[epoch:42, iter:3986] Loss: 0.069 | Acc: 97.000% \n",
      "[epoch:42, iter:3987] Loss: 0.069 | Acc: 97.059% \n",
      "[epoch:42, iter:3988] Loss: 0.068 | Acc: 96.995% \n",
      "[epoch:42, iter:3989] Loss: 0.069 | Acc: 96.934% \n",
      "[epoch:42, iter:3990] Loss: 0.067 | Acc: 96.991% \n",
      "[epoch:42, iter:3991] Loss: 0.066 | Acc: 97.045% \n",
      "[epoch:42, iter:3992] Loss: 0.065 | Acc: 97.098% \n",
      "[epoch:42, iter:3993] Loss: 0.064 | Acc: 97.149% \n",
      "[epoch:42, iter:3994] Loss: 0.064 | Acc: 97.198% \n",
      "[epoch:42, iter:3995] Loss: 0.064 | Acc: 97.140% \n",
      "[epoch:42, iter:3996] Loss: 0.063 | Acc: 97.188% \n",
      "[epoch:42, iter:3997] Loss: 0.062 | Acc: 97.234% \n",
      "[epoch:42, iter:3998] Loss: 0.065 | Acc: 97.177% \n",
      "[epoch:42, iter:3999] Loss: 0.064 | Acc: 97.222% \n",
      "[epoch:42, iter:4000] Loss: 0.064 | Acc: 97.266% \n",
      "[epoch:42, iter:4001] Loss: 0.069 | Acc: 97.115% \n",
      "[epoch:42, iter:4002] Loss: 0.068 | Acc: 97.159% \n",
      "[epoch:42, iter:4003] Loss: 0.067 | Acc: 97.201% \n",
      "[epoch:42, iter:4004] Loss: 0.066 | Acc: 97.243% \n",
      "[epoch:42, iter:4005] Loss: 0.069 | Acc: 97.192% \n",
      "[epoch:42, iter:4006] Loss: 0.078 | Acc: 96.786% \n",
      "[epoch:42, iter:4007] Loss: 0.077 | Acc: 96.831% \n",
      "[epoch:42, iter:4008] Loss: 0.077 | Acc: 96.788% \n",
      "[epoch:42, iter:4009] Loss: 0.077 | Acc: 96.747% \n",
      "[epoch:42, iter:4010] Loss: 0.077 | Acc: 96.706% \n",
      "[epoch:42, iter:4011] Loss: 0.076 | Acc: 96.750% \n",
      "[epoch:42, iter:4012] Loss: 0.076 | Acc: 96.793% \n",
      "[epoch:42, iter:4013] Loss: 0.075 | Acc: 96.834% \n",
      "[epoch:42, iter:4014] Loss: 0.075 | Acc: 96.875% \n",
      "[epoch:42, iter:4015] Loss: 0.075 | Acc: 96.835% \n",
      "[epoch:42, iter:4016] Loss: 0.074 | Acc: 96.875% \n",
      "[epoch:42, iter:4017] Loss: 0.074 | Acc: 96.914% \n",
      "[epoch:42, iter:4018] Loss: 0.073 | Acc: 96.951% \n",
      "[epoch:42, iter:4019] Loss: 0.073 | Acc: 96.988% \n",
      "[epoch:42, iter:4020] Loss: 0.074 | Acc: 96.949% \n",
      "[epoch:42, iter:4021] Loss: 0.073 | Acc: 96.985% \n",
      "[epoch:42, iter:4022] Loss: 0.073 | Acc: 97.020% \n",
      "[epoch:42, iter:4023] Loss: 0.072 | Acc: 97.055% \n",
      "[epoch:42, iter:4024] Loss: 0.072 | Acc: 97.088% \n",
      "[epoch:42, iter:4025] Loss: 0.072 | Acc: 97.051% \n",
      "[epoch:42, iter:4026] Loss: 0.072 | Acc: 97.083% \n",
      "[epoch:42, iter:4027] Loss: 0.071 | Acc: 97.115% \n",
      "[epoch:42, iter:4028] Loss: 0.071 | Acc: 97.147% \n",
      "[epoch:42, iter:4029] Loss: 0.072 | Acc: 97.110% \n",
      "[epoch:42, iter:4030] Loss: 0.074 | Acc: 96.941% \n",
      "[epoch:42, iter:4031] Loss: 0.074 | Acc: 96.974% \n",
      "[epoch:42, iter:4032] Loss: 0.075 | Acc: 96.926% \n",
      "Waiting Test!\n",
      "测试分类准确率为：81.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 43\n",
      "[epoch:43, iter:4033] Loss: 0.006 | Acc: 100.000% \n",
      "[epoch:43, iter:4034] Loss: 0.007 | Acc: 100.000% \n",
      "[epoch:43, iter:4035] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:43, iter:4036] Loss: 0.035 | Acc: 98.438% \n",
      "[epoch:43, iter:4037] Loss: 0.046 | Acc: 98.750% \n",
      "[epoch:43, iter:4038] Loss: 0.039 | Acc: 98.958% \n",
      "[epoch:43, iter:4039] Loss: 0.042 | Acc: 99.107% \n",
      "[epoch:43, iter:4040] Loss: 0.046 | Acc: 98.438% \n",
      "[epoch:43, iter:4041] Loss: 0.050 | Acc: 97.917% \n",
      "[epoch:43, iter:4042] Loss: 0.046 | Acc: 98.125% \n",
      "[epoch:43, iter:4043] Loss: 0.046 | Acc: 98.295% \n",
      "[epoch:43, iter:4044] Loss: 0.042 | Acc: 98.438% \n",
      "[epoch:43, iter:4045] Loss: 0.040 | Acc: 98.558% \n",
      "[epoch:43, iter:4046] Loss: 0.044 | Acc: 97.768% \n",
      "[epoch:43, iter:4047] Loss: 0.042 | Acc: 97.917% \n",
      "[epoch:43, iter:4048] Loss: 0.046 | Acc: 97.656% \n",
      "[epoch:43, iter:4049] Loss: 0.054 | Acc: 97.059% \n",
      "[epoch:43, iter:4050] Loss: 0.052 | Acc: 97.222% \n",
      "[epoch:43, iter:4051] Loss: 0.051 | Acc: 97.368% \n",
      "[epoch:43, iter:4052] Loss: 0.063 | Acc: 96.875% \n",
      "[epoch:43, iter:4053] Loss: 0.070 | Acc: 96.726% \n",
      "[epoch:43, iter:4054] Loss: 0.067 | Acc: 96.875% \n",
      "[epoch:43, iter:4055] Loss: 0.065 | Acc: 97.011% \n",
      "[epoch:43, iter:4056] Loss: 0.063 | Acc: 97.135% \n",
      "[epoch:43, iter:4057] Loss: 0.061 | Acc: 97.250% \n",
      "[epoch:43, iter:4058] Loss: 0.059 | Acc: 97.356% \n",
      "[epoch:43, iter:4059] Loss: 0.057 | Acc: 97.454% \n",
      "[epoch:43, iter:4060] Loss: 0.055 | Acc: 97.545% \n",
      "[epoch:43, iter:4061] Loss: 0.054 | Acc: 97.629% \n",
      "[epoch:43, iter:4062] Loss: 0.055 | Acc: 97.500% \n",
      "[epoch:43, iter:4063] Loss: 0.054 | Acc: 97.581% \n",
      "[epoch:43, iter:4064] Loss: 0.055 | Acc: 97.461% \n",
      "[epoch:43, iter:4065] Loss: 0.053 | Acc: 97.538% \n",
      "[epoch:43, iter:4066] Loss: 0.052 | Acc: 97.610% \n",
      "[epoch:43, iter:4067] Loss: 0.053 | Acc: 97.500% \n",
      "[epoch:43, iter:4068] Loss: 0.053 | Acc: 97.396% \n",
      "[epoch:43, iter:4069] Loss: 0.052 | Acc: 97.466% \n",
      "[epoch:43, iter:4070] Loss: 0.054 | Acc: 97.368% \n",
      "[epoch:43, iter:4071] Loss: 0.054 | Acc: 97.276% \n",
      "[epoch:43, iter:4072] Loss: 0.053 | Acc: 97.344% \n",
      "[epoch:43, iter:4073] Loss: 0.052 | Acc: 97.409% \n",
      "[epoch:43, iter:4074] Loss: 0.051 | Acc: 97.470% \n",
      "[epoch:43, iter:4075] Loss: 0.050 | Acc: 97.529% \n",
      "[epoch:43, iter:4076] Loss: 0.051 | Acc: 97.443% \n",
      "[epoch:43, iter:4077] Loss: 0.050 | Acc: 97.500% \n",
      "[epoch:43, iter:4078] Loss: 0.053 | Acc: 97.418% \n",
      "[epoch:43, iter:4079] Loss: 0.052 | Acc: 97.473% \n",
      "[epoch:43, iter:4080] Loss: 0.053 | Acc: 97.396% \n",
      "[epoch:43, iter:4081] Loss: 0.057 | Acc: 97.321% \n",
      "[epoch:43, iter:4082] Loss: 0.057 | Acc: 97.375% \n",
      "[epoch:43, iter:4083] Loss: 0.056 | Acc: 97.426% \n",
      "[epoch:43, iter:4084] Loss: 0.055 | Acc: 97.476% \n",
      "[epoch:43, iter:4085] Loss: 0.055 | Acc: 97.524% \n",
      "[epoch:43, iter:4086] Loss: 0.054 | Acc: 97.569% \n",
      "[epoch:43, iter:4087] Loss: 0.053 | Acc: 97.614% \n",
      "[epoch:43, iter:4088] Loss: 0.052 | Acc: 97.656% \n",
      "[epoch:43, iter:4089] Loss: 0.051 | Acc: 97.697% \n",
      "[epoch:43, iter:4090] Loss: 0.050 | Acc: 97.737% \n",
      "[epoch:43, iter:4091] Loss: 0.051 | Acc: 97.775% \n",
      "[epoch:43, iter:4092] Loss: 0.051 | Acc: 97.708% \n",
      "[epoch:43, iter:4093] Loss: 0.052 | Acc: 97.643% \n",
      "[epoch:43, iter:4094] Loss: 0.051 | Acc: 97.681% \n",
      "[epoch:43, iter:4095] Loss: 0.050 | Acc: 97.718% \n",
      "[epoch:43, iter:4096] Loss: 0.049 | Acc: 97.754% \n",
      "[epoch:43, iter:4097] Loss: 0.049 | Acc: 97.788% \n",
      "[epoch:43, iter:4098] Loss: 0.049 | Acc: 97.822% \n",
      "[epoch:43, iter:4099] Loss: 0.048 | Acc: 97.854% \n",
      "[epoch:43, iter:4100] Loss: 0.048 | Acc: 97.886% \n",
      "[epoch:43, iter:4101] Loss: 0.047 | Acc: 97.917% \n",
      "[epoch:43, iter:4102] Loss: 0.046 | Acc: 97.946% \n",
      "[epoch:43, iter:4103] Loss: 0.047 | Acc: 97.887% \n",
      "[epoch:43, iter:4104] Loss: 0.046 | Acc: 97.917% \n",
      "[epoch:43, iter:4105] Loss: 0.046 | Acc: 97.945% \n",
      "[epoch:43, iter:4106] Loss: 0.047 | Acc: 97.889% \n",
      "[epoch:43, iter:4107] Loss: 0.047 | Acc: 97.917% \n",
      "[epoch:43, iter:4108] Loss: 0.047 | Acc: 97.944% \n",
      "[epoch:43, iter:4109] Loss: 0.048 | Acc: 97.890% \n",
      "[epoch:43, iter:4110] Loss: 0.049 | Acc: 97.837% \n",
      "[epoch:43, iter:4111] Loss: 0.048 | Acc: 97.864% \n",
      "[epoch:43, iter:4112] Loss: 0.048 | Acc: 97.891% \n",
      "[epoch:43, iter:4113] Loss: 0.050 | Acc: 97.762% \n",
      "[epoch:43, iter:4114] Loss: 0.049 | Acc: 97.790% \n",
      "[epoch:43, iter:4115] Loss: 0.049 | Acc: 97.816% \n",
      "[epoch:43, iter:4116] Loss: 0.048 | Acc: 97.842% \n",
      "[epoch:43, iter:4117] Loss: 0.053 | Acc: 97.794% \n",
      "[epoch:43, iter:4118] Loss: 0.053 | Acc: 97.820% \n",
      "[epoch:43, iter:4119] Loss: 0.053 | Acc: 97.845% \n",
      "[epoch:43, iter:4120] Loss: 0.052 | Acc: 97.869% \n",
      "[epoch:43, iter:4121] Loss: 0.054 | Acc: 97.753% \n",
      "[epoch:43, iter:4122] Loss: 0.054 | Acc: 97.778% \n",
      "[epoch:43, iter:4123] Loss: 0.053 | Acc: 97.802% \n",
      "[epoch:43, iter:4124] Loss: 0.053 | Acc: 97.758% \n",
      "[epoch:43, iter:4125] Loss: 0.053 | Acc: 97.782% \n",
      "[epoch:43, iter:4126] Loss: 0.053 | Acc: 97.739% \n",
      "[epoch:43, iter:4127] Loss: 0.053 | Acc: 97.763% \n",
      "[epoch:43, iter:4128] Loss: 0.054 | Acc: 97.711% \n",
      "Waiting Test!\n",
      "测试分类准确率为：68.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 44\n",
      "[epoch:44, iter:4129] Loss: 0.028 | Acc: 100.000% \n",
      "[epoch:44, iter:4130] Loss: 0.044 | Acc: 100.000% \n",
      "[epoch:44, iter:4131] Loss: 0.031 | Acc: 100.000% \n",
      "[epoch:44, iter:4132] Loss: 0.027 | Acc: 100.000% \n",
      "[epoch:44, iter:4133] Loss: 0.041 | Acc: 98.750% \n",
      "[epoch:44, iter:4134] Loss: 0.037 | Acc: 98.958% \n",
      "[epoch:44, iter:4135] Loss: 0.032 | Acc: 99.107% \n",
      "[epoch:44, iter:4136] Loss: 0.029 | Acc: 99.219% \n",
      "[epoch:44, iter:4137] Loss: 0.028 | Acc: 99.306% \n",
      "[epoch:44, iter:4138] Loss: 0.031 | Acc: 98.750% \n",
      "[epoch:44, iter:4139] Loss: 0.035 | Acc: 98.295% \n",
      "[epoch:44, iter:4140] Loss: 0.032 | Acc: 98.438% \n",
      "[epoch:44, iter:4141] Loss: 0.033 | Acc: 98.077% \n",
      "[epoch:44, iter:4142] Loss: 0.033 | Acc: 98.214% \n",
      "[epoch:44, iter:4143] Loss: 0.032 | Acc: 98.333% \n",
      "[epoch:44, iter:4144] Loss: 0.035 | Acc: 98.047% \n",
      "[epoch:44, iter:4145] Loss: 0.033 | Acc: 98.162% \n",
      "[epoch:44, iter:4146] Loss: 0.032 | Acc: 98.264% \n",
      "[epoch:44, iter:4147] Loss: 0.030 | Acc: 98.355% \n",
      "[epoch:44, iter:4148] Loss: 0.029 | Acc: 98.438% \n",
      "[epoch:44, iter:4149] Loss: 0.028 | Acc: 98.512% \n",
      "[epoch:44, iter:4150] Loss: 0.027 | Acc: 98.580% \n",
      "[epoch:44, iter:4151] Loss: 0.026 | Acc: 98.641% \n",
      "[epoch:44, iter:4152] Loss: 0.030 | Acc: 98.438% \n",
      "[epoch:44, iter:4153] Loss: 0.031 | Acc: 98.250% \n",
      "[epoch:44, iter:4154] Loss: 0.032 | Acc: 98.077% \n",
      "[epoch:44, iter:4155] Loss: 0.031 | Acc: 98.148% \n",
      "[epoch:44, iter:4156] Loss: 0.032 | Acc: 98.214% \n",
      "[epoch:44, iter:4157] Loss: 0.031 | Acc: 98.276% \n",
      "[epoch:44, iter:4158] Loss: 0.031 | Acc: 98.333% \n",
      "[epoch:44, iter:4159] Loss: 0.031 | Acc: 98.185% \n",
      "[epoch:44, iter:4160] Loss: 0.031 | Acc: 98.242% \n",
      "[epoch:44, iter:4161] Loss: 0.030 | Acc: 98.295% \n",
      "[epoch:44, iter:4162] Loss: 0.029 | Acc: 98.346% \n",
      "[epoch:44, iter:4163] Loss: 0.029 | Acc: 98.393% \n",
      "[epoch:44, iter:4164] Loss: 0.028 | Acc: 98.438% \n",
      "[epoch:44, iter:4165] Loss: 0.028 | Acc: 98.480% \n",
      "[epoch:44, iter:4166] Loss: 0.028 | Acc: 98.520% \n",
      "[epoch:44, iter:4167] Loss: 0.028 | Acc: 98.558% \n",
      "[epoch:44, iter:4168] Loss: 0.027 | Acc: 98.594% \n",
      "[epoch:44, iter:4169] Loss: 0.028 | Acc: 98.476% \n",
      "[epoch:44, iter:4170] Loss: 0.030 | Acc: 98.363% \n",
      "[epoch:44, iter:4171] Loss: 0.030 | Acc: 98.401% \n",
      "[epoch:44, iter:4172] Loss: 0.029 | Acc: 98.438% \n",
      "[epoch:44, iter:4173] Loss: 0.029 | Acc: 98.472% \n",
      "[epoch:44, iter:4174] Loss: 0.028 | Acc: 98.505% \n",
      "[epoch:44, iter:4175] Loss: 0.030 | Acc: 98.271% \n",
      "[epoch:44, iter:4176] Loss: 0.030 | Acc: 98.307% \n",
      "[epoch:44, iter:4177] Loss: 0.029 | Acc: 98.342% \n",
      "[epoch:44, iter:4178] Loss: 0.030 | Acc: 98.250% \n",
      "[epoch:44, iter:4179] Loss: 0.031 | Acc: 98.284% \n",
      "[epoch:44, iter:4180] Loss: 0.030 | Acc: 98.317% \n",
      "[epoch:44, iter:4181] Loss: 0.030 | Acc: 98.349% \n",
      "[epoch:44, iter:4182] Loss: 0.029 | Acc: 98.380% \n",
      "[epoch:44, iter:4183] Loss: 0.029 | Acc: 98.409% \n",
      "[epoch:44, iter:4184] Loss: 0.029 | Acc: 98.438% \n",
      "[epoch:44, iter:4185] Loss: 0.028 | Acc: 98.465% \n",
      "[epoch:44, iter:4186] Loss: 0.028 | Acc: 98.491% \n",
      "[epoch:44, iter:4187] Loss: 0.028 | Acc: 98.411% \n",
      "[epoch:44, iter:4188] Loss: 0.028 | Acc: 98.438% \n",
      "[epoch:44, iter:4189] Loss: 0.029 | Acc: 98.361% \n",
      "[epoch:44, iter:4190] Loss: 0.031 | Acc: 98.286% \n",
      "[epoch:44, iter:4191] Loss: 0.030 | Acc: 98.313% \n",
      "[epoch:44, iter:4192] Loss: 0.030 | Acc: 98.340% \n",
      "[epoch:44, iter:4193] Loss: 0.030 | Acc: 98.365% \n",
      "[epoch:44, iter:4194] Loss: 0.030 | Acc: 98.390% \n",
      "[epoch:44, iter:4195] Loss: 0.029 | Acc: 98.414% \n",
      "[epoch:44, iter:4196] Loss: 0.029 | Acc: 98.438% \n",
      "[epoch:44, iter:4197] Loss: 0.029 | Acc: 98.370% \n",
      "[epoch:44, iter:4198] Loss: 0.029 | Acc: 98.393% \n",
      "[epoch:44, iter:4199] Loss: 0.028 | Acc: 98.415% \n",
      "[epoch:44, iter:4200] Loss: 0.029 | Acc: 98.351% \n",
      "[epoch:44, iter:4201] Loss: 0.030 | Acc: 98.288% \n",
      "[epoch:44, iter:4202] Loss: 0.030 | Acc: 98.226% \n",
      "[epoch:44, iter:4203] Loss: 0.030 | Acc: 98.250% \n",
      "[epoch:44, iter:4204] Loss: 0.030 | Acc: 98.273% \n",
      "[epoch:44, iter:4205] Loss: 0.031 | Acc: 98.214% \n",
      "[epoch:44, iter:4206] Loss: 0.031 | Acc: 98.237% \n",
      "[epoch:44, iter:4207] Loss: 0.031 | Acc: 98.259% \n",
      "[epoch:44, iter:4208] Loss: 0.030 | Acc: 98.281% \n",
      "[epoch:44, iter:4209] Loss: 0.031 | Acc: 98.225% \n",
      "[epoch:44, iter:4210] Loss: 0.031 | Acc: 98.247% \n",
      "[epoch:44, iter:4211] Loss: 0.032 | Acc: 98.268% \n",
      "[epoch:44, iter:4212] Loss: 0.032 | Acc: 98.214% \n",
      "[epoch:44, iter:4213] Loss: 0.032 | Acc: 98.235% \n",
      "[epoch:44, iter:4214] Loss: 0.032 | Acc: 98.256% \n",
      "[epoch:44, iter:4215] Loss: 0.032 | Acc: 98.276% \n",
      "[epoch:44, iter:4216] Loss: 0.031 | Acc: 98.295% \n",
      "[epoch:44, iter:4217] Loss: 0.031 | Acc: 98.315% \n",
      "[epoch:44, iter:4218] Loss: 0.032 | Acc: 98.333% \n",
      "[epoch:44, iter:4219] Loss: 0.032 | Acc: 98.283% \n",
      "[epoch:44, iter:4220] Loss: 0.032 | Acc: 98.302% \n",
      "[epoch:44, iter:4221] Loss: 0.032 | Acc: 98.320% \n",
      "[epoch:44, iter:4222] Loss: 0.032 | Acc: 98.271% \n",
      "[epoch:44, iter:4223] Loss: 0.032 | Acc: 98.289% \n",
      "[epoch:44, iter:4224] Loss: 0.032 | Acc: 98.300% \n",
      "Waiting Test!\n",
      "测试分类准确率为：83.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 45\n",
      "[epoch:45, iter:4225] Loss: 0.048 | Acc: 93.750% \n",
      "[epoch:45, iter:4226] Loss: 0.026 | Acc: 96.875% \n",
      "[epoch:45, iter:4227] Loss: 0.018 | Acc: 97.917% \n",
      "[epoch:45, iter:4228] Loss: 0.015 | Acc: 98.438% \n",
      "[epoch:45, iter:4229] Loss: 0.022 | Acc: 98.750% \n",
      "[epoch:45, iter:4230] Loss: 0.019 | Acc: 98.958% \n",
      "[epoch:45, iter:4231] Loss: 0.024 | Acc: 98.214% \n",
      "[epoch:45, iter:4232] Loss: 0.021 | Acc: 98.438% \n",
      "[epoch:45, iter:4233] Loss: 0.019 | Acc: 98.611% \n",
      "[epoch:45, iter:4234] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:45, iter:4235] Loss: 0.050 | Acc: 97.727% \n",
      "[epoch:45, iter:4236] Loss: 0.049 | Acc: 97.917% \n",
      "[epoch:45, iter:4237] Loss: 0.045 | Acc: 98.077% \n",
      "[epoch:45, iter:4238] Loss: 0.043 | Acc: 98.214% \n",
      "[epoch:45, iter:4239] Loss: 0.040 | Acc: 98.333% \n",
      "[epoch:45, iter:4240] Loss: 0.038 | Acc: 98.438% \n",
      "[epoch:45, iter:4241] Loss: 0.036 | Acc: 98.529% \n",
      "[epoch:45, iter:4242] Loss: 0.037 | Acc: 98.264% \n",
      "[epoch:45, iter:4243] Loss: 0.036 | Acc: 98.355% \n",
      "[epoch:45, iter:4244] Loss: 0.034 | Acc: 98.438% \n",
      "[epoch:45, iter:4245] Loss: 0.035 | Acc: 98.214% \n",
      "[epoch:45, iter:4246] Loss: 0.034 | Acc: 98.295% \n",
      "[epoch:45, iter:4247] Loss: 0.033 | Acc: 98.370% \n",
      "[epoch:45, iter:4248] Loss: 0.035 | Acc: 98.177% \n",
      "[epoch:45, iter:4249] Loss: 0.048 | Acc: 97.750% \n",
      "[epoch:45, iter:4250] Loss: 0.054 | Acc: 97.596% \n",
      "[epoch:45, iter:4251] Loss: 0.057 | Acc: 97.454% \n",
      "[epoch:45, iter:4252] Loss: 0.055 | Acc: 97.545% \n",
      "[epoch:45, iter:4253] Loss: 0.055 | Acc: 97.414% \n",
      "[epoch:45, iter:4254] Loss: 0.053 | Acc: 97.500% \n",
      "[epoch:45, iter:4255] Loss: 0.052 | Acc: 97.581% \n",
      "[epoch:45, iter:4256] Loss: 0.050 | Acc: 97.656% \n",
      "[epoch:45, iter:4257] Loss: 0.049 | Acc: 97.727% \n",
      "[epoch:45, iter:4258] Loss: 0.047 | Acc: 97.794% \n",
      "[epoch:45, iter:4259] Loss: 0.046 | Acc: 97.857% \n",
      "[epoch:45, iter:4260] Loss: 0.046 | Acc: 97.917% \n",
      "[epoch:45, iter:4261] Loss: 0.045 | Acc: 97.973% \n",
      "[epoch:45, iter:4262] Loss: 0.044 | Acc: 98.026% \n",
      "[epoch:45, iter:4263] Loss: 0.043 | Acc: 98.077% \n",
      "[epoch:45, iter:4264] Loss: 0.042 | Acc: 98.125% \n",
      "[epoch:45, iter:4265] Loss: 0.041 | Acc: 98.171% \n",
      "[epoch:45, iter:4266] Loss: 0.041 | Acc: 98.214% \n",
      "[epoch:45, iter:4267] Loss: 0.040 | Acc: 98.256% \n",
      "[epoch:45, iter:4268] Loss: 0.039 | Acc: 98.295% \n",
      "[epoch:45, iter:4269] Loss: 0.039 | Acc: 98.333% \n",
      "[epoch:45, iter:4270] Loss: 0.038 | Acc: 98.370% \n",
      "[epoch:45, iter:4271] Loss: 0.040 | Acc: 98.271% \n",
      "[epoch:45, iter:4272] Loss: 0.040 | Acc: 98.307% \n",
      "[epoch:45, iter:4273] Loss: 0.039 | Acc: 98.342% \n",
      "[epoch:45, iter:4274] Loss: 0.043 | Acc: 98.125% \n",
      "[epoch:45, iter:4275] Loss: 0.043 | Acc: 98.162% \n",
      "[epoch:45, iter:4276] Loss: 0.044 | Acc: 98.077% \n",
      "[epoch:45, iter:4277] Loss: 0.043 | Acc: 98.113% \n",
      "[epoch:45, iter:4278] Loss: 0.042 | Acc: 98.148% \n",
      "[epoch:45, iter:4279] Loss: 0.043 | Acc: 98.068% \n",
      "[epoch:45, iter:4280] Loss: 0.042 | Acc: 98.103% \n",
      "[epoch:45, iter:4281] Loss: 0.042 | Acc: 98.026% \n",
      "[epoch:45, iter:4282] Loss: 0.042 | Acc: 98.060% \n",
      "[epoch:45, iter:4283] Loss: 0.041 | Acc: 98.093% \n",
      "[epoch:45, iter:4284] Loss: 0.041 | Acc: 98.125% \n",
      "[epoch:45, iter:4285] Loss: 0.041 | Acc: 98.156% \n",
      "[epoch:45, iter:4286] Loss: 0.040 | Acc: 98.185% \n",
      "[epoch:45, iter:4287] Loss: 0.040 | Acc: 98.214% \n",
      "[epoch:45, iter:4288] Loss: 0.039 | Acc: 98.242% \n",
      "[epoch:45, iter:4289] Loss: 0.039 | Acc: 98.269% \n",
      "[epoch:45, iter:4290] Loss: 0.039 | Acc: 98.295% \n",
      "[epoch:45, iter:4291] Loss: 0.038 | Acc: 98.321% \n",
      "[epoch:45, iter:4292] Loss: 0.040 | Acc: 98.254% \n",
      "[epoch:45, iter:4293] Loss: 0.039 | Acc: 98.279% \n",
      "[epoch:45, iter:4294] Loss: 0.041 | Acc: 98.125% \n",
      "[epoch:45, iter:4295] Loss: 0.040 | Acc: 98.151% \n",
      "[epoch:45, iter:4296] Loss: 0.040 | Acc: 98.090% \n",
      "[epoch:45, iter:4297] Loss: 0.040 | Acc: 98.116% \n",
      "[epoch:45, iter:4298] Loss: 0.041 | Acc: 98.057% \n",
      "[epoch:45, iter:4299] Loss: 0.040 | Acc: 98.083% \n",
      "[epoch:45, iter:4300] Loss: 0.040 | Acc: 98.109% \n",
      "[epoch:45, iter:4301] Loss: 0.039 | Acc: 98.133% \n",
      "[epoch:45, iter:4302] Loss: 0.039 | Acc: 98.157% \n",
      "[epoch:45, iter:4303] Loss: 0.038 | Acc: 98.180% \n",
      "[epoch:45, iter:4304] Loss: 0.038 | Acc: 98.203% \n",
      "[epoch:45, iter:4305] Loss: 0.038 | Acc: 98.225% \n",
      "[epoch:45, iter:4306] Loss: 0.038 | Acc: 98.247% \n",
      "[epoch:45, iter:4307] Loss: 0.037 | Acc: 98.268% \n",
      "[epoch:45, iter:4308] Loss: 0.038 | Acc: 98.214% \n",
      "[epoch:45, iter:4309] Loss: 0.039 | Acc: 98.162% \n",
      "[epoch:45, iter:4310] Loss: 0.038 | Acc: 98.183% \n",
      "[epoch:45, iter:4311] Loss: 0.038 | Acc: 98.204% \n",
      "[epoch:45, iter:4312] Loss: 0.037 | Acc: 98.224% \n",
      "[epoch:45, iter:4313] Loss: 0.037 | Acc: 98.244% \n",
      "[epoch:45, iter:4314] Loss: 0.038 | Acc: 98.194% \n",
      "[epoch:45, iter:4315] Loss: 0.038 | Acc: 98.214% \n",
      "[epoch:45, iter:4316] Loss: 0.038 | Acc: 98.166% \n",
      "[epoch:45, iter:4317] Loss: 0.038 | Acc: 98.185% \n",
      "[epoch:45, iter:4318] Loss: 0.039 | Acc: 98.138% \n",
      "[epoch:45, iter:4319] Loss: 0.040 | Acc: 98.092% \n",
      "[epoch:45, iter:4320] Loss: 0.039 | Acc: 98.103% \n",
      "Waiting Test!\n",
      "测试分类准确率为：78.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 46\n",
      "[epoch:46, iter:4321] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:46, iter:4322] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:46, iter:4323] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:46, iter:4324] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:46, iter:4325] Loss: 0.021 | Acc: 98.750% \n",
      "[epoch:46, iter:4326] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:46, iter:4327] Loss: 0.015 | Acc: 99.107% \n",
      "[epoch:46, iter:4328] Loss: 0.020 | Acc: 98.438% \n",
      "[epoch:46, iter:4329] Loss: 0.019 | Acc: 98.611% \n",
      "[epoch:46, iter:4330] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:46, iter:4331] Loss: 0.015 | Acc: 98.864% \n",
      "[epoch:46, iter:4332] Loss: 0.018 | Acc: 98.438% \n",
      "[epoch:46, iter:4333] Loss: 0.019 | Acc: 98.558% \n",
      "[epoch:46, iter:4334] Loss: 0.018 | Acc: 98.661% \n",
      "[epoch:46, iter:4335] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:46, iter:4336] Loss: 0.016 | Acc: 98.828% \n",
      "[epoch:46, iter:4337] Loss: 0.016 | Acc: 98.897% \n",
      "[epoch:46, iter:4338] Loss: 0.016 | Acc: 98.958% \n",
      "[epoch:46, iter:4339] Loss: 0.018 | Acc: 98.684% \n",
      "[epoch:46, iter:4340] Loss: 0.019 | Acc: 98.438% \n",
      "[epoch:46, iter:4341] Loss: 0.020 | Acc: 98.512% \n",
      "[epoch:46, iter:4342] Loss: 0.023 | Acc: 98.011% \n",
      "[epoch:46, iter:4343] Loss: 0.024 | Acc: 97.826% \n",
      "[epoch:46, iter:4344] Loss: 0.024 | Acc: 97.917% \n",
      "[epoch:46, iter:4345] Loss: 0.025 | Acc: 98.000% \n",
      "[epoch:46, iter:4346] Loss: 0.024 | Acc: 98.077% \n",
      "[epoch:46, iter:4347] Loss: 0.025 | Acc: 98.148% \n",
      "[epoch:46, iter:4348] Loss: 0.026 | Acc: 98.214% \n",
      "[epoch:46, iter:4349] Loss: 0.025 | Acc: 98.276% \n",
      "[epoch:46, iter:4350] Loss: 0.024 | Acc: 98.333% \n",
      "[epoch:46, iter:4351] Loss: 0.024 | Acc: 98.387% \n",
      "[epoch:46, iter:4352] Loss: 0.023 | Acc: 98.438% \n",
      "[epoch:46, iter:4353] Loss: 0.022 | Acc: 98.485% \n",
      "[epoch:46, iter:4354] Loss: 0.022 | Acc: 98.529% \n",
      "[epoch:46, iter:4355] Loss: 0.021 | Acc: 98.571% \n",
      "[epoch:46, iter:4356] Loss: 0.022 | Acc: 98.611% \n",
      "[epoch:46, iter:4357] Loss: 0.021 | Acc: 98.649% \n",
      "[epoch:46, iter:4358] Loss: 0.021 | Acc: 98.684% \n",
      "[epoch:46, iter:4359] Loss: 0.024 | Acc: 98.558% \n",
      "[epoch:46, iter:4360] Loss: 0.024 | Acc: 98.594% \n",
      "[epoch:46, iter:4361] Loss: 0.023 | Acc: 98.628% \n",
      "[epoch:46, iter:4362] Loss: 0.023 | Acc: 98.661% \n",
      "[epoch:46, iter:4363] Loss: 0.023 | Acc: 98.692% \n",
      "[epoch:46, iter:4364] Loss: 0.023 | Acc: 98.722% \n",
      "[epoch:46, iter:4365] Loss: 0.023 | Acc: 98.750% \n",
      "[epoch:46, iter:4366] Loss: 0.022 | Acc: 98.777% \n",
      "[epoch:46, iter:4367] Loss: 0.027 | Acc: 98.537% \n",
      "[epoch:46, iter:4368] Loss: 0.027 | Acc: 98.568% \n",
      "[epoch:46, iter:4369] Loss: 0.026 | Acc: 98.597% \n",
      "[epoch:46, iter:4370] Loss: 0.026 | Acc: 98.625% \n",
      "[epoch:46, iter:4371] Loss: 0.026 | Acc: 98.652% \n",
      "[epoch:46, iter:4372] Loss: 0.026 | Acc: 98.678% \n",
      "[epoch:46, iter:4373] Loss: 0.025 | Acc: 98.703% \n",
      "[epoch:46, iter:4374] Loss: 0.025 | Acc: 98.727% \n",
      "[epoch:46, iter:4375] Loss: 0.025 | Acc: 98.750% \n",
      "[epoch:46, iter:4376] Loss: 0.025 | Acc: 98.772% \n",
      "[epoch:46, iter:4377] Loss: 0.024 | Acc: 98.794% \n",
      "[epoch:46, iter:4378] Loss: 0.024 | Acc: 98.815% \n",
      "[epoch:46, iter:4379] Loss: 0.024 | Acc: 98.835% \n",
      "[epoch:46, iter:4380] Loss: 0.024 | Acc: 98.750% \n",
      "[epoch:46, iter:4381] Loss: 0.024 | Acc: 98.770% \n",
      "[epoch:46, iter:4382] Loss: 0.023 | Acc: 98.790% \n",
      "[epoch:46, iter:4383] Loss: 0.024 | Acc: 98.710% \n",
      "[epoch:46, iter:4384] Loss: 0.024 | Acc: 98.730% \n",
      "[epoch:46, iter:4385] Loss: 0.024 | Acc: 98.654% \n",
      "[epoch:46, iter:4386] Loss: 0.024 | Acc: 98.674% \n",
      "[epoch:46, iter:4387] Loss: 0.023 | Acc: 98.694% \n",
      "[epoch:46, iter:4388] Loss: 0.023 | Acc: 98.713% \n",
      "[epoch:46, iter:4389] Loss: 0.025 | Acc: 98.641% \n",
      "[epoch:46, iter:4390] Loss: 0.026 | Acc: 98.482% \n",
      "[epoch:46, iter:4391] Loss: 0.026 | Acc: 98.504% \n",
      "[epoch:46, iter:4392] Loss: 0.026 | Acc: 98.524% \n",
      "[epoch:46, iter:4393] Loss: 0.025 | Acc: 98.545% \n",
      "[epoch:46, iter:4394] Loss: 0.026 | Acc: 98.480% \n",
      "[epoch:46, iter:4395] Loss: 0.025 | Acc: 98.500% \n",
      "[epoch:46, iter:4396] Loss: 0.025 | Acc: 98.520% \n",
      "[epoch:46, iter:4397] Loss: 0.025 | Acc: 98.539% \n",
      "[epoch:46, iter:4398] Loss: 0.025 | Acc: 98.558% \n",
      "[epoch:46, iter:4399] Loss: 0.026 | Acc: 98.497% \n",
      "[epoch:46, iter:4400] Loss: 0.026 | Acc: 98.516% \n",
      "[epoch:46, iter:4401] Loss: 0.028 | Acc: 98.380% \n",
      "[epoch:46, iter:4402] Loss: 0.028 | Acc: 98.399% \n",
      "[epoch:46, iter:4403] Loss: 0.028 | Acc: 98.419% \n",
      "[epoch:46, iter:4404] Loss: 0.028 | Acc: 98.438% \n",
      "[epoch:46, iter:4405] Loss: 0.028 | Acc: 98.456% \n",
      "[epoch:46, iter:4406] Loss: 0.029 | Acc: 98.401% \n",
      "[epoch:46, iter:4407] Loss: 0.033 | Acc: 98.204% \n",
      "[epoch:46, iter:4408] Loss: 0.033 | Acc: 98.224% \n",
      "[epoch:46, iter:4409] Loss: 0.033 | Acc: 98.174% \n",
      "[epoch:46, iter:4410] Loss: 0.033 | Acc: 98.194% \n",
      "[epoch:46, iter:4411] Loss: 0.034 | Acc: 98.146% \n",
      "[epoch:46, iter:4412] Loss: 0.033 | Acc: 98.166% \n",
      "[epoch:46, iter:4413] Loss: 0.033 | Acc: 98.185% \n",
      "[epoch:46, iter:4414] Loss: 0.033 | Acc: 98.205% \n",
      "[epoch:46, iter:4415] Loss: 0.033 | Acc: 98.224% \n",
      "[epoch:46, iter:4416] Loss: 0.034 | Acc: 98.234% \n",
      "Waiting Test!\n",
      "测试分类准确率为：84.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 47\n",
      "[epoch:47, iter:4417] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:47, iter:4418] Loss: 0.019 | Acc: 100.000% \n",
      "[epoch:47, iter:4419] Loss: 0.013 | Acc: 100.000% \n",
      "[epoch:47, iter:4420] Loss: 0.011 | Acc: 100.000% \n",
      "[epoch:47, iter:4421] Loss: 0.020 | Acc: 98.750% \n",
      "[epoch:47, iter:4422] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:47, iter:4423] Loss: 0.016 | Acc: 99.107% \n",
      "[epoch:47, iter:4424] Loss: 0.019 | Acc: 99.219% \n",
      "[epoch:47, iter:4425] Loss: 0.020 | Acc: 99.306% \n",
      "[epoch:47, iter:4426] Loss: 0.018 | Acc: 99.375% \n",
      "[epoch:47, iter:4427] Loss: 0.019 | Acc: 99.432% \n",
      "[epoch:47, iter:4428] Loss: 0.020 | Acc: 99.479% \n",
      "[epoch:47, iter:4429] Loss: 0.022 | Acc: 99.038% \n",
      "[epoch:47, iter:4430] Loss: 0.025 | Acc: 98.661% \n",
      "[epoch:47, iter:4431] Loss: 0.024 | Acc: 98.750% \n",
      "[epoch:47, iter:4432] Loss: 0.023 | Acc: 98.828% \n",
      "[epoch:47, iter:4433] Loss: 0.021 | Acc: 98.897% \n",
      "[epoch:47, iter:4434] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:47, iter:4435] Loss: 0.019 | Acc: 99.013% \n",
      "[epoch:47, iter:4436] Loss: 0.019 | Acc: 99.062% \n",
      "[epoch:47, iter:4437] Loss: 0.019 | Acc: 99.107% \n",
      "[epoch:47, iter:4438] Loss: 0.021 | Acc: 99.148% \n",
      "[epoch:47, iter:4439] Loss: 0.022 | Acc: 98.913% \n",
      "[epoch:47, iter:4440] Loss: 0.021 | Acc: 98.958% \n",
      "[epoch:47, iter:4441] Loss: 0.021 | Acc: 99.000% \n",
      "[epoch:47, iter:4442] Loss: 0.020 | Acc: 99.038% \n",
      "[epoch:47, iter:4443] Loss: 0.019 | Acc: 99.074% \n",
      "[epoch:47, iter:4444] Loss: 0.019 | Acc: 99.107% \n",
      "[epoch:47, iter:4445] Loss: 0.018 | Acc: 99.138% \n",
      "[epoch:47, iter:4446] Loss: 0.018 | Acc: 99.167% \n",
      "[epoch:47, iter:4447] Loss: 0.018 | Acc: 99.194% \n",
      "[epoch:47, iter:4448] Loss: 0.018 | Acc: 99.219% \n",
      "[epoch:47, iter:4449] Loss: 0.019 | Acc: 99.053% \n",
      "[epoch:47, iter:4450] Loss: 0.021 | Acc: 98.897% \n",
      "[epoch:47, iter:4451] Loss: 0.020 | Acc: 98.929% \n",
      "[epoch:47, iter:4452] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:47, iter:4453] Loss: 0.019 | Acc: 98.986% \n",
      "[epoch:47, iter:4454] Loss: 0.019 | Acc: 99.013% \n",
      "[epoch:47, iter:4455] Loss: 0.020 | Acc: 99.038% \n",
      "[epoch:47, iter:4456] Loss: 0.020 | Acc: 99.062% \n",
      "[epoch:47, iter:4457] Loss: 0.020 | Acc: 99.085% \n",
      "[epoch:47, iter:4458] Loss: 0.020 | Acc: 99.107% \n",
      "[epoch:47, iter:4459] Loss: 0.021 | Acc: 98.983% \n",
      "[epoch:47, iter:4460] Loss: 0.020 | Acc: 99.006% \n",
      "[epoch:47, iter:4461] Loss: 0.023 | Acc: 98.889% \n",
      "[epoch:47, iter:4462] Loss: 0.024 | Acc: 98.777% \n",
      "[epoch:47, iter:4463] Loss: 0.024 | Acc: 98.803% \n",
      "[epoch:47, iter:4464] Loss: 0.023 | Acc: 98.828% \n",
      "[epoch:47, iter:4465] Loss: 0.023 | Acc: 98.852% \n",
      "[epoch:47, iter:4466] Loss: 0.022 | Acc: 98.875% \n",
      "[epoch:47, iter:4467] Loss: 0.022 | Acc: 98.897% \n",
      "[epoch:47, iter:4468] Loss: 0.022 | Acc: 98.918% \n",
      "[epoch:47, iter:4469] Loss: 0.023 | Acc: 98.821% \n",
      "[epoch:47, iter:4470] Loss: 0.024 | Acc: 98.727% \n",
      "[epoch:47, iter:4471] Loss: 0.024 | Acc: 98.750% \n",
      "[epoch:47, iter:4472] Loss: 0.024 | Acc: 98.661% \n",
      "[epoch:47, iter:4473] Loss: 0.034 | Acc: 98.136% \n",
      "[epoch:47, iter:4474] Loss: 0.034 | Acc: 98.168% \n",
      "[epoch:47, iter:4475] Loss: 0.033 | Acc: 98.199% \n",
      "[epoch:47, iter:4476] Loss: 0.034 | Acc: 98.125% \n",
      "[epoch:47, iter:4477] Loss: 0.034 | Acc: 98.156% \n",
      "[epoch:47, iter:4478] Loss: 0.033 | Acc: 98.185% \n",
      "[epoch:47, iter:4479] Loss: 0.033 | Acc: 98.214% \n",
      "[epoch:47, iter:4480] Loss: 0.033 | Acc: 98.242% \n",
      "[epoch:47, iter:4481] Loss: 0.033 | Acc: 98.269% \n",
      "[epoch:47, iter:4482] Loss: 0.033 | Acc: 98.295% \n",
      "[epoch:47, iter:4483] Loss: 0.033 | Acc: 98.321% \n",
      "[epoch:47, iter:4484] Loss: 0.035 | Acc: 98.162% \n",
      "[epoch:47, iter:4485] Loss: 0.039 | Acc: 98.098% \n",
      "[epoch:47, iter:4486] Loss: 0.039 | Acc: 98.036% \n",
      "[epoch:47, iter:4487] Loss: 0.039 | Acc: 98.063% \n",
      "[epoch:47, iter:4488] Loss: 0.038 | Acc: 98.090% \n",
      "[epoch:47, iter:4489] Loss: 0.038 | Acc: 98.116% \n",
      "[epoch:47, iter:4490] Loss: 0.039 | Acc: 98.142% \n",
      "[epoch:47, iter:4491] Loss: 0.039 | Acc: 98.167% \n",
      "[epoch:47, iter:4492] Loss: 0.038 | Acc: 98.191% \n",
      "[epoch:47, iter:4493] Loss: 0.038 | Acc: 98.214% \n",
      "[epoch:47, iter:4494] Loss: 0.039 | Acc: 98.157% \n",
      "[epoch:47, iter:4495] Loss: 0.038 | Acc: 98.180% \n",
      "[epoch:47, iter:4496] Loss: 0.039 | Acc: 98.125% \n",
      "[epoch:47, iter:4497] Loss: 0.039 | Acc: 98.148% \n",
      "[epoch:47, iter:4498] Loss: 0.038 | Acc: 98.171% \n",
      "[epoch:47, iter:4499] Loss: 0.038 | Acc: 98.193% \n",
      "[epoch:47, iter:4500] Loss: 0.040 | Acc: 98.065% \n",
      "[epoch:47, iter:4501] Loss: 0.040 | Acc: 98.088% \n",
      "[epoch:47, iter:4502] Loss: 0.040 | Acc: 98.110% \n",
      "[epoch:47, iter:4503] Loss: 0.043 | Acc: 97.989% \n",
      "[epoch:47, iter:4504] Loss: 0.043 | Acc: 97.940% \n",
      "[epoch:47, iter:4505] Loss: 0.043 | Acc: 97.963% \n",
      "[epoch:47, iter:4506] Loss: 0.043 | Acc: 97.917% \n",
      "[epoch:47, iter:4507] Loss: 0.043 | Acc: 97.871% \n",
      "[epoch:47, iter:4508] Loss: 0.043 | Acc: 97.894% \n",
      "[epoch:47, iter:4509] Loss: 0.043 | Acc: 97.917% \n",
      "[epoch:47, iter:4510] Loss: 0.043 | Acc: 97.939% \n",
      "[epoch:47, iter:4511] Loss: 0.043 | Acc: 97.961% \n",
      "[epoch:47, iter:4512] Loss: 0.043 | Acc: 97.973% \n",
      "Waiting Test!\n",
      "测试分类准确率为：77.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 48\n",
      "[epoch:48, iter:4513] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:48, iter:4514] Loss: 0.018 | Acc: 100.000% \n",
      "[epoch:48, iter:4515] Loss: 0.022 | Acc: 100.000% \n",
      "[epoch:48, iter:4516] Loss: 0.020 | Acc: 100.000% \n",
      "[epoch:48, iter:4517] Loss: 0.016 | Acc: 100.000% \n",
      "[epoch:48, iter:4518] Loss: 0.014 | Acc: 100.000% \n",
      "[epoch:48, iter:4519] Loss: 0.031 | Acc: 99.107% \n",
      "[epoch:48, iter:4520] Loss: 0.028 | Acc: 99.219% \n",
      "[epoch:48, iter:4521] Loss: 0.027 | Acc: 99.306% \n",
      "[epoch:48, iter:4522] Loss: 0.027 | Acc: 99.375% \n",
      "[epoch:48, iter:4523] Loss: 0.026 | Acc: 99.432% \n",
      "[epoch:48, iter:4524] Loss: 0.024 | Acc: 99.479% \n",
      "[epoch:48, iter:4525] Loss: 0.027 | Acc: 99.038% \n",
      "[epoch:48, iter:4526] Loss: 0.025 | Acc: 99.107% \n",
      "[epoch:48, iter:4527] Loss: 0.024 | Acc: 99.167% \n",
      "[epoch:48, iter:4528] Loss: 0.026 | Acc: 98.828% \n",
      "[epoch:48, iter:4529] Loss: 0.025 | Acc: 98.897% \n",
      "[epoch:48, iter:4530] Loss: 0.024 | Acc: 98.958% \n",
      "[epoch:48, iter:4531] Loss: 0.022 | Acc: 99.013% \n",
      "[epoch:48, iter:4532] Loss: 0.021 | Acc: 99.062% \n",
      "[epoch:48, iter:4533] Loss: 0.023 | Acc: 99.107% \n",
      "[epoch:48, iter:4534] Loss: 0.022 | Acc: 99.148% \n",
      "[epoch:48, iter:4535] Loss: 0.021 | Acc: 99.185% \n",
      "[epoch:48, iter:4536] Loss: 0.020 | Acc: 99.219% \n",
      "[epoch:48, iter:4537] Loss: 0.019 | Acc: 99.250% \n",
      "[epoch:48, iter:4538] Loss: 0.020 | Acc: 99.279% \n",
      "[epoch:48, iter:4539] Loss: 0.020 | Acc: 99.306% \n",
      "[epoch:48, iter:4540] Loss: 0.029 | Acc: 99.107% \n",
      "[epoch:48, iter:4541] Loss: 0.028 | Acc: 99.138% \n",
      "[epoch:48, iter:4542] Loss: 0.028 | Acc: 99.167% \n",
      "[epoch:48, iter:4543] Loss: 0.029 | Acc: 98.992% \n",
      "[epoch:48, iter:4544] Loss: 0.030 | Acc: 98.828% \n",
      "[epoch:48, iter:4545] Loss: 0.029 | Acc: 98.864% \n",
      "[epoch:48, iter:4546] Loss: 0.033 | Acc: 98.713% \n",
      "[epoch:48, iter:4547] Loss: 0.032 | Acc: 98.750% \n",
      "[epoch:48, iter:4548] Loss: 0.033 | Acc: 98.785% \n",
      "[epoch:48, iter:4549] Loss: 0.032 | Acc: 98.818% \n",
      "[epoch:48, iter:4550] Loss: 0.032 | Acc: 98.684% \n",
      "[epoch:48, iter:4551] Loss: 0.032 | Acc: 98.718% \n",
      "[epoch:48, iter:4552] Loss: 0.032 | Acc: 98.750% \n",
      "[epoch:48, iter:4553] Loss: 0.033 | Acc: 98.628% \n",
      "[epoch:48, iter:4554] Loss: 0.032 | Acc: 98.661% \n",
      "[epoch:48, iter:4555] Loss: 0.033 | Acc: 98.692% \n",
      "[epoch:48, iter:4556] Loss: 0.032 | Acc: 98.722% \n",
      "[epoch:48, iter:4557] Loss: 0.032 | Acc: 98.750% \n",
      "[epoch:48, iter:4558] Loss: 0.032 | Acc: 98.777% \n",
      "[epoch:48, iter:4559] Loss: 0.032 | Acc: 98.803% \n",
      "[epoch:48, iter:4560] Loss: 0.033 | Acc: 98.698% \n",
      "[epoch:48, iter:4561] Loss: 0.033 | Acc: 98.724% \n",
      "[epoch:48, iter:4562] Loss: 0.033 | Acc: 98.750% \n",
      "[epoch:48, iter:4563] Loss: 0.032 | Acc: 98.775% \n",
      "[epoch:48, iter:4564] Loss: 0.031 | Acc: 98.798% \n",
      "[epoch:48, iter:4565] Loss: 0.033 | Acc: 98.703% \n",
      "[epoch:48, iter:4566] Loss: 0.032 | Acc: 98.727% \n",
      "[epoch:48, iter:4567] Loss: 0.032 | Acc: 98.750% \n",
      "[epoch:48, iter:4568] Loss: 0.033 | Acc: 98.661% \n",
      "[epoch:48, iter:4569] Loss: 0.032 | Acc: 98.684% \n",
      "[epoch:48, iter:4570] Loss: 0.032 | Acc: 98.707% \n",
      "[epoch:48, iter:4571] Loss: 0.033 | Acc: 98.623% \n",
      "[epoch:48, iter:4572] Loss: 0.035 | Acc: 98.542% \n",
      "[epoch:48, iter:4573] Loss: 0.036 | Acc: 98.463% \n",
      "[epoch:48, iter:4574] Loss: 0.036 | Acc: 98.488% \n",
      "[epoch:48, iter:4575] Loss: 0.035 | Acc: 98.512% \n",
      "[epoch:48, iter:4576] Loss: 0.042 | Acc: 98.340% \n",
      "[epoch:48, iter:4577] Loss: 0.041 | Acc: 98.365% \n",
      "[epoch:48, iter:4578] Loss: 0.041 | Acc: 98.390% \n",
      "[epoch:48, iter:4579] Loss: 0.041 | Acc: 98.414% \n",
      "[epoch:48, iter:4580] Loss: 0.043 | Acc: 98.346% \n",
      "[epoch:48, iter:4581] Loss: 0.043 | Acc: 98.370% \n",
      "[epoch:48, iter:4582] Loss: 0.042 | Acc: 98.393% \n",
      "[epoch:48, iter:4583] Loss: 0.042 | Acc: 98.415% \n",
      "[epoch:48, iter:4584] Loss: 0.044 | Acc: 98.351% \n",
      "[epoch:48, iter:4585] Loss: 0.046 | Acc: 98.288% \n",
      "[epoch:48, iter:4586] Loss: 0.045 | Acc: 98.311% \n",
      "[epoch:48, iter:4587] Loss: 0.045 | Acc: 98.333% \n",
      "[epoch:48, iter:4588] Loss: 0.044 | Acc: 98.355% \n",
      "[epoch:48, iter:4589] Loss: 0.044 | Acc: 98.377% \n",
      "[epoch:48, iter:4590] Loss: 0.045 | Acc: 98.317% \n",
      "[epoch:48, iter:4591] Loss: 0.045 | Acc: 98.339% \n",
      "[epoch:48, iter:4592] Loss: 0.045 | Acc: 98.359% \n",
      "[epoch:48, iter:4593] Loss: 0.044 | Acc: 98.380% \n",
      "[epoch:48, iter:4594] Loss: 0.044 | Acc: 98.399% \n",
      "[epoch:48, iter:4595] Loss: 0.043 | Acc: 98.419% \n",
      "[epoch:48, iter:4596] Loss: 0.044 | Acc: 98.363% \n",
      "[epoch:48, iter:4597] Loss: 0.044 | Acc: 98.382% \n",
      "[epoch:48, iter:4598] Loss: 0.046 | Acc: 98.256% \n",
      "[epoch:48, iter:4599] Loss: 0.045 | Acc: 98.276% \n",
      "[epoch:48, iter:4600] Loss: 0.045 | Acc: 98.295% \n",
      "[epoch:48, iter:4601] Loss: 0.044 | Acc: 98.315% \n",
      "[epoch:48, iter:4602] Loss: 0.044 | Acc: 98.333% \n",
      "[epoch:48, iter:4603] Loss: 0.044 | Acc: 98.352% \n",
      "[epoch:48, iter:4604] Loss: 0.045 | Acc: 98.302% \n",
      "[epoch:48, iter:4605] Loss: 0.045 | Acc: 98.320% \n",
      "[epoch:48, iter:4606] Loss: 0.044 | Acc: 98.338% \n",
      "[epoch:48, iter:4607] Loss: 0.044 | Acc: 98.355% \n",
      "[epoch:48, iter:4608] Loss: 0.043 | Acc: 98.365% \n",
      "Waiting Test!\n",
      "测试分类准确率为：86.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 49\n",
      "[epoch:49, iter:4609] Loss: 0.212 | Acc: 87.500% \n",
      "[epoch:49, iter:4610] Loss: 0.130 | Acc: 93.750% \n",
      "[epoch:49, iter:4611] Loss: 0.087 | Acc: 95.833% \n",
      "[epoch:49, iter:4612] Loss: 0.065 | Acc: 96.875% \n",
      "[epoch:49, iter:4613] Loss: 0.055 | Acc: 97.500% \n",
      "[epoch:49, iter:4614] Loss: 0.096 | Acc: 95.833% \n",
      "[epoch:49, iter:4615] Loss: 0.083 | Acc: 96.429% \n",
      "[epoch:49, iter:4616] Loss: 0.076 | Acc: 96.875% \n",
      "[epoch:49, iter:4617] Loss: 0.068 | Acc: 97.222% \n",
      "[epoch:49, iter:4618] Loss: 0.062 | Acc: 97.500% \n",
      "[epoch:49, iter:4619] Loss: 0.056 | Acc: 97.727% \n",
      "[epoch:49, iter:4620] Loss: 0.064 | Acc: 97.396% \n",
      "[epoch:49, iter:4621] Loss: 0.062 | Acc: 97.596% \n",
      "[epoch:49, iter:4622] Loss: 0.058 | Acc: 97.768% \n",
      "[epoch:49, iter:4623] Loss: 0.054 | Acc: 97.917% \n",
      "[epoch:49, iter:4624] Loss: 0.051 | Acc: 98.047% \n",
      "[epoch:49, iter:4625] Loss: 0.050 | Acc: 98.162% \n",
      "[epoch:49, iter:4626] Loss: 0.049 | Acc: 98.264% \n",
      "[epoch:49, iter:4627] Loss: 0.048 | Acc: 98.355% \n",
      "[epoch:49, iter:4628] Loss: 0.046 | Acc: 98.438% \n",
      "[epoch:49, iter:4629] Loss: 0.045 | Acc: 98.512% \n",
      "[epoch:49, iter:4630] Loss: 0.043 | Acc: 98.580% \n",
      "[epoch:49, iter:4631] Loss: 0.041 | Acc: 98.641% \n",
      "[epoch:49, iter:4632] Loss: 0.040 | Acc: 98.698% \n",
      "[epoch:49, iter:4633] Loss: 0.038 | Acc: 98.750% \n",
      "[epoch:49, iter:4634] Loss: 0.037 | Acc: 98.798% \n",
      "[epoch:49, iter:4635] Loss: 0.037 | Acc: 98.843% \n",
      "[epoch:49, iter:4636] Loss: 0.036 | Acc: 98.884% \n",
      "[epoch:49, iter:4637] Loss: 0.039 | Acc: 98.491% \n",
      "[epoch:49, iter:4638] Loss: 0.039 | Acc: 98.542% \n",
      "[epoch:49, iter:4639] Loss: 0.038 | Acc: 98.589% \n",
      "[epoch:49, iter:4640] Loss: 0.037 | Acc: 98.633% \n",
      "[epoch:49, iter:4641] Loss: 0.037 | Acc: 98.674% \n",
      "[epoch:49, iter:4642] Loss: 0.036 | Acc: 98.713% \n",
      "[epoch:49, iter:4643] Loss: 0.035 | Acc: 98.750% \n",
      "[epoch:49, iter:4644] Loss: 0.034 | Acc: 98.785% \n",
      "[epoch:49, iter:4645] Loss: 0.033 | Acc: 98.818% \n",
      "[epoch:49, iter:4646] Loss: 0.033 | Acc: 98.849% \n",
      "[epoch:49, iter:4647] Loss: 0.032 | Acc: 98.878% \n",
      "[epoch:49, iter:4648] Loss: 0.032 | Acc: 98.906% \n",
      "[epoch:49, iter:4649] Loss: 0.032 | Acc: 98.933% \n",
      "[epoch:49, iter:4650] Loss: 0.031 | Acc: 98.958% \n",
      "[epoch:49, iter:4651] Loss: 0.031 | Acc: 98.983% \n",
      "[epoch:49, iter:4652] Loss: 0.030 | Acc: 99.006% \n",
      "[epoch:49, iter:4653] Loss: 0.030 | Acc: 99.028% \n",
      "[epoch:49, iter:4654] Loss: 0.029 | Acc: 99.049% \n",
      "[epoch:49, iter:4655] Loss: 0.030 | Acc: 98.936% \n",
      "[epoch:49, iter:4656] Loss: 0.029 | Acc: 98.958% \n",
      "[epoch:49, iter:4657] Loss: 0.029 | Acc: 98.980% \n",
      "[epoch:49, iter:4658] Loss: 0.028 | Acc: 99.000% \n",
      "[epoch:49, iter:4659] Loss: 0.028 | Acc: 99.020% \n",
      "[epoch:49, iter:4660] Loss: 0.028 | Acc: 99.038% \n",
      "[epoch:49, iter:4661] Loss: 0.029 | Acc: 99.057% \n",
      "[epoch:49, iter:4662] Loss: 0.033 | Acc: 98.958% \n",
      "[epoch:49, iter:4663] Loss: 0.032 | Acc: 98.977% \n",
      "[epoch:49, iter:4664] Loss: 0.033 | Acc: 98.884% \n",
      "[epoch:49, iter:4665] Loss: 0.033 | Acc: 98.904% \n",
      "[epoch:49, iter:4666] Loss: 0.033 | Acc: 98.922% \n",
      "[epoch:49, iter:4667] Loss: 0.034 | Acc: 98.941% \n",
      "[epoch:49, iter:4668] Loss: 0.033 | Acc: 98.958% \n",
      "[epoch:49, iter:4669] Loss: 0.033 | Acc: 98.975% \n",
      "[epoch:49, iter:4670] Loss: 0.032 | Acc: 98.992% \n",
      "[epoch:49, iter:4671] Loss: 0.032 | Acc: 99.008% \n",
      "[epoch:49, iter:4672] Loss: 0.031 | Acc: 99.023% \n",
      "[epoch:49, iter:4673] Loss: 0.032 | Acc: 98.942% \n",
      "[epoch:49, iter:4674] Loss: 0.032 | Acc: 98.864% \n",
      "[epoch:49, iter:4675] Loss: 0.033 | Acc: 98.787% \n",
      "[epoch:49, iter:4676] Loss: 0.032 | Acc: 98.805% \n",
      "[epoch:49, iter:4677] Loss: 0.032 | Acc: 98.822% \n",
      "[epoch:49, iter:4678] Loss: 0.031 | Acc: 98.839% \n",
      "[epoch:49, iter:4679] Loss: 0.031 | Acc: 98.856% \n",
      "[epoch:49, iter:4680] Loss: 0.032 | Acc: 98.785% \n",
      "[epoch:49, iter:4681] Loss: 0.035 | Acc: 98.545% \n",
      "[epoch:49, iter:4682] Loss: 0.037 | Acc: 98.395% \n",
      "[epoch:49, iter:4683] Loss: 0.037 | Acc: 98.417% \n",
      "[epoch:49, iter:4684] Loss: 0.036 | Acc: 98.438% \n",
      "[epoch:49, iter:4685] Loss: 0.036 | Acc: 98.458% \n",
      "[epoch:49, iter:4686] Loss: 0.036 | Acc: 98.478% \n",
      "[epoch:49, iter:4687] Loss: 0.035 | Acc: 98.497% \n",
      "[epoch:49, iter:4688] Loss: 0.035 | Acc: 98.516% \n",
      "[epoch:49, iter:4689] Loss: 0.036 | Acc: 98.457% \n",
      "[epoch:49, iter:4690] Loss: 0.035 | Acc: 98.476% \n",
      "[epoch:49, iter:4691] Loss: 0.036 | Acc: 98.494% \n",
      "[epoch:49, iter:4692] Loss: 0.035 | Acc: 98.512% \n",
      "[epoch:49, iter:4693] Loss: 0.038 | Acc: 98.456% \n",
      "[epoch:49, iter:4694] Loss: 0.038 | Acc: 98.474% \n",
      "[epoch:49, iter:4695] Loss: 0.039 | Acc: 98.420% \n",
      "[epoch:49, iter:4696] Loss: 0.039 | Acc: 98.366% \n",
      "[epoch:49, iter:4697] Loss: 0.040 | Acc: 98.315% \n",
      "[epoch:49, iter:4698] Loss: 0.041 | Acc: 98.194% \n",
      "[epoch:49, iter:4699] Loss: 0.041 | Acc: 98.214% \n",
      "[epoch:49, iter:4700] Loss: 0.041 | Acc: 98.166% \n",
      "[epoch:49, iter:4701] Loss: 0.040 | Acc: 98.185% \n",
      "[epoch:49, iter:4702] Loss: 0.040 | Acc: 98.205% \n",
      "[epoch:49, iter:4703] Loss: 0.041 | Acc: 98.158% \n",
      "[epoch:49, iter:4704] Loss: 0.040 | Acc: 98.169% \n",
      "Waiting Test!\n",
      "测试分类准确率为：80.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 50\n",
      "[epoch:50, iter:4705] Loss: 0.037 | Acc: 100.000% \n",
      "[epoch:50, iter:4706] Loss: 0.019 | Acc: 100.000% \n",
      "[epoch:50, iter:4707] Loss: 0.021 | Acc: 100.000% \n",
      "[epoch:50, iter:4708] Loss: 0.036 | Acc: 98.438% \n",
      "[epoch:50, iter:4709] Loss: 0.033 | Acc: 98.750% \n",
      "[epoch:50, iter:4710] Loss: 0.038 | Acc: 97.917% \n",
      "[epoch:50, iter:4711] Loss: 0.034 | Acc: 98.214% \n",
      "[epoch:50, iter:4712] Loss: 0.036 | Acc: 97.656% \n",
      "[epoch:50, iter:4713] Loss: 0.033 | Acc: 97.917% \n",
      "[epoch:50, iter:4714] Loss: 0.029 | Acc: 98.125% \n",
      "[epoch:50, iter:4715] Loss: 0.033 | Acc: 97.727% \n",
      "[epoch:50, iter:4716] Loss: 0.031 | Acc: 97.917% \n",
      "[epoch:50, iter:4717] Loss: 0.032 | Acc: 98.077% \n",
      "[epoch:50, iter:4718] Loss: 0.030 | Acc: 98.214% \n",
      "[epoch:50, iter:4719] Loss: 0.028 | Acc: 98.333% \n",
      "[epoch:50, iter:4720] Loss: 0.033 | Acc: 98.047% \n",
      "[epoch:50, iter:4721] Loss: 0.031 | Acc: 98.162% \n",
      "[epoch:50, iter:4722] Loss: 0.039 | Acc: 97.917% \n",
      "[epoch:50, iter:4723] Loss: 0.045 | Acc: 97.697% \n",
      "[epoch:50, iter:4724] Loss: 0.043 | Acc: 97.812% \n",
      "[epoch:50, iter:4725] Loss: 0.041 | Acc: 97.917% \n",
      "[epoch:50, iter:4726] Loss: 0.039 | Acc: 98.011% \n",
      "[epoch:50, iter:4727] Loss: 0.039 | Acc: 98.098% \n",
      "[epoch:50, iter:4728] Loss: 0.040 | Acc: 97.917% \n",
      "[epoch:50, iter:4729] Loss: 0.039 | Acc: 98.000% \n",
      "[epoch:50, iter:4730] Loss: 0.044 | Acc: 97.596% \n",
      "[epoch:50, iter:4731] Loss: 0.044 | Acc: 97.685% \n",
      "[epoch:50, iter:4732] Loss: 0.045 | Acc: 97.545% \n",
      "[epoch:50, iter:4733] Loss: 0.050 | Acc: 97.198% \n",
      "[epoch:50, iter:4734] Loss: 0.050 | Acc: 97.083% \n",
      "[epoch:50, iter:4735] Loss: 0.049 | Acc: 97.177% \n",
      "[epoch:50, iter:4736] Loss: 0.053 | Acc: 96.875% \n",
      "[epoch:50, iter:4737] Loss: 0.052 | Acc: 96.970% \n",
      "[epoch:50, iter:4738] Loss: 0.053 | Acc: 96.875% \n",
      "[epoch:50, iter:4739] Loss: 0.052 | Acc: 96.964% \n",
      "[epoch:50, iter:4740] Loss: 0.051 | Acc: 97.049% \n",
      "[epoch:50, iter:4741] Loss: 0.052 | Acc: 96.959% \n",
      "[epoch:50, iter:4742] Loss: 0.063 | Acc: 96.875% \n",
      "[epoch:50, iter:4743] Loss: 0.062 | Acc: 96.955% \n",
      "[epoch:50, iter:4744] Loss: 0.060 | Acc: 97.031% \n",
      "[epoch:50, iter:4745] Loss: 0.060 | Acc: 97.104% \n",
      "[epoch:50, iter:4746] Loss: 0.062 | Acc: 96.875% \n",
      "[epoch:50, iter:4747] Loss: 0.060 | Acc: 96.948% \n",
      "[epoch:50, iter:4748] Loss: 0.060 | Acc: 96.875% \n",
      "[epoch:50, iter:4749] Loss: 0.062 | Acc: 96.806% \n",
      "[epoch:50, iter:4750] Loss: 0.062 | Acc: 96.875% \n",
      "[epoch:50, iter:4751] Loss: 0.062 | Acc: 96.809% \n",
      "[epoch:50, iter:4752] Loss: 0.061 | Acc: 96.875% \n",
      "[epoch:50, iter:4753] Loss: 0.063 | Acc: 96.811% \n",
      "[epoch:50, iter:4754] Loss: 0.062 | Acc: 96.875% \n",
      "[epoch:50, iter:4755] Loss: 0.063 | Acc: 96.814% \n",
      "[epoch:50, iter:4756] Loss: 0.062 | Acc: 96.875% \n",
      "[epoch:50, iter:4757] Loss: 0.062 | Acc: 96.816% \n",
      "[epoch:50, iter:4758] Loss: 0.061 | Acc: 96.875% \n",
      "[epoch:50, iter:4759] Loss: 0.060 | Acc: 96.932% \n",
      "[epoch:50, iter:4760] Loss: 0.060 | Acc: 96.987% \n",
      "[epoch:50, iter:4761] Loss: 0.061 | Acc: 96.930% \n",
      "[epoch:50, iter:4762] Loss: 0.061 | Acc: 96.875% \n",
      "[epoch:50, iter:4763] Loss: 0.060 | Acc: 96.928% \n",
      "[epoch:50, iter:4764] Loss: 0.059 | Acc: 96.979% \n",
      "[epoch:50, iter:4765] Loss: 0.059 | Acc: 96.926% \n",
      "[epoch:50, iter:4766] Loss: 0.058 | Acc: 96.976% \n",
      "[epoch:50, iter:4767] Loss: 0.058 | Acc: 97.024% \n",
      "[epoch:50, iter:4768] Loss: 0.057 | Acc: 97.070% \n",
      "[epoch:50, iter:4769] Loss: 0.056 | Acc: 97.115% \n",
      "[epoch:50, iter:4770] Loss: 0.056 | Acc: 97.064% \n",
      "[epoch:50, iter:4771] Loss: 0.056 | Acc: 97.108% \n",
      "[epoch:50, iter:4772] Loss: 0.055 | Acc: 97.151% \n",
      "[epoch:50, iter:4773] Loss: 0.055 | Acc: 97.192% \n",
      "[epoch:50, iter:4774] Loss: 0.054 | Acc: 97.232% \n",
      "[epoch:50, iter:4775] Loss: 0.053 | Acc: 97.271% \n",
      "[epoch:50, iter:4776] Loss: 0.053 | Acc: 97.222% \n",
      "[epoch:50, iter:4777] Loss: 0.054 | Acc: 97.175% \n",
      "[epoch:50, iter:4778] Loss: 0.053 | Acc: 97.213% \n",
      "[epoch:50, iter:4779] Loss: 0.053 | Acc: 97.250% \n",
      "[epoch:50, iter:4780] Loss: 0.057 | Acc: 97.039% \n",
      "[epoch:50, iter:4781] Loss: 0.056 | Acc: 97.078% \n",
      "[epoch:50, iter:4782] Loss: 0.056 | Acc: 97.115% \n",
      "[epoch:50, iter:4783] Loss: 0.055 | Acc: 97.152% \n",
      "[epoch:50, iter:4784] Loss: 0.054 | Acc: 97.188% \n",
      "[epoch:50, iter:4785] Loss: 0.054 | Acc: 97.145% \n",
      "[epoch:50, iter:4786] Loss: 0.054 | Acc: 97.104% \n",
      "[epoch:50, iter:4787] Loss: 0.054 | Acc: 97.139% \n",
      "[epoch:50, iter:4788] Loss: 0.053 | Acc: 97.173% \n",
      "[epoch:50, iter:4789] Loss: 0.052 | Acc: 97.206% \n",
      "[epoch:50, iter:4790] Loss: 0.052 | Acc: 97.238% \n",
      "[epoch:50, iter:4791] Loss: 0.051 | Acc: 97.270% \n",
      "[epoch:50, iter:4792] Loss: 0.051 | Acc: 97.301% \n",
      "[epoch:50, iter:4793] Loss: 0.050 | Acc: 97.331% \n",
      "[epoch:50, iter:4794] Loss: 0.050 | Acc: 97.361% \n",
      "[epoch:50, iter:4795] Loss: 0.051 | Acc: 97.321% \n",
      "[epoch:50, iter:4796] Loss: 0.050 | Acc: 97.351% \n",
      "[epoch:50, iter:4797] Loss: 0.050 | Acc: 97.379% \n",
      "[epoch:50, iter:4798] Loss: 0.050 | Acc: 97.407% \n",
      "[epoch:50, iter:4799] Loss: 0.049 | Acc: 97.434% \n",
      "[epoch:50, iter:4800] Loss: 0.052 | Acc: 97.384% \n",
      "Waiting Test!\n",
      "测试分类准确率为：78.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 51\n",
      "[epoch:51, iter:4801] Loss: 0.007 | Acc: 100.000% \n",
      "[epoch:51, iter:4802] Loss: 0.028 | Acc: 96.875% \n",
      "[epoch:51, iter:4803] Loss: 0.019 | Acc: 97.917% \n",
      "[epoch:51, iter:4804] Loss: 0.055 | Acc: 95.312% \n",
      "[epoch:51, iter:4805] Loss: 0.049 | Acc: 96.250% \n",
      "[epoch:51, iter:4806] Loss: 0.041 | Acc: 96.875% \n",
      "[epoch:51, iter:4807] Loss: 0.036 | Acc: 97.321% \n",
      "[epoch:51, iter:4808] Loss: 0.031 | Acc: 97.656% \n",
      "[epoch:51, iter:4809] Loss: 0.034 | Acc: 97.222% \n",
      "[epoch:51, iter:4810] Loss: 0.036 | Acc: 97.500% \n",
      "[epoch:51, iter:4811] Loss: 0.033 | Acc: 97.727% \n",
      "[epoch:51, iter:4812] Loss: 0.032 | Acc: 97.917% \n",
      "[epoch:51, iter:4813] Loss: 0.030 | Acc: 98.077% \n",
      "[epoch:51, iter:4814] Loss: 0.031 | Acc: 97.768% \n",
      "[epoch:51, iter:4815] Loss: 0.029 | Acc: 97.917% \n",
      "[epoch:51, iter:4816] Loss: 0.032 | Acc: 98.047% \n",
      "[epoch:51, iter:4817] Loss: 0.033 | Acc: 98.162% \n",
      "[epoch:51, iter:4818] Loss: 0.038 | Acc: 97.917% \n",
      "[epoch:51, iter:4819] Loss: 0.036 | Acc: 98.026% \n",
      "[epoch:51, iter:4820] Loss: 0.034 | Acc: 98.125% \n",
      "[epoch:51, iter:4821] Loss: 0.037 | Acc: 97.917% \n",
      "[epoch:51, iter:4822] Loss: 0.036 | Acc: 98.011% \n",
      "[epoch:51, iter:4823] Loss: 0.035 | Acc: 98.098% \n",
      "[epoch:51, iter:4824] Loss: 0.035 | Acc: 98.177% \n",
      "[epoch:51, iter:4825] Loss: 0.034 | Acc: 98.250% \n",
      "[epoch:51, iter:4826] Loss: 0.036 | Acc: 98.077% \n",
      "[epoch:51, iter:4827] Loss: 0.035 | Acc: 98.148% \n",
      "[epoch:51, iter:4828] Loss: 0.034 | Acc: 98.214% \n",
      "[epoch:51, iter:4829] Loss: 0.033 | Acc: 98.276% \n",
      "[epoch:51, iter:4830] Loss: 0.032 | Acc: 98.333% \n",
      "[epoch:51, iter:4831] Loss: 0.031 | Acc: 98.387% \n",
      "[epoch:51, iter:4832] Loss: 0.030 | Acc: 98.438% \n",
      "[epoch:51, iter:4833] Loss: 0.029 | Acc: 98.485% \n",
      "[epoch:51, iter:4834] Loss: 0.029 | Acc: 98.529% \n",
      "[epoch:51, iter:4835] Loss: 0.028 | Acc: 98.571% \n",
      "[epoch:51, iter:4836] Loss: 0.028 | Acc: 98.611% \n",
      "[epoch:51, iter:4837] Loss: 0.032 | Acc: 98.480% \n",
      "[epoch:51, iter:4838] Loss: 0.031 | Acc: 98.520% \n",
      "[epoch:51, iter:4839] Loss: 0.030 | Acc: 98.558% \n",
      "[epoch:51, iter:4840] Loss: 0.030 | Acc: 98.594% \n",
      "[epoch:51, iter:4841] Loss: 0.029 | Acc: 98.628% \n",
      "[epoch:51, iter:4842] Loss: 0.028 | Acc: 98.661% \n",
      "[epoch:51, iter:4843] Loss: 0.028 | Acc: 98.692% \n",
      "[epoch:51, iter:4844] Loss: 0.029 | Acc: 98.580% \n",
      "[epoch:51, iter:4845] Loss: 0.029 | Acc: 98.611% \n",
      "[epoch:51, iter:4846] Loss: 0.030 | Acc: 98.641% \n",
      "[epoch:51, iter:4847] Loss: 0.031 | Acc: 98.670% \n",
      "[epoch:51, iter:4848] Loss: 0.030 | Acc: 98.698% \n",
      "[epoch:51, iter:4849] Loss: 0.030 | Acc: 98.724% \n",
      "[epoch:51, iter:4850] Loss: 0.029 | Acc: 98.750% \n",
      "[epoch:51, iter:4851] Loss: 0.030 | Acc: 98.652% \n",
      "[epoch:51, iter:4852] Loss: 0.030 | Acc: 98.678% \n",
      "[epoch:51, iter:4853] Loss: 0.029 | Acc: 98.703% \n",
      "[epoch:51, iter:4854] Loss: 0.029 | Acc: 98.727% \n",
      "[epoch:51, iter:4855] Loss: 0.029 | Acc: 98.636% \n",
      "[epoch:51, iter:4856] Loss: 0.028 | Acc: 98.661% \n",
      "[epoch:51, iter:4857] Loss: 0.029 | Acc: 98.684% \n",
      "[epoch:51, iter:4858] Loss: 0.029 | Acc: 98.707% \n",
      "[epoch:51, iter:4859] Loss: 0.028 | Acc: 98.729% \n",
      "[epoch:51, iter:4860] Loss: 0.028 | Acc: 98.750% \n",
      "[epoch:51, iter:4861] Loss: 0.027 | Acc: 98.770% \n",
      "[epoch:51, iter:4862] Loss: 0.027 | Acc: 98.790% \n",
      "[epoch:51, iter:4863] Loss: 0.026 | Acc: 98.810% \n",
      "[epoch:51, iter:4864] Loss: 0.026 | Acc: 98.828% \n",
      "[epoch:51, iter:4865] Loss: 0.026 | Acc: 98.846% \n",
      "[epoch:51, iter:4866] Loss: 0.026 | Acc: 98.864% \n",
      "[epoch:51, iter:4867] Loss: 0.025 | Acc: 98.881% \n",
      "[epoch:51, iter:4868] Loss: 0.025 | Acc: 98.897% \n",
      "[epoch:51, iter:4869] Loss: 0.025 | Acc: 98.913% \n",
      "[epoch:51, iter:4870] Loss: 0.024 | Acc: 98.929% \n",
      "[epoch:51, iter:4871] Loss: 0.030 | Acc: 98.680% \n",
      "[epoch:51, iter:4872] Loss: 0.030 | Acc: 98.698% \n",
      "[epoch:51, iter:4873] Loss: 0.030 | Acc: 98.716% \n",
      "[epoch:51, iter:4874] Loss: 0.030 | Acc: 98.733% \n",
      "[epoch:51, iter:4875] Loss: 0.031 | Acc: 98.583% \n",
      "[epoch:51, iter:4876] Loss: 0.031 | Acc: 98.602% \n",
      "[epoch:51, iter:4877] Loss: 0.031 | Acc: 98.620% \n",
      "[epoch:51, iter:4878] Loss: 0.030 | Acc: 98.638% \n",
      "[epoch:51, iter:4879] Loss: 0.031 | Acc: 98.655% \n",
      "[epoch:51, iter:4880] Loss: 0.032 | Acc: 98.516% \n",
      "[epoch:51, iter:4881] Loss: 0.032 | Acc: 98.534% \n",
      "[epoch:51, iter:4882] Loss: 0.032 | Acc: 98.476% \n",
      "[epoch:51, iter:4883] Loss: 0.033 | Acc: 98.494% \n",
      "[epoch:51, iter:4884] Loss: 0.032 | Acc: 98.512% \n",
      "[epoch:51, iter:4885] Loss: 0.032 | Acc: 98.529% \n",
      "[epoch:51, iter:4886] Loss: 0.033 | Acc: 98.474% \n",
      "[epoch:51, iter:4887] Loss: 0.032 | Acc: 98.491% \n",
      "[epoch:51, iter:4888] Loss: 0.032 | Acc: 98.509% \n",
      "[epoch:51, iter:4889] Loss: 0.032 | Acc: 98.525% \n",
      "[epoch:51, iter:4890] Loss: 0.031 | Acc: 98.542% \n",
      "[epoch:51, iter:4891] Loss: 0.031 | Acc: 98.558% \n",
      "[epoch:51, iter:4892] Loss: 0.031 | Acc: 98.573% \n",
      "[epoch:51, iter:4893] Loss: 0.031 | Acc: 98.589% \n",
      "[epoch:51, iter:4894] Loss: 0.031 | Acc: 98.604% \n",
      "[epoch:51, iter:4895] Loss: 0.032 | Acc: 98.487% \n",
      "[epoch:51, iter:4896] Loss: 0.032 | Acc: 98.496% \n",
      "Waiting Test!\n",
      "测试分类准确率为：73.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 52\n",
      "[epoch:52, iter:4897] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:52, iter:4898] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:52, iter:4899] Loss: 0.003 | Acc: 100.000% \n",
      "[epoch:52, iter:4900] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:52, iter:4901] Loss: 0.016 | Acc: 100.000% \n",
      "[epoch:52, iter:4902] Loss: 0.013 | Acc: 100.000% \n",
      "[epoch:52, iter:4903] Loss: 0.012 | Acc: 100.000% \n",
      "[epoch:52, iter:4904] Loss: 0.010 | Acc: 100.000% \n",
      "[epoch:52, iter:4905] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:52, iter:4906] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:52, iter:4907] Loss: 0.008 | Acc: 100.000% \n",
      "[epoch:52, iter:4908] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:52, iter:4909] Loss: 0.008 | Acc: 100.000% \n",
      "[epoch:52, iter:4910] Loss: 0.008 | Acc: 100.000% \n",
      "[epoch:52, iter:4911] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:52, iter:4912] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:52, iter:4913] Loss: 0.008 | Acc: 100.000% \n",
      "[epoch:52, iter:4914] Loss: 0.012 | Acc: 99.653% \n",
      "[epoch:52, iter:4915] Loss: 0.011 | Acc: 99.671% \n",
      "[epoch:52, iter:4916] Loss: 0.011 | Acc: 99.688% \n",
      "[epoch:52, iter:4917] Loss: 0.011 | Acc: 99.702% \n",
      "[epoch:52, iter:4918] Loss: 0.030 | Acc: 98.864% \n",
      "[epoch:52, iter:4919] Loss: 0.031 | Acc: 98.641% \n",
      "[epoch:52, iter:4920] Loss: 0.029 | Acc: 98.698% \n",
      "[epoch:52, iter:4921] Loss: 0.028 | Acc: 98.750% \n",
      "[epoch:52, iter:4922] Loss: 0.028 | Acc: 98.798% \n",
      "[epoch:52, iter:4923] Loss: 0.032 | Acc: 98.380% \n",
      "[epoch:52, iter:4924] Loss: 0.032 | Acc: 98.438% \n",
      "[epoch:52, iter:4925] Loss: 0.032 | Acc: 98.491% \n",
      "[epoch:52, iter:4926] Loss: 0.033 | Acc: 98.542% \n",
      "[epoch:52, iter:4927] Loss: 0.034 | Acc: 98.589% \n",
      "[epoch:52, iter:4928] Loss: 0.033 | Acc: 98.633% \n",
      "[epoch:52, iter:4929] Loss: 0.033 | Acc: 98.674% \n",
      "[epoch:52, iter:4930] Loss: 0.032 | Acc: 98.713% \n",
      "[epoch:52, iter:4931] Loss: 0.031 | Acc: 98.750% \n",
      "[epoch:52, iter:4932] Loss: 0.032 | Acc: 98.785% \n",
      "[epoch:52, iter:4933] Loss: 0.031 | Acc: 98.818% \n",
      "[epoch:52, iter:4934] Loss: 0.031 | Acc: 98.849% \n",
      "[epoch:52, iter:4935] Loss: 0.030 | Acc: 98.878% \n",
      "[epoch:52, iter:4936] Loss: 0.030 | Acc: 98.906% \n",
      "[epoch:52, iter:4937] Loss: 0.029 | Acc: 98.933% \n",
      "[epoch:52, iter:4938] Loss: 0.029 | Acc: 98.958% \n",
      "[epoch:52, iter:4939] Loss: 0.029 | Acc: 98.983% \n",
      "[epoch:52, iter:4940] Loss: 0.029 | Acc: 99.006% \n",
      "[epoch:52, iter:4941] Loss: 0.030 | Acc: 98.889% \n",
      "[epoch:52, iter:4942] Loss: 0.031 | Acc: 98.913% \n",
      "[epoch:52, iter:4943] Loss: 0.030 | Acc: 98.936% \n",
      "[epoch:52, iter:4944] Loss: 0.030 | Acc: 98.958% \n",
      "[epoch:52, iter:4945] Loss: 0.039 | Acc: 98.852% \n",
      "[epoch:52, iter:4946] Loss: 0.039 | Acc: 98.875% \n",
      "[epoch:52, iter:4947] Loss: 0.040 | Acc: 98.775% \n",
      "[epoch:52, iter:4948] Loss: 0.039 | Acc: 98.798% \n",
      "[epoch:52, iter:4949] Loss: 0.040 | Acc: 98.821% \n",
      "[epoch:52, iter:4950] Loss: 0.040 | Acc: 98.843% \n",
      "[epoch:52, iter:4951] Loss: 0.041 | Acc: 98.864% \n",
      "[epoch:52, iter:4952] Loss: 0.040 | Acc: 98.884% \n",
      "[epoch:52, iter:4953] Loss: 0.039 | Acc: 98.904% \n",
      "[epoch:52, iter:4954] Loss: 0.038 | Acc: 98.922% \n",
      "[epoch:52, iter:4955] Loss: 0.038 | Acc: 98.941% \n",
      "[epoch:52, iter:4956] Loss: 0.041 | Acc: 98.750% \n",
      "[epoch:52, iter:4957] Loss: 0.040 | Acc: 98.770% \n",
      "[epoch:52, iter:4958] Loss: 0.040 | Acc: 98.790% \n",
      "[epoch:52, iter:4959] Loss: 0.041 | Acc: 98.710% \n",
      "[epoch:52, iter:4960] Loss: 0.040 | Acc: 98.730% \n",
      "[epoch:52, iter:4961] Loss: 0.040 | Acc: 98.750% \n",
      "[epoch:52, iter:4962] Loss: 0.039 | Acc: 98.769% \n",
      "[epoch:52, iter:4963] Loss: 0.042 | Acc: 98.507% \n",
      "[epoch:52, iter:4964] Loss: 0.041 | Acc: 98.529% \n",
      "[epoch:52, iter:4965] Loss: 0.040 | Acc: 98.551% \n",
      "[epoch:52, iter:4966] Loss: 0.041 | Acc: 98.482% \n",
      "[epoch:52, iter:4967] Loss: 0.041 | Acc: 98.504% \n",
      "[epoch:52, iter:4968] Loss: 0.042 | Acc: 98.438% \n",
      "[epoch:52, iter:4969] Loss: 0.043 | Acc: 98.373% \n",
      "[epoch:52, iter:4970] Loss: 0.042 | Acc: 98.395% \n",
      "[epoch:52, iter:4971] Loss: 0.042 | Acc: 98.333% \n",
      "[epoch:52, iter:4972] Loss: 0.042 | Acc: 98.355% \n",
      "[epoch:52, iter:4973] Loss: 0.041 | Acc: 98.377% \n",
      "[epoch:52, iter:4974] Loss: 0.042 | Acc: 98.317% \n",
      "[epoch:52, iter:4975] Loss: 0.041 | Acc: 98.339% \n",
      "[epoch:52, iter:4976] Loss: 0.043 | Acc: 98.203% \n",
      "[epoch:52, iter:4977] Loss: 0.042 | Acc: 98.225% \n",
      "[epoch:52, iter:4978] Loss: 0.042 | Acc: 98.247% \n",
      "[epoch:52, iter:4979] Loss: 0.041 | Acc: 98.268% \n",
      "[epoch:52, iter:4980] Loss: 0.041 | Acc: 98.289% \n",
      "[epoch:52, iter:4981] Loss: 0.041 | Acc: 98.309% \n",
      "[epoch:52, iter:4982] Loss: 0.041 | Acc: 98.256% \n",
      "[epoch:52, iter:4983] Loss: 0.043 | Acc: 98.204% \n",
      "[epoch:52, iter:4984] Loss: 0.043 | Acc: 98.224% \n",
      "[epoch:52, iter:4985] Loss: 0.043 | Acc: 98.244% \n",
      "[epoch:52, iter:4986] Loss: 0.042 | Acc: 98.264% \n",
      "[epoch:52, iter:4987] Loss: 0.042 | Acc: 98.283% \n",
      "[epoch:52, iter:4988] Loss: 0.042 | Acc: 98.302% \n",
      "[epoch:52, iter:4989] Loss: 0.041 | Acc: 98.320% \n",
      "[epoch:52, iter:4990] Loss: 0.042 | Acc: 98.205% \n",
      "[epoch:52, iter:4991] Loss: 0.042 | Acc: 98.224% \n",
      "[epoch:52, iter:4992] Loss: 0.042 | Acc: 98.169% \n",
      "Waiting Test!\n",
      "测试分类准确率为：76.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 53\n",
      "[epoch:53, iter:4993] Loss: 0.049 | Acc: 93.750% \n",
      "[epoch:53, iter:4994] Loss: 0.048 | Acc: 96.875% \n",
      "[epoch:53, iter:4995] Loss: 0.035 | Acc: 97.917% \n",
      "[epoch:53, iter:4996] Loss: 0.077 | Acc: 96.875% \n",
      "[epoch:53, iter:4997] Loss: 0.062 | Acc: 97.500% \n",
      "[epoch:53, iter:4998] Loss: 0.052 | Acc: 97.917% \n",
      "[epoch:53, iter:4999] Loss: 0.045 | Acc: 98.214% \n",
      "[epoch:53, iter:5000] Loss: 0.046 | Acc: 97.656% \n",
      "[epoch:53, iter:5001] Loss: 0.041 | Acc: 97.917% \n",
      "[epoch:53, iter:5002] Loss: 0.039 | Acc: 98.125% \n",
      "[epoch:53, iter:5003] Loss: 0.035 | Acc: 98.295% \n",
      "[epoch:53, iter:5004] Loss: 0.035 | Acc: 98.438% \n",
      "[epoch:53, iter:5005] Loss: 0.032 | Acc: 98.558% \n",
      "[epoch:53, iter:5006] Loss: 0.030 | Acc: 98.661% \n",
      "[epoch:53, iter:5007] Loss: 0.031 | Acc: 98.750% \n",
      "[epoch:53, iter:5008] Loss: 0.029 | Acc: 98.828% \n",
      "[epoch:53, iter:5009] Loss: 0.029 | Acc: 98.897% \n",
      "[epoch:53, iter:5010] Loss: 0.032 | Acc: 98.611% \n",
      "[epoch:53, iter:5011] Loss: 0.031 | Acc: 98.684% \n",
      "[epoch:53, iter:5012] Loss: 0.029 | Acc: 98.750% \n",
      "[epoch:53, iter:5013] Loss: 0.028 | Acc: 98.810% \n",
      "[epoch:53, iter:5014] Loss: 0.027 | Acc: 98.864% \n",
      "[epoch:53, iter:5015] Loss: 0.029 | Acc: 98.641% \n",
      "[epoch:53, iter:5016] Loss: 0.028 | Acc: 98.698% \n",
      "[epoch:53, iter:5017] Loss: 0.031 | Acc: 98.500% \n",
      "[epoch:53, iter:5018] Loss: 0.030 | Acc: 98.558% \n",
      "[epoch:53, iter:5019] Loss: 0.029 | Acc: 98.611% \n",
      "[epoch:53, iter:5020] Loss: 0.030 | Acc: 98.661% \n",
      "[epoch:53, iter:5021] Loss: 0.038 | Acc: 98.491% \n",
      "[epoch:53, iter:5022] Loss: 0.037 | Acc: 98.542% \n",
      "[epoch:53, iter:5023] Loss: 0.036 | Acc: 98.589% \n",
      "[epoch:53, iter:5024] Loss: 0.035 | Acc: 98.633% \n",
      "[epoch:53, iter:5025] Loss: 0.034 | Acc: 98.674% \n",
      "[epoch:53, iter:5026] Loss: 0.033 | Acc: 98.713% \n",
      "[epoch:53, iter:5027] Loss: 0.032 | Acc: 98.750% \n",
      "[epoch:53, iter:5028] Loss: 0.031 | Acc: 98.785% \n",
      "[epoch:53, iter:5029] Loss: 0.031 | Acc: 98.818% \n",
      "[epoch:53, iter:5030] Loss: 0.030 | Acc: 98.849% \n",
      "[epoch:53, iter:5031] Loss: 0.029 | Acc: 98.878% \n",
      "[epoch:53, iter:5032] Loss: 0.031 | Acc: 98.750% \n",
      "[epoch:53, iter:5033] Loss: 0.035 | Acc: 98.628% \n",
      "[epoch:53, iter:5034] Loss: 0.034 | Acc: 98.661% \n",
      "[epoch:53, iter:5035] Loss: 0.034 | Acc: 98.692% \n",
      "[epoch:53, iter:5036] Loss: 0.033 | Acc: 98.722% \n",
      "[epoch:53, iter:5037] Loss: 0.033 | Acc: 98.750% \n",
      "[epoch:53, iter:5038] Loss: 0.032 | Acc: 98.777% \n",
      "[epoch:53, iter:5039] Loss: 0.034 | Acc: 98.670% \n",
      "[epoch:53, iter:5040] Loss: 0.036 | Acc: 98.568% \n",
      "[epoch:53, iter:5041] Loss: 0.036 | Acc: 98.469% \n",
      "[epoch:53, iter:5042] Loss: 0.035 | Acc: 98.500% \n",
      "[epoch:53, iter:5043] Loss: 0.042 | Acc: 98.407% \n",
      "[epoch:53, iter:5044] Loss: 0.042 | Acc: 98.438% \n",
      "[epoch:53, iter:5045] Loss: 0.042 | Acc: 98.349% \n",
      "[epoch:53, iter:5046] Loss: 0.041 | Acc: 98.380% \n",
      "[epoch:53, iter:5047] Loss: 0.041 | Acc: 98.409% \n",
      "[epoch:53, iter:5048] Loss: 0.040 | Acc: 98.438% \n",
      "[epoch:53, iter:5049] Loss: 0.040 | Acc: 98.465% \n",
      "[epoch:53, iter:5050] Loss: 0.040 | Acc: 98.491% \n",
      "[epoch:53, iter:5051] Loss: 0.040 | Acc: 98.411% \n",
      "[epoch:53, iter:5052] Loss: 0.040 | Acc: 98.438% \n",
      "[epoch:53, iter:5053] Loss: 0.039 | Acc: 98.463% \n",
      "[epoch:53, iter:5054] Loss: 0.038 | Acc: 98.488% \n",
      "[epoch:53, iter:5055] Loss: 0.038 | Acc: 98.512% \n",
      "[epoch:53, iter:5056] Loss: 0.038 | Acc: 98.438% \n",
      "[epoch:53, iter:5057] Loss: 0.038 | Acc: 98.462% \n",
      "[epoch:53, iter:5058] Loss: 0.037 | Acc: 98.485% \n",
      "[epoch:53, iter:5059] Loss: 0.037 | Acc: 98.507% \n",
      "[epoch:53, iter:5060] Loss: 0.036 | Acc: 98.529% \n",
      "[epoch:53, iter:5061] Loss: 0.039 | Acc: 98.460% \n",
      "[epoch:53, iter:5062] Loss: 0.039 | Acc: 98.393% \n",
      "[epoch:53, iter:5063] Loss: 0.039 | Acc: 98.415% \n",
      "[epoch:53, iter:5064] Loss: 0.039 | Acc: 98.438% \n",
      "[epoch:53, iter:5065] Loss: 0.039 | Acc: 98.459% \n",
      "[epoch:53, iter:5066] Loss: 0.039 | Acc: 98.480% \n",
      "[epoch:53, iter:5067] Loss: 0.038 | Acc: 98.500% \n",
      "[epoch:53, iter:5068] Loss: 0.038 | Acc: 98.520% \n",
      "[epoch:53, iter:5069] Loss: 0.038 | Acc: 98.539% \n",
      "[epoch:53, iter:5070] Loss: 0.037 | Acc: 98.558% \n",
      "[epoch:53, iter:5071] Loss: 0.037 | Acc: 98.576% \n",
      "[epoch:53, iter:5072] Loss: 0.037 | Acc: 98.594% \n",
      "[epoch:53, iter:5073] Loss: 0.036 | Acc: 98.611% \n",
      "[epoch:53, iter:5074] Loss: 0.036 | Acc: 98.628% \n",
      "[epoch:53, iter:5075] Loss: 0.036 | Acc: 98.569% \n",
      "[epoch:53, iter:5076] Loss: 0.036 | Acc: 98.586% \n",
      "[epoch:53, iter:5077] Loss: 0.036 | Acc: 98.603% \n",
      "[epoch:53, iter:5078] Loss: 0.036 | Acc: 98.547% \n",
      "[epoch:53, iter:5079] Loss: 0.037 | Acc: 98.491% \n",
      "[epoch:53, iter:5080] Loss: 0.036 | Acc: 98.509% \n",
      "[epoch:53, iter:5081] Loss: 0.037 | Acc: 98.455% \n",
      "[epoch:53, iter:5082] Loss: 0.038 | Acc: 98.403% \n",
      "[epoch:53, iter:5083] Loss: 0.038 | Acc: 98.420% \n",
      "[epoch:53, iter:5084] Loss: 0.038 | Acc: 98.438% \n",
      "[epoch:53, iter:5085] Loss: 0.037 | Acc: 98.454% \n",
      "[epoch:53, iter:5086] Loss: 0.037 | Acc: 98.471% \n",
      "[epoch:53, iter:5087] Loss: 0.039 | Acc: 98.421% \n",
      "[epoch:53, iter:5088] Loss: 0.039 | Acc: 98.430% \n",
      "Waiting Test!\n",
      "测试分类准确率为：75.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 54\n",
      "[epoch:54, iter:5089] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:54, iter:5090] Loss: 0.005 | Acc: 100.000% \n",
      "[epoch:54, iter:5091] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:54, iter:5092] Loss: 0.003 | Acc: 100.000% \n",
      "[epoch:54, iter:5093] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:54, iter:5094] Loss: 0.008 | Acc: 100.000% \n",
      "[epoch:54, iter:5095] Loss: 0.011 | Acc: 100.000% \n",
      "[epoch:54, iter:5096] Loss: 0.017 | Acc: 99.219% \n",
      "[epoch:54, iter:5097] Loss: 0.017 | Acc: 99.306% \n",
      "[epoch:54, iter:5098] Loss: 0.015 | Acc: 99.375% \n",
      "[epoch:54, iter:5099] Loss: 0.015 | Acc: 99.432% \n",
      "[epoch:54, iter:5100] Loss: 0.014 | Acc: 99.479% \n",
      "[epoch:54, iter:5101] Loss: 0.019 | Acc: 99.038% \n",
      "[epoch:54, iter:5102] Loss: 0.018 | Acc: 99.107% \n",
      "[epoch:54, iter:5103] Loss: 0.017 | Acc: 99.167% \n",
      "[epoch:54, iter:5104] Loss: 0.016 | Acc: 99.219% \n",
      "[epoch:54, iter:5105] Loss: 0.015 | Acc: 99.265% \n",
      "[epoch:54, iter:5106] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:54, iter:5107] Loss: 0.017 | Acc: 99.013% \n",
      "[epoch:54, iter:5108] Loss: 0.019 | Acc: 98.750% \n",
      "[epoch:54, iter:5109] Loss: 0.021 | Acc: 98.512% \n",
      "[epoch:54, iter:5110] Loss: 0.020 | Acc: 98.580% \n",
      "[epoch:54, iter:5111] Loss: 0.022 | Acc: 98.370% \n",
      "[epoch:54, iter:5112] Loss: 0.021 | Acc: 98.438% \n",
      "[epoch:54, iter:5113] Loss: 0.020 | Acc: 98.500% \n",
      "[epoch:54, iter:5114] Loss: 0.019 | Acc: 98.558% \n",
      "[epoch:54, iter:5115] Loss: 0.020 | Acc: 98.380% \n",
      "[epoch:54, iter:5116] Loss: 0.020 | Acc: 98.438% \n",
      "[epoch:54, iter:5117] Loss: 0.020 | Acc: 98.276% \n",
      "[epoch:54, iter:5118] Loss: 0.020 | Acc: 98.333% \n",
      "[epoch:54, iter:5119] Loss: 0.021 | Acc: 98.185% \n",
      "[epoch:54, iter:5120] Loss: 0.020 | Acc: 98.242% \n",
      "[epoch:54, iter:5121] Loss: 0.020 | Acc: 98.295% \n",
      "[epoch:54, iter:5122] Loss: 0.019 | Acc: 98.346% \n",
      "[epoch:54, iter:5123] Loss: 0.020 | Acc: 98.214% \n",
      "[epoch:54, iter:5124] Loss: 0.021 | Acc: 98.090% \n",
      "[epoch:54, iter:5125] Loss: 0.020 | Acc: 98.142% \n",
      "[epoch:54, iter:5126] Loss: 0.021 | Acc: 98.191% \n",
      "[epoch:54, iter:5127] Loss: 0.020 | Acc: 98.237% \n",
      "[epoch:54, iter:5128] Loss: 0.020 | Acc: 98.281% \n",
      "[epoch:54, iter:5129] Loss: 0.019 | Acc: 98.323% \n",
      "[epoch:54, iter:5130] Loss: 0.019 | Acc: 98.363% \n",
      "[epoch:54, iter:5131] Loss: 0.019 | Acc: 98.256% \n",
      "[epoch:54, iter:5132] Loss: 0.019 | Acc: 98.295% \n",
      "[epoch:54, iter:5133] Loss: 0.019 | Acc: 98.333% \n",
      "[epoch:54, iter:5134] Loss: 0.019 | Acc: 98.370% \n",
      "[epoch:54, iter:5135] Loss: 0.023 | Acc: 98.138% \n",
      "[epoch:54, iter:5136] Loss: 0.023 | Acc: 98.177% \n",
      "[epoch:54, iter:5137] Loss: 0.022 | Acc: 98.214% \n",
      "[epoch:54, iter:5138] Loss: 0.022 | Acc: 98.250% \n",
      "[epoch:54, iter:5139] Loss: 0.021 | Acc: 98.284% \n",
      "[epoch:54, iter:5140] Loss: 0.021 | Acc: 98.317% \n",
      "[epoch:54, iter:5141] Loss: 0.021 | Acc: 98.349% \n",
      "[epoch:54, iter:5142] Loss: 0.021 | Acc: 98.380% \n",
      "[epoch:54, iter:5143] Loss: 0.021 | Acc: 98.409% \n",
      "[epoch:54, iter:5144] Loss: 0.021 | Acc: 98.438% \n",
      "[epoch:54, iter:5145] Loss: 0.021 | Acc: 98.465% \n",
      "[epoch:54, iter:5146] Loss: 0.023 | Acc: 98.276% \n",
      "[epoch:54, iter:5147] Loss: 0.022 | Acc: 98.305% \n",
      "[epoch:54, iter:5148] Loss: 0.022 | Acc: 98.333% \n",
      "[epoch:54, iter:5149] Loss: 0.022 | Acc: 98.361% \n",
      "[epoch:54, iter:5150] Loss: 0.022 | Acc: 98.387% \n",
      "[epoch:54, iter:5151] Loss: 0.022 | Acc: 98.413% \n",
      "[epoch:54, iter:5152] Loss: 0.022 | Acc: 98.340% \n",
      "[epoch:54, iter:5153] Loss: 0.022 | Acc: 98.365% \n",
      "[epoch:54, iter:5154] Loss: 0.022 | Acc: 98.390% \n",
      "[epoch:54, iter:5155] Loss: 0.022 | Acc: 98.414% \n",
      "[epoch:54, iter:5156] Loss: 0.022 | Acc: 98.438% \n",
      "[epoch:54, iter:5157] Loss: 0.022 | Acc: 98.460% \n",
      "[epoch:54, iter:5158] Loss: 0.021 | Acc: 98.482% \n",
      "[epoch:54, iter:5159] Loss: 0.022 | Acc: 98.415% \n",
      "[epoch:54, iter:5160] Loss: 0.022 | Acc: 98.438% \n",
      "[epoch:54, iter:5161] Loss: 0.022 | Acc: 98.459% \n",
      "[epoch:54, iter:5162] Loss: 0.022 | Acc: 98.480% \n",
      "[epoch:54, iter:5163] Loss: 0.021 | Acc: 98.500% \n",
      "[epoch:54, iter:5164] Loss: 0.021 | Acc: 98.520% \n",
      "[epoch:54, iter:5165] Loss: 0.022 | Acc: 98.458% \n",
      "[epoch:54, iter:5166] Loss: 0.021 | Acc: 98.478% \n",
      "[epoch:54, iter:5167] Loss: 0.022 | Acc: 98.418% \n",
      "[epoch:54, iter:5168] Loss: 0.023 | Acc: 98.359% \n",
      "[epoch:54, iter:5169] Loss: 0.023 | Acc: 98.302% \n",
      "[epoch:54, iter:5170] Loss: 0.023 | Acc: 98.323% \n",
      "[epoch:54, iter:5171] Loss: 0.023 | Acc: 98.343% \n",
      "[epoch:54, iter:5172] Loss: 0.024 | Acc: 98.289% \n",
      "[epoch:54, iter:5173] Loss: 0.025 | Acc: 98.309% \n",
      "[epoch:54, iter:5174] Loss: 0.024 | Acc: 98.328% \n",
      "[epoch:54, iter:5175] Loss: 0.025 | Acc: 98.276% \n",
      "[epoch:54, iter:5176] Loss: 0.025 | Acc: 98.295% \n",
      "[epoch:54, iter:5177] Loss: 0.025 | Acc: 98.315% \n",
      "[epoch:54, iter:5178] Loss: 0.025 | Acc: 98.264% \n",
      "[epoch:54, iter:5179] Loss: 0.025 | Acc: 98.283% \n",
      "[epoch:54, iter:5180] Loss: 0.026 | Acc: 98.234% \n",
      "[epoch:54, iter:5181] Loss: 0.026 | Acc: 98.253% \n",
      "[epoch:54, iter:5182] Loss: 0.025 | Acc: 98.271% \n",
      "[epoch:54, iter:5183] Loss: 0.025 | Acc: 98.289% \n",
      "[epoch:54, iter:5184] Loss: 0.025 | Acc: 98.300% \n",
      "Waiting Test!\n",
      "测试分类准确率为：74.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 55\n",
      "[epoch:55, iter:5185] Loss: 0.003 | Acc: 100.000% \n",
      "[epoch:55, iter:5186] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:55, iter:5187] Loss: 0.017 | Acc: 97.917% \n",
      "[epoch:55, iter:5188] Loss: 0.014 | Acc: 98.438% \n",
      "[epoch:55, iter:5189] Loss: 0.011 | Acc: 98.750% \n",
      "[epoch:55, iter:5190] Loss: 0.012 | Acc: 98.958% \n",
      "[epoch:55, iter:5191] Loss: 0.010 | Acc: 99.107% \n",
      "[epoch:55, iter:5192] Loss: 0.009 | Acc: 99.219% \n",
      "[epoch:55, iter:5193] Loss: 0.008 | Acc: 99.306% \n",
      "[epoch:55, iter:5194] Loss: 0.007 | Acc: 99.375% \n",
      "[epoch:55, iter:5195] Loss: 0.007 | Acc: 99.432% \n",
      "[epoch:55, iter:5196] Loss: 0.007 | Acc: 99.479% \n",
      "[epoch:55, iter:5197] Loss: 0.009 | Acc: 99.519% \n",
      "[epoch:55, iter:5198] Loss: 0.008 | Acc: 99.554% \n",
      "[epoch:55, iter:5199] Loss: 0.010 | Acc: 99.583% \n",
      "[epoch:55, iter:5200] Loss: 0.012 | Acc: 99.609% \n",
      "[epoch:55, iter:5201] Loss: 0.011 | Acc: 99.632% \n",
      "[epoch:55, iter:5202] Loss: 0.011 | Acc: 99.653% \n",
      "[epoch:55, iter:5203] Loss: 0.010 | Acc: 99.671% \n",
      "[epoch:55, iter:5204] Loss: 0.012 | Acc: 99.688% \n",
      "[epoch:55, iter:5205] Loss: 0.014 | Acc: 99.405% \n",
      "[epoch:55, iter:5206] Loss: 0.015 | Acc: 99.148% \n",
      "[epoch:55, iter:5207] Loss: 0.018 | Acc: 98.913% \n",
      "[epoch:55, iter:5208] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:55, iter:5209] Loss: 0.017 | Acc: 99.000% \n",
      "[epoch:55, iter:5210] Loss: 0.016 | Acc: 99.038% \n",
      "[epoch:55, iter:5211] Loss: 0.015 | Acc: 99.074% \n",
      "[epoch:55, iter:5212] Loss: 0.016 | Acc: 99.107% \n",
      "[epoch:55, iter:5213] Loss: 0.026 | Acc: 98.922% \n",
      "[epoch:55, iter:5214] Loss: 0.025 | Acc: 98.958% \n",
      "[epoch:55, iter:5215] Loss: 0.025 | Acc: 98.992% \n",
      "[epoch:55, iter:5216] Loss: 0.024 | Acc: 99.023% \n",
      "[epoch:55, iter:5217] Loss: 0.024 | Acc: 99.053% \n",
      "[epoch:55, iter:5218] Loss: 0.023 | Acc: 99.081% \n",
      "[epoch:55, iter:5219] Loss: 0.024 | Acc: 98.929% \n",
      "[epoch:55, iter:5220] Loss: 0.026 | Acc: 98.785% \n",
      "[epoch:55, iter:5221] Loss: 0.027 | Acc: 98.649% \n",
      "[epoch:55, iter:5222] Loss: 0.026 | Acc: 98.684% \n",
      "[epoch:55, iter:5223] Loss: 0.026 | Acc: 98.718% \n",
      "[epoch:55, iter:5224] Loss: 0.026 | Acc: 98.750% \n",
      "[epoch:55, iter:5225] Loss: 0.026 | Acc: 98.628% \n",
      "[epoch:55, iter:5226] Loss: 0.026 | Acc: 98.661% \n",
      "[epoch:55, iter:5227] Loss: 0.025 | Acc: 98.692% \n",
      "[epoch:55, iter:5228] Loss: 0.026 | Acc: 98.580% \n",
      "[epoch:55, iter:5229] Loss: 0.026 | Acc: 98.611% \n",
      "[epoch:55, iter:5230] Loss: 0.026 | Acc: 98.641% \n",
      "[epoch:55, iter:5231] Loss: 0.026 | Acc: 98.670% \n",
      "[epoch:55, iter:5232] Loss: 0.025 | Acc: 98.698% \n",
      "[epoch:55, iter:5233] Loss: 0.026 | Acc: 98.597% \n",
      "[epoch:55, iter:5234] Loss: 0.028 | Acc: 98.375% \n",
      "[epoch:55, iter:5235] Loss: 0.027 | Acc: 98.407% \n",
      "[epoch:55, iter:5236] Loss: 0.027 | Acc: 98.438% \n",
      "[epoch:55, iter:5237] Loss: 0.026 | Acc: 98.467% \n",
      "[epoch:55, iter:5238] Loss: 0.026 | Acc: 98.495% \n",
      "[epoch:55, iter:5239] Loss: 0.028 | Acc: 98.409% \n",
      "[epoch:55, iter:5240] Loss: 0.028 | Acc: 98.438% \n",
      "[epoch:55, iter:5241] Loss: 0.029 | Acc: 98.355% \n",
      "[epoch:55, iter:5242] Loss: 0.028 | Acc: 98.384% \n",
      "[epoch:55, iter:5243] Loss: 0.028 | Acc: 98.411% \n",
      "[epoch:55, iter:5244] Loss: 0.028 | Acc: 98.438% \n",
      "[epoch:55, iter:5245] Loss: 0.028 | Acc: 98.361% \n",
      "[epoch:55, iter:5246] Loss: 0.028 | Acc: 98.387% \n",
      "[epoch:55, iter:5247] Loss: 0.028 | Acc: 98.413% \n",
      "[epoch:55, iter:5248] Loss: 0.033 | Acc: 98.242% \n",
      "[epoch:55, iter:5249] Loss: 0.033 | Acc: 98.269% \n",
      "[epoch:55, iter:5250] Loss: 0.034 | Acc: 98.201% \n",
      "[epoch:55, iter:5251] Loss: 0.034 | Acc: 98.228% \n",
      "[epoch:55, iter:5252] Loss: 0.033 | Acc: 98.254% \n",
      "[epoch:55, iter:5253] Loss: 0.033 | Acc: 98.279% \n",
      "[epoch:55, iter:5254] Loss: 0.032 | Acc: 98.304% \n",
      "[epoch:55, iter:5255] Loss: 0.032 | Acc: 98.239% \n",
      "[epoch:55, iter:5256] Loss: 0.033 | Acc: 98.264% \n",
      "[epoch:55, iter:5257] Loss: 0.032 | Acc: 98.288% \n",
      "[epoch:55, iter:5258] Loss: 0.032 | Acc: 98.311% \n",
      "[epoch:55, iter:5259] Loss: 0.032 | Acc: 98.333% \n",
      "[epoch:55, iter:5260] Loss: 0.031 | Acc: 98.355% \n",
      "[epoch:55, iter:5261] Loss: 0.031 | Acc: 98.377% \n",
      "[epoch:55, iter:5262] Loss: 0.031 | Acc: 98.397% \n",
      "[epoch:55, iter:5263] Loss: 0.031 | Acc: 98.418% \n",
      "[epoch:55, iter:5264] Loss: 0.030 | Acc: 98.438% \n",
      "[epoch:55, iter:5265] Loss: 0.031 | Acc: 98.380% \n",
      "[epoch:55, iter:5266] Loss: 0.032 | Acc: 98.323% \n",
      "[epoch:55, iter:5267] Loss: 0.031 | Acc: 98.343% \n",
      "[epoch:55, iter:5268] Loss: 0.032 | Acc: 98.289% \n",
      "[epoch:55, iter:5269] Loss: 0.032 | Acc: 98.309% \n",
      "[epoch:55, iter:5270] Loss: 0.032 | Acc: 98.328% \n",
      "[epoch:55, iter:5271] Loss: 0.032 | Acc: 98.276% \n",
      "[epoch:55, iter:5272] Loss: 0.032 | Acc: 98.295% \n",
      "[epoch:55, iter:5273] Loss: 0.032 | Acc: 98.244% \n",
      "[epoch:55, iter:5274] Loss: 0.032 | Acc: 98.264% \n",
      "[epoch:55, iter:5275] Loss: 0.032 | Acc: 98.214% \n",
      "[epoch:55, iter:5276] Loss: 0.032 | Acc: 98.234% \n",
      "[epoch:55, iter:5277] Loss: 0.032 | Acc: 98.253% \n",
      "[epoch:55, iter:5278] Loss: 0.036 | Acc: 98.138% \n",
      "[epoch:55, iter:5279] Loss: 0.036 | Acc: 98.092% \n",
      "[epoch:55, iter:5280] Loss: 0.036 | Acc: 98.103% \n",
      "Waiting Test!\n",
      "测试分类准确率为：71.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 56\n",
      "[epoch:56, iter:5281] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:56, iter:5282] Loss: 0.028 | Acc: 96.875% \n",
      "[epoch:56, iter:5283] Loss: 0.020 | Acc: 97.917% \n",
      "[epoch:56, iter:5284] Loss: 0.015 | Acc: 98.438% \n",
      "[epoch:56, iter:5285] Loss: 0.013 | Acc: 98.750% \n",
      "[epoch:56, iter:5286] Loss: 0.012 | Acc: 98.958% \n",
      "[epoch:56, iter:5287] Loss: 0.016 | Acc: 99.107% \n",
      "[epoch:56, iter:5288] Loss: 0.014 | Acc: 99.219% \n",
      "[epoch:56, iter:5289] Loss: 0.013 | Acc: 99.306% \n",
      "[epoch:56, iter:5290] Loss: 0.016 | Acc: 98.750% \n",
      "[epoch:56, iter:5291] Loss: 0.014 | Acc: 98.864% \n",
      "[epoch:56, iter:5292] Loss: 0.014 | Acc: 98.958% \n",
      "[epoch:56, iter:5293] Loss: 0.013 | Acc: 99.038% \n",
      "[epoch:56, iter:5294] Loss: 0.012 | Acc: 99.107% \n",
      "[epoch:56, iter:5295] Loss: 0.012 | Acc: 99.167% \n",
      "[epoch:56, iter:5296] Loss: 0.011 | Acc: 99.219% \n",
      "[epoch:56, iter:5297] Loss: 0.010 | Acc: 99.265% \n",
      "[epoch:56, iter:5298] Loss: 0.018 | Acc: 98.958% \n",
      "[epoch:56, iter:5299] Loss: 0.017 | Acc: 99.013% \n",
      "[epoch:56, iter:5300] Loss: 0.019 | Acc: 99.062% \n",
      "[epoch:56, iter:5301] Loss: 0.019 | Acc: 99.107% \n",
      "[epoch:56, iter:5302] Loss: 0.021 | Acc: 98.864% \n",
      "[epoch:56, iter:5303] Loss: 0.023 | Acc: 98.641% \n",
      "[epoch:56, iter:5304] Loss: 0.024 | Acc: 98.698% \n",
      "[epoch:56, iter:5305] Loss: 0.061 | Acc: 97.000% \n",
      "[epoch:56, iter:5306] Loss: 0.062 | Acc: 96.875% \n",
      "[epoch:56, iter:5307] Loss: 0.062 | Acc: 96.759% \n",
      "[epoch:56, iter:5308] Loss: 0.060 | Acc: 96.875% \n",
      "[epoch:56, iter:5309] Loss: 0.060 | Acc: 96.767% \n",
      "[epoch:56, iter:5310] Loss: 0.058 | Acc: 96.875% \n",
      "[epoch:56, iter:5311] Loss: 0.057 | Acc: 96.976% \n",
      "[epoch:56, iter:5312] Loss: 0.059 | Acc: 96.875% \n",
      "[epoch:56, iter:5313] Loss: 0.065 | Acc: 96.780% \n",
      "[epoch:56, iter:5314] Loss: 0.068 | Acc: 96.691% \n",
      "[epoch:56, iter:5315] Loss: 0.068 | Acc: 96.786% \n",
      "[epoch:56, iter:5316] Loss: 0.066 | Acc: 96.875% \n",
      "[epoch:56, iter:5317] Loss: 0.064 | Acc: 96.959% \n",
      "[epoch:56, iter:5318] Loss: 0.063 | Acc: 97.039% \n",
      "[epoch:56, iter:5319] Loss: 0.062 | Acc: 97.115% \n",
      "[epoch:56, iter:5320] Loss: 0.061 | Acc: 97.188% \n",
      "[epoch:56, iter:5321] Loss: 0.061 | Acc: 97.104% \n",
      "[epoch:56, iter:5322] Loss: 0.060 | Acc: 97.173% \n",
      "[epoch:56, iter:5323] Loss: 0.059 | Acc: 97.238% \n",
      "[epoch:56, iter:5324] Loss: 0.058 | Acc: 97.301% \n",
      "[epoch:56, iter:5325] Loss: 0.060 | Acc: 97.361% \n",
      "[epoch:56, iter:5326] Loss: 0.059 | Acc: 97.418% \n",
      "[epoch:56, iter:5327] Loss: 0.058 | Acc: 97.473% \n",
      "[epoch:56, iter:5328] Loss: 0.056 | Acc: 97.526% \n",
      "[epoch:56, iter:5329] Loss: 0.055 | Acc: 97.577% \n",
      "[epoch:56, iter:5330] Loss: 0.055 | Acc: 97.625% \n",
      "[epoch:56, iter:5331] Loss: 0.055 | Acc: 97.549% \n",
      "[epoch:56, iter:5332] Loss: 0.054 | Acc: 97.596% \n",
      "[epoch:56, iter:5333] Loss: 0.053 | Acc: 97.642% \n",
      "[epoch:56, iter:5334] Loss: 0.052 | Acc: 97.685% \n",
      "[epoch:56, iter:5335] Loss: 0.052 | Acc: 97.727% \n",
      "[epoch:56, iter:5336] Loss: 0.051 | Acc: 97.768% \n",
      "[epoch:56, iter:5337] Loss: 0.050 | Acc: 97.807% \n",
      "[epoch:56, iter:5338] Loss: 0.050 | Acc: 97.845% \n",
      "[epoch:56, iter:5339] Loss: 0.049 | Acc: 97.881% \n",
      "[epoch:56, iter:5340] Loss: 0.049 | Acc: 97.917% \n",
      "[epoch:56, iter:5341] Loss: 0.049 | Acc: 97.848% \n",
      "[epoch:56, iter:5342] Loss: 0.049 | Acc: 97.883% \n",
      "[epoch:56, iter:5343] Loss: 0.048 | Acc: 97.917% \n",
      "[epoch:56, iter:5344] Loss: 0.047 | Acc: 97.949% \n",
      "[epoch:56, iter:5345] Loss: 0.047 | Acc: 97.981% \n",
      "[epoch:56, iter:5346] Loss: 0.047 | Acc: 97.917% \n",
      "[epoch:56, iter:5347] Loss: 0.047 | Acc: 97.948% \n",
      "[epoch:56, iter:5348] Loss: 0.047 | Acc: 97.886% \n",
      "[epoch:56, iter:5349] Loss: 0.046 | Acc: 97.917% \n",
      "[epoch:56, iter:5350] Loss: 0.048 | Acc: 97.857% \n",
      "[epoch:56, iter:5351] Loss: 0.049 | Acc: 97.887% \n",
      "[epoch:56, iter:5352] Loss: 0.048 | Acc: 97.917% \n",
      "[epoch:56, iter:5353] Loss: 0.048 | Acc: 97.945% \n",
      "[epoch:56, iter:5354] Loss: 0.048 | Acc: 97.889% \n",
      "[epoch:56, iter:5355] Loss: 0.047 | Acc: 97.917% \n",
      "[epoch:56, iter:5356] Loss: 0.047 | Acc: 97.944% \n",
      "[epoch:56, iter:5357] Loss: 0.046 | Acc: 97.971% \n",
      "[epoch:56, iter:5358] Loss: 0.046 | Acc: 97.917% \n",
      "[epoch:56, iter:5359] Loss: 0.046 | Acc: 97.943% \n",
      "[epoch:56, iter:5360] Loss: 0.046 | Acc: 97.891% \n",
      "[epoch:56, iter:5361] Loss: 0.046 | Acc: 97.840% \n",
      "[epoch:56, iter:5362] Loss: 0.047 | Acc: 97.790% \n",
      "[epoch:56, iter:5363] Loss: 0.046 | Acc: 97.816% \n",
      "[epoch:56, iter:5364] Loss: 0.047 | Acc: 97.768% \n",
      "[epoch:56, iter:5365] Loss: 0.046 | Acc: 97.794% \n",
      "[epoch:56, iter:5366] Loss: 0.046 | Acc: 97.820% \n",
      "[epoch:56, iter:5367] Loss: 0.046 | Acc: 97.773% \n",
      "[epoch:56, iter:5368] Loss: 0.046 | Acc: 97.798% \n",
      "[epoch:56, iter:5369] Loss: 0.046 | Acc: 97.753% \n",
      "[epoch:56, iter:5370] Loss: 0.046 | Acc: 97.778% \n",
      "[epoch:56, iter:5371] Loss: 0.047 | Acc: 97.734% \n",
      "[epoch:56, iter:5372] Loss: 0.047 | Acc: 97.690% \n",
      "[epoch:56, iter:5373] Loss: 0.047 | Acc: 97.715% \n",
      "[epoch:56, iter:5374] Loss: 0.047 | Acc: 97.739% \n",
      "[epoch:56, iter:5375] Loss: 0.047 | Acc: 97.763% \n",
      "[epoch:56, iter:5376] Loss: 0.046 | Acc: 97.776% \n",
      "Waiting Test!\n",
      "测试分类准确率为：79.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 57\n",
      "[epoch:57, iter:5377] Loss: 0.023 | Acc: 100.000% \n",
      "[epoch:57, iter:5378] Loss: 0.017 | Acc: 100.000% \n",
      "[epoch:57, iter:5379] Loss: 0.039 | Acc: 97.917% \n",
      "[epoch:57, iter:5380] Loss: 0.065 | Acc: 96.875% \n",
      "[epoch:57, iter:5381] Loss: 0.053 | Acc: 97.500% \n",
      "[epoch:57, iter:5382] Loss: 0.066 | Acc: 95.833% \n",
      "[epoch:57, iter:5383] Loss: 0.057 | Acc: 96.429% \n",
      "[epoch:57, iter:5384] Loss: 0.052 | Acc: 96.875% \n",
      "[epoch:57, iter:5385] Loss: 0.053 | Acc: 96.528% \n",
      "[epoch:57, iter:5386] Loss: 0.051 | Acc: 96.875% \n",
      "[epoch:57, iter:5387] Loss: 0.047 | Acc: 97.159% \n",
      "[epoch:57, iter:5388] Loss: 0.043 | Acc: 97.396% \n",
      "[epoch:57, iter:5389] Loss: 0.040 | Acc: 97.596% \n",
      "[epoch:57, iter:5390] Loss: 0.059 | Acc: 97.321% \n",
      "[epoch:57, iter:5391] Loss: 0.057 | Acc: 97.500% \n",
      "[epoch:57, iter:5392] Loss: 0.054 | Acc: 97.656% \n",
      "[epoch:57, iter:5393] Loss: 0.085 | Acc: 96.324% \n",
      "[epoch:57, iter:5394] Loss: 0.083 | Acc: 96.528% \n",
      "[epoch:57, iter:5395] Loss: 0.080 | Acc: 96.711% \n",
      "[epoch:57, iter:5396] Loss: 0.078 | Acc: 96.875% \n",
      "[epoch:57, iter:5397] Loss: 0.075 | Acc: 97.024% \n",
      "[epoch:57, iter:5398] Loss: 0.072 | Acc: 97.159% \n",
      "[epoch:57, iter:5399] Loss: 0.069 | Acc: 97.283% \n",
      "[epoch:57, iter:5400] Loss: 0.066 | Acc: 97.396% \n",
      "[epoch:57, iter:5401] Loss: 0.063 | Acc: 97.500% \n",
      "[epoch:57, iter:5402] Loss: 0.061 | Acc: 97.596% \n",
      "[epoch:57, iter:5403] Loss: 0.060 | Acc: 97.685% \n",
      "[epoch:57, iter:5404] Loss: 0.060 | Acc: 97.768% \n",
      "[epoch:57, iter:5405] Loss: 0.059 | Acc: 97.845% \n",
      "[epoch:57, iter:5406] Loss: 0.059 | Acc: 97.708% \n",
      "[epoch:57, iter:5407] Loss: 0.060 | Acc: 97.581% \n",
      "[epoch:57, iter:5408] Loss: 0.058 | Acc: 97.656% \n",
      "[epoch:57, iter:5409] Loss: 0.060 | Acc: 97.538% \n",
      "[epoch:57, iter:5410] Loss: 0.059 | Acc: 97.610% \n",
      "[epoch:57, iter:5411] Loss: 0.060 | Acc: 97.500% \n",
      "[epoch:57, iter:5412] Loss: 0.059 | Acc: 97.569% \n",
      "[epoch:57, iter:5413] Loss: 0.062 | Acc: 97.297% \n",
      "[epoch:57, iter:5414] Loss: 0.063 | Acc: 97.204% \n",
      "[epoch:57, iter:5415] Loss: 0.063 | Acc: 97.276% \n",
      "[epoch:57, iter:5416] Loss: 0.061 | Acc: 97.344% \n",
      "[epoch:57, iter:5417] Loss: 0.061 | Acc: 97.409% \n",
      "[epoch:57, iter:5418] Loss: 0.060 | Acc: 97.470% \n",
      "[epoch:57, iter:5419] Loss: 0.059 | Acc: 97.529% \n",
      "[epoch:57, iter:5420] Loss: 0.058 | Acc: 97.585% \n",
      "[epoch:57, iter:5421] Loss: 0.057 | Acc: 97.639% \n",
      "[epoch:57, iter:5422] Loss: 0.059 | Acc: 97.554% \n",
      "[epoch:57, iter:5423] Loss: 0.059 | Acc: 97.473% \n",
      "[epoch:57, iter:5424] Loss: 0.057 | Acc: 97.526% \n",
      "[epoch:57, iter:5425] Loss: 0.056 | Acc: 97.577% \n",
      "[epoch:57, iter:5426] Loss: 0.055 | Acc: 97.625% \n",
      "[epoch:57, iter:5427] Loss: 0.054 | Acc: 97.672% \n",
      "[epoch:57, iter:5428] Loss: 0.054 | Acc: 97.716% \n",
      "[epoch:57, iter:5429] Loss: 0.053 | Acc: 97.759% \n",
      "[epoch:57, iter:5430] Loss: 0.052 | Acc: 97.801% \n",
      "[epoch:57, iter:5431] Loss: 0.051 | Acc: 97.841% \n",
      "[epoch:57, iter:5432] Loss: 0.053 | Acc: 97.656% \n",
      "[epoch:57, iter:5433] Loss: 0.054 | Acc: 97.588% \n",
      "[epoch:57, iter:5434] Loss: 0.059 | Acc: 97.306% \n",
      "[epoch:57, iter:5435] Loss: 0.058 | Acc: 97.352% \n",
      "[epoch:57, iter:5436] Loss: 0.059 | Acc: 97.292% \n",
      "[epoch:57, iter:5437] Loss: 0.058 | Acc: 97.336% \n",
      "[epoch:57, iter:5438] Loss: 0.057 | Acc: 97.379% \n",
      "[epoch:57, iter:5439] Loss: 0.061 | Acc: 97.222% \n",
      "[epoch:57, iter:5440] Loss: 0.060 | Acc: 97.266% \n",
      "[epoch:57, iter:5441] Loss: 0.059 | Acc: 97.308% \n",
      "[epoch:57, iter:5442] Loss: 0.059 | Acc: 97.348% \n",
      "[epoch:57, iter:5443] Loss: 0.058 | Acc: 97.388% \n",
      "[epoch:57, iter:5444] Loss: 0.058 | Acc: 97.335% \n",
      "[epoch:57, iter:5445] Loss: 0.057 | Acc: 97.373% \n",
      "[epoch:57, iter:5446] Loss: 0.056 | Acc: 97.411% \n",
      "[epoch:57, iter:5447] Loss: 0.056 | Acc: 97.447% \n",
      "[epoch:57, iter:5448] Loss: 0.056 | Acc: 97.483% \n",
      "[epoch:57, iter:5449] Loss: 0.055 | Acc: 97.517% \n",
      "[epoch:57, iter:5450] Loss: 0.056 | Acc: 97.466% \n",
      "[epoch:57, iter:5451] Loss: 0.056 | Acc: 97.500% \n",
      "[epoch:57, iter:5452] Loss: 0.055 | Acc: 97.533% \n",
      "[epoch:57, iter:5453] Loss: 0.054 | Acc: 97.565% \n",
      "[epoch:57, iter:5454] Loss: 0.054 | Acc: 97.596% \n",
      "[epoch:57, iter:5455] Loss: 0.053 | Acc: 97.627% \n",
      "[epoch:57, iter:5456] Loss: 0.053 | Acc: 97.656% \n",
      "[epoch:57, iter:5457] Loss: 0.052 | Acc: 97.685% \n",
      "[epoch:57, iter:5458] Loss: 0.052 | Acc: 97.713% \n",
      "[epoch:57, iter:5459] Loss: 0.051 | Acc: 97.741% \n",
      "[epoch:57, iter:5460] Loss: 0.051 | Acc: 97.768% \n",
      "[epoch:57, iter:5461] Loss: 0.051 | Acc: 97.794% \n",
      "[epoch:57, iter:5462] Loss: 0.050 | Acc: 97.820% \n",
      "[epoch:57, iter:5463] Loss: 0.051 | Acc: 97.701% \n",
      "[epoch:57, iter:5464] Loss: 0.053 | Acc: 97.656% \n",
      "[epoch:57, iter:5465] Loss: 0.053 | Acc: 97.612% \n",
      "[epoch:57, iter:5466] Loss: 0.053 | Acc: 97.639% \n",
      "[epoch:57, iter:5467] Loss: 0.052 | Acc: 97.665% \n",
      "[epoch:57, iter:5468] Loss: 0.052 | Acc: 97.690% \n",
      "[epoch:57, iter:5469] Loss: 0.052 | Acc: 97.648% \n",
      "[epoch:57, iter:5470] Loss: 0.053 | Acc: 97.606% \n",
      "[epoch:57, iter:5471] Loss: 0.053 | Acc: 97.632% \n",
      "[epoch:57, iter:5472] Loss: 0.052 | Acc: 97.646% \n",
      "Waiting Test!\n",
      "测试分类准确率为：73.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 58\n",
      "[epoch:58, iter:5473] Loss: 0.096 | Acc: 93.750% \n",
      "[epoch:58, iter:5474] Loss: 0.063 | Acc: 96.875% \n",
      "[epoch:58, iter:5475] Loss: 0.066 | Acc: 97.917% \n",
      "[epoch:58, iter:5476] Loss: 0.089 | Acc: 95.312% \n",
      "[epoch:58, iter:5477] Loss: 0.073 | Acc: 96.250% \n",
      "[epoch:58, iter:5478] Loss: 0.069 | Acc: 96.875% \n",
      "[epoch:58, iter:5479] Loss: 0.060 | Acc: 97.321% \n",
      "[epoch:58, iter:5480] Loss: 0.057 | Acc: 97.656% \n",
      "[epoch:58, iter:5481] Loss: 0.057 | Acc: 97.222% \n",
      "[epoch:58, iter:5482] Loss: 0.062 | Acc: 96.875% \n",
      "[epoch:58, iter:5483] Loss: 0.057 | Acc: 97.159% \n",
      "[epoch:58, iter:5484] Loss: 0.053 | Acc: 97.396% \n",
      "[epoch:58, iter:5485] Loss: 0.055 | Acc: 97.596% \n",
      "[epoch:58, iter:5486] Loss: 0.054 | Acc: 97.321% \n",
      "[epoch:58, iter:5487] Loss: 0.051 | Acc: 97.500% \n",
      "[epoch:58, iter:5488] Loss: 0.048 | Acc: 97.656% \n",
      "[epoch:58, iter:5489] Loss: 0.045 | Acc: 97.794% \n",
      "[epoch:58, iter:5490] Loss: 0.044 | Acc: 97.917% \n",
      "[epoch:58, iter:5491] Loss: 0.043 | Acc: 98.026% \n",
      "[epoch:58, iter:5492] Loss: 0.041 | Acc: 98.125% \n",
      "[epoch:58, iter:5493] Loss: 0.039 | Acc: 98.214% \n",
      "[epoch:58, iter:5494] Loss: 0.038 | Acc: 98.295% \n",
      "[epoch:58, iter:5495] Loss: 0.039 | Acc: 98.098% \n",
      "[epoch:58, iter:5496] Loss: 0.038 | Acc: 98.177% \n",
      "[epoch:58, iter:5497] Loss: 0.038 | Acc: 98.250% \n",
      "[epoch:58, iter:5498] Loss: 0.042 | Acc: 97.837% \n",
      "[epoch:58, iter:5499] Loss: 0.040 | Acc: 97.917% \n",
      "[epoch:58, iter:5500] Loss: 0.039 | Acc: 97.991% \n",
      "[epoch:58, iter:5501] Loss: 0.047 | Acc: 97.845% \n",
      "[epoch:58, iter:5502] Loss: 0.046 | Acc: 97.917% \n",
      "[epoch:58, iter:5503] Loss: 0.046 | Acc: 97.984% \n",
      "[epoch:58, iter:5504] Loss: 0.047 | Acc: 97.852% \n",
      "[epoch:58, iter:5505] Loss: 0.045 | Acc: 97.917% \n",
      "[epoch:58, iter:5506] Loss: 0.044 | Acc: 97.978% \n",
      "[epoch:58, iter:5507] Loss: 0.043 | Acc: 98.036% \n",
      "[epoch:58, iter:5508] Loss: 0.044 | Acc: 97.917% \n",
      "[epoch:58, iter:5509] Loss: 0.044 | Acc: 97.804% \n",
      "[epoch:58, iter:5510] Loss: 0.043 | Acc: 97.862% \n",
      "[epoch:58, iter:5511] Loss: 0.042 | Acc: 97.917% \n",
      "[epoch:58, iter:5512] Loss: 0.041 | Acc: 97.969% \n",
      "[epoch:58, iter:5513] Loss: 0.040 | Acc: 98.018% \n",
      "[epoch:58, iter:5514] Loss: 0.040 | Acc: 98.065% \n",
      "[epoch:58, iter:5515] Loss: 0.039 | Acc: 98.110% \n",
      "[epoch:58, iter:5516] Loss: 0.043 | Acc: 97.869% \n",
      "[epoch:58, iter:5517] Loss: 0.042 | Acc: 97.917% \n",
      "[epoch:58, iter:5518] Loss: 0.042 | Acc: 97.962% \n",
      "[epoch:58, iter:5519] Loss: 0.043 | Acc: 97.872% \n",
      "[epoch:58, iter:5520] Loss: 0.043 | Acc: 97.917% \n",
      "[epoch:58, iter:5521] Loss: 0.042 | Acc: 97.959% \n",
      "[epoch:58, iter:5522] Loss: 0.043 | Acc: 97.875% \n",
      "[epoch:58, iter:5523] Loss: 0.042 | Acc: 97.917% \n",
      "[epoch:58, iter:5524] Loss: 0.041 | Acc: 97.957% \n",
      "[epoch:58, iter:5525] Loss: 0.041 | Acc: 97.995% \n",
      "[epoch:58, iter:5526] Loss: 0.040 | Acc: 98.032% \n",
      "[epoch:58, iter:5527] Loss: 0.039 | Acc: 98.068% \n",
      "[epoch:58, iter:5528] Loss: 0.039 | Acc: 98.103% \n",
      "[epoch:58, iter:5529] Loss: 0.038 | Acc: 98.136% \n",
      "[epoch:58, iter:5530] Loss: 0.039 | Acc: 98.168% \n",
      "[epoch:58, iter:5531] Loss: 0.038 | Acc: 98.199% \n",
      "[epoch:58, iter:5532] Loss: 0.038 | Acc: 98.229% \n",
      "[epoch:58, iter:5533] Loss: 0.039 | Acc: 98.053% \n",
      "[epoch:58, iter:5534] Loss: 0.040 | Acc: 97.984% \n",
      "[epoch:58, iter:5535] Loss: 0.039 | Acc: 98.016% \n",
      "[epoch:58, iter:5536] Loss: 0.039 | Acc: 98.047% \n",
      "[epoch:58, iter:5537] Loss: 0.039 | Acc: 98.077% \n",
      "[epoch:58, iter:5538] Loss: 0.039 | Acc: 98.011% \n",
      "[epoch:58, iter:5539] Loss: 0.040 | Acc: 97.854% \n",
      "[epoch:58, iter:5540] Loss: 0.042 | Acc: 97.794% \n",
      "[epoch:58, iter:5541] Loss: 0.041 | Acc: 97.826% \n",
      "[epoch:58, iter:5542] Loss: 0.041 | Acc: 97.857% \n",
      "[epoch:58, iter:5543] Loss: 0.040 | Acc: 97.887% \n",
      "[epoch:58, iter:5544] Loss: 0.040 | Acc: 97.830% \n",
      "[epoch:58, iter:5545] Loss: 0.041 | Acc: 97.774% \n",
      "[epoch:58, iter:5546] Loss: 0.043 | Acc: 97.720% \n",
      "[epoch:58, iter:5547] Loss: 0.042 | Acc: 97.750% \n",
      "[epoch:58, iter:5548] Loss: 0.042 | Acc: 97.780% \n",
      "[epoch:58, iter:5549] Loss: 0.043 | Acc: 97.727% \n",
      "[epoch:58, iter:5550] Loss: 0.043 | Acc: 97.676% \n",
      "[epoch:58, iter:5551] Loss: 0.043 | Acc: 97.706% \n",
      "[epoch:58, iter:5552] Loss: 0.042 | Acc: 97.734% \n",
      "[epoch:58, iter:5553] Loss: 0.042 | Acc: 97.762% \n",
      "[epoch:58, iter:5554] Loss: 0.041 | Acc: 97.790% \n",
      "[epoch:58, iter:5555] Loss: 0.041 | Acc: 97.816% \n",
      "[epoch:58, iter:5556] Loss: 0.040 | Acc: 97.842% \n",
      "[epoch:58, iter:5557] Loss: 0.040 | Acc: 97.868% \n",
      "[epoch:58, iter:5558] Loss: 0.040 | Acc: 97.892% \n",
      "[epoch:58, iter:5559] Loss: 0.041 | Acc: 97.845% \n",
      "[epoch:58, iter:5560] Loss: 0.041 | Acc: 97.798% \n",
      "[epoch:58, iter:5561] Loss: 0.041 | Acc: 97.823% \n",
      "[epoch:58, iter:5562] Loss: 0.041 | Acc: 97.847% \n",
      "[epoch:58, iter:5563] Loss: 0.040 | Acc: 97.871% \n",
      "[epoch:58, iter:5564] Loss: 0.040 | Acc: 97.894% \n",
      "[epoch:58, iter:5565] Loss: 0.041 | Acc: 97.849% \n",
      "[epoch:58, iter:5566] Loss: 0.041 | Acc: 97.872% \n",
      "[epoch:58, iter:5567] Loss: 0.041 | Acc: 97.829% \n",
      "[epoch:58, iter:5568] Loss: 0.041 | Acc: 97.842% \n",
      "Waiting Test!\n",
      "测试分类准确率为：78.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 59\n",
      "[epoch:59, iter:5569] Loss: 0.057 | Acc: 93.750% \n",
      "[epoch:59, iter:5570] Loss: 0.038 | Acc: 96.875% \n",
      "[epoch:59, iter:5571] Loss: 0.103 | Acc: 93.750% \n",
      "[epoch:59, iter:5572] Loss: 0.077 | Acc: 95.312% \n",
      "[epoch:59, iter:5573] Loss: 0.079 | Acc: 95.000% \n",
      "[epoch:59, iter:5574] Loss: 0.068 | Acc: 95.833% \n",
      "[epoch:59, iter:5575] Loss: 0.066 | Acc: 95.536% \n",
      "[epoch:59, iter:5576] Loss: 0.058 | Acc: 96.094% \n",
      "[epoch:59, iter:5577] Loss: 0.054 | Acc: 96.528% \n",
      "[epoch:59, iter:5578] Loss: 0.049 | Acc: 96.875% \n",
      "[epoch:59, iter:5579] Loss: 0.045 | Acc: 97.159% \n",
      "[epoch:59, iter:5580] Loss: 0.045 | Acc: 97.396% \n",
      "[epoch:59, iter:5581] Loss: 0.042 | Acc: 97.596% \n",
      "[epoch:59, iter:5582] Loss: 0.050 | Acc: 97.321% \n",
      "[epoch:59, iter:5583] Loss: 0.047 | Acc: 97.500% \n",
      "[epoch:59, iter:5584] Loss: 0.045 | Acc: 97.656% \n",
      "[epoch:59, iter:5585] Loss: 0.042 | Acc: 97.794% \n",
      "[epoch:59, iter:5586] Loss: 0.045 | Acc: 97.569% \n",
      "[epoch:59, iter:5587] Loss: 0.046 | Acc: 97.697% \n",
      "[epoch:59, iter:5588] Loss: 0.044 | Acc: 97.812% \n",
      "[epoch:59, iter:5589] Loss: 0.042 | Acc: 97.917% \n",
      "[epoch:59, iter:5590] Loss: 0.041 | Acc: 98.011% \n",
      "[epoch:59, iter:5591] Loss: 0.039 | Acc: 98.098% \n",
      "[epoch:59, iter:5592] Loss: 0.039 | Acc: 98.177% \n",
      "[epoch:59, iter:5593] Loss: 0.039 | Acc: 98.250% \n",
      "[epoch:59, iter:5594] Loss: 0.038 | Acc: 98.317% \n",
      "[epoch:59, iter:5595] Loss: 0.038 | Acc: 98.148% \n",
      "[epoch:59, iter:5596] Loss: 0.037 | Acc: 98.214% \n",
      "[epoch:59, iter:5597] Loss: 0.036 | Acc: 98.276% \n",
      "[epoch:59, iter:5598] Loss: 0.036 | Acc: 98.333% \n",
      "[epoch:59, iter:5599] Loss: 0.038 | Acc: 98.185% \n",
      "[epoch:59, iter:5600] Loss: 0.036 | Acc: 98.242% \n",
      "[epoch:59, iter:5601] Loss: 0.035 | Acc: 98.295% \n",
      "[epoch:59, iter:5602] Loss: 0.048 | Acc: 98.162% \n",
      "[epoch:59, iter:5603] Loss: 0.047 | Acc: 98.214% \n",
      "[epoch:59, iter:5604] Loss: 0.047 | Acc: 98.090% \n",
      "[epoch:59, iter:5605] Loss: 0.047 | Acc: 98.142% \n",
      "[epoch:59, iter:5606] Loss: 0.047 | Acc: 98.191% \n",
      "[epoch:59, iter:5607] Loss: 0.047 | Acc: 98.237% \n",
      "[epoch:59, iter:5608] Loss: 0.046 | Acc: 98.281% \n",
      "[epoch:59, iter:5609] Loss: 0.049 | Acc: 98.171% \n",
      "[epoch:59, iter:5610] Loss: 0.048 | Acc: 98.214% \n",
      "[epoch:59, iter:5611] Loss: 0.049 | Acc: 98.110% \n",
      "[epoch:59, iter:5612] Loss: 0.047 | Acc: 98.153% \n",
      "[epoch:59, iter:5613] Loss: 0.047 | Acc: 98.194% \n",
      "[epoch:59, iter:5614] Loss: 0.050 | Acc: 97.826% \n",
      "[epoch:59, iter:5615] Loss: 0.050 | Acc: 97.872% \n",
      "[epoch:59, iter:5616] Loss: 0.050 | Acc: 97.917% \n",
      "[epoch:59, iter:5617] Loss: 0.049 | Acc: 97.959% \n",
      "[epoch:59, iter:5618] Loss: 0.050 | Acc: 97.875% \n",
      "[epoch:59, iter:5619] Loss: 0.050 | Acc: 97.917% \n",
      "[epoch:59, iter:5620] Loss: 0.049 | Acc: 97.957% \n",
      "[epoch:59, iter:5621] Loss: 0.048 | Acc: 97.995% \n",
      "[epoch:59, iter:5622] Loss: 0.048 | Acc: 98.032% \n",
      "[epoch:59, iter:5623] Loss: 0.049 | Acc: 97.955% \n",
      "[epoch:59, iter:5624] Loss: 0.048 | Acc: 97.991% \n",
      "[epoch:59, iter:5625] Loss: 0.048 | Acc: 98.026% \n",
      "[epoch:59, iter:5626] Loss: 0.048 | Acc: 98.060% \n",
      "[epoch:59, iter:5627] Loss: 0.048 | Acc: 97.987% \n",
      "[epoch:59, iter:5628] Loss: 0.048 | Acc: 98.021% \n",
      "[epoch:59, iter:5629] Loss: 0.048 | Acc: 98.053% \n",
      "[epoch:59, iter:5630] Loss: 0.048 | Acc: 98.085% \n",
      "[epoch:59, iter:5631] Loss: 0.047 | Acc: 98.115% \n",
      "[epoch:59, iter:5632] Loss: 0.047 | Acc: 98.145% \n",
      "[epoch:59, iter:5633] Loss: 0.047 | Acc: 98.173% \n",
      "[epoch:59, iter:5634] Loss: 0.046 | Acc: 98.201% \n",
      "[epoch:59, iter:5635] Loss: 0.048 | Acc: 98.134% \n",
      "[epoch:59, iter:5636] Loss: 0.047 | Acc: 98.162% \n",
      "[epoch:59, iter:5637] Loss: 0.047 | Acc: 98.188% \n",
      "[epoch:59, iter:5638] Loss: 0.046 | Acc: 98.214% \n",
      "[epoch:59, iter:5639] Loss: 0.046 | Acc: 98.239% \n",
      "[epoch:59, iter:5640] Loss: 0.053 | Acc: 98.177% \n",
      "[epoch:59, iter:5641] Loss: 0.054 | Acc: 98.031% \n",
      "[epoch:59, iter:5642] Loss: 0.054 | Acc: 98.057% \n",
      "[epoch:59, iter:5643] Loss: 0.053 | Acc: 98.083% \n",
      "[epoch:59, iter:5644] Loss: 0.053 | Acc: 98.109% \n",
      "[epoch:59, iter:5645] Loss: 0.053 | Acc: 98.052% \n",
      "[epoch:59, iter:5646] Loss: 0.053 | Acc: 97.997% \n",
      "[epoch:59, iter:5647] Loss: 0.053 | Acc: 98.022% \n",
      "[epoch:59, iter:5648] Loss: 0.052 | Acc: 98.047% \n",
      "[epoch:59, iter:5649] Loss: 0.052 | Acc: 98.071% \n",
      "[epoch:59, iter:5650] Loss: 0.051 | Acc: 98.095% \n",
      "[epoch:59, iter:5651] Loss: 0.050 | Acc: 98.117% \n",
      "[epoch:59, iter:5652] Loss: 0.050 | Acc: 98.140% \n",
      "[epoch:59, iter:5653] Loss: 0.049 | Acc: 98.162% \n",
      "[epoch:59, iter:5654] Loss: 0.050 | Acc: 98.110% \n",
      "[epoch:59, iter:5655] Loss: 0.053 | Acc: 97.989% \n",
      "[epoch:59, iter:5656] Loss: 0.053 | Acc: 98.011% \n",
      "[epoch:59, iter:5657] Loss: 0.052 | Acc: 98.034% \n",
      "[epoch:59, iter:5658] Loss: 0.055 | Acc: 97.986% \n",
      "[epoch:59, iter:5659] Loss: 0.054 | Acc: 98.008% \n",
      "[epoch:59, iter:5660] Loss: 0.054 | Acc: 98.030% \n",
      "[epoch:59, iter:5661] Loss: 0.055 | Acc: 97.917% \n",
      "[epoch:59, iter:5662] Loss: 0.055 | Acc: 97.872% \n",
      "[epoch:59, iter:5663] Loss: 0.055 | Acc: 97.895% \n",
      "[epoch:59, iter:5664] Loss: 0.054 | Acc: 97.907% \n",
      "Waiting Test!\n",
      "测试分类准确率为：75.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 60\n",
      "[epoch:60, iter:5665] Loss: 0.028 | Acc: 100.000% \n",
      "[epoch:60, iter:5666] Loss: 0.015 | Acc: 100.000% \n",
      "[epoch:60, iter:5667] Loss: 0.011 | Acc: 100.000% \n",
      "[epoch:60, iter:5668] Loss: 0.010 | Acc: 100.000% \n",
      "[epoch:60, iter:5669] Loss: 0.011 | Acc: 100.000% \n",
      "[epoch:60, iter:5670] Loss: 0.010 | Acc: 100.000% \n",
      "[epoch:60, iter:5671] Loss: 0.011 | Acc: 100.000% \n",
      "[epoch:60, iter:5672] Loss: 0.010 | Acc: 100.000% \n",
      "[epoch:60, iter:5673] Loss: 0.013 | Acc: 100.000% \n",
      "[epoch:60, iter:5674] Loss: 0.012 | Acc: 100.000% \n",
      "[epoch:60, iter:5675] Loss: 0.030 | Acc: 100.000% \n",
      "[epoch:60, iter:5676] Loss: 0.106 | Acc: 98.958% \n",
      "[epoch:60, iter:5677] Loss: 0.098 | Acc: 99.038% \n",
      "[epoch:60, iter:5678] Loss: 0.091 | Acc: 99.107% \n",
      "[epoch:60, iter:5679] Loss: 0.087 | Acc: 99.167% \n",
      "[epoch:60, iter:5680] Loss: 0.081 | Acc: 99.219% \n",
      "[epoch:60, iter:5681] Loss: 0.077 | Acc: 99.265% \n",
      "[epoch:60, iter:5682] Loss: 0.073 | Acc: 99.306% \n",
      "[epoch:60, iter:5683] Loss: 0.072 | Acc: 99.342% \n",
      "[epoch:60, iter:5684] Loss: 0.069 | Acc: 99.375% \n",
      "[epoch:60, iter:5685] Loss: 0.067 | Acc: 99.405% \n",
      "[epoch:60, iter:5686] Loss: 0.067 | Acc: 99.148% \n",
      "[epoch:60, iter:5687] Loss: 0.064 | Acc: 99.185% \n",
      "[epoch:60, iter:5688] Loss: 0.062 | Acc: 99.219% \n",
      "[epoch:60, iter:5689] Loss: 0.067 | Acc: 98.500% \n",
      "[epoch:60, iter:5690] Loss: 0.065 | Acc: 98.558% \n",
      "[epoch:60, iter:5691] Loss: 0.063 | Acc: 98.611% \n",
      "[epoch:60, iter:5692] Loss: 0.062 | Acc: 98.661% \n",
      "[epoch:60, iter:5693] Loss: 0.063 | Acc: 98.491% \n",
      "[epoch:60, iter:5694] Loss: 0.062 | Acc: 98.333% \n",
      "[epoch:60, iter:5695] Loss: 0.065 | Acc: 98.185% \n",
      "[epoch:60, iter:5696] Loss: 0.063 | Acc: 98.242% \n",
      "[epoch:60, iter:5697] Loss: 0.061 | Acc: 98.295% \n",
      "[epoch:60, iter:5698] Loss: 0.062 | Acc: 98.346% \n",
      "[epoch:60, iter:5699] Loss: 0.060 | Acc: 98.393% \n",
      "[epoch:60, iter:5700] Loss: 0.061 | Acc: 98.264% \n",
      "[epoch:60, iter:5701] Loss: 0.059 | Acc: 98.311% \n",
      "[epoch:60, iter:5702] Loss: 0.058 | Acc: 98.355% \n",
      "[epoch:60, iter:5703] Loss: 0.061 | Acc: 98.237% \n",
      "[epoch:60, iter:5704] Loss: 0.059 | Acc: 98.281% \n",
      "[epoch:60, iter:5705] Loss: 0.062 | Acc: 98.171% \n",
      "[epoch:60, iter:5706] Loss: 0.061 | Acc: 98.214% \n",
      "[epoch:60, iter:5707] Loss: 0.060 | Acc: 98.256% \n",
      "[epoch:60, iter:5708] Loss: 0.060 | Acc: 98.295% \n",
      "[epoch:60, iter:5709] Loss: 0.059 | Acc: 98.333% \n",
      "[epoch:60, iter:5710] Loss: 0.058 | Acc: 98.370% \n",
      "[epoch:60, iter:5711] Loss: 0.058 | Acc: 98.271% \n",
      "[epoch:60, iter:5712] Loss: 0.059 | Acc: 98.177% \n",
      "[epoch:60, iter:5713] Loss: 0.060 | Acc: 98.087% \n",
      "[epoch:60, iter:5714] Loss: 0.059 | Acc: 98.125% \n",
      "[epoch:60, iter:5715] Loss: 0.060 | Acc: 98.039% \n",
      "[epoch:60, iter:5716] Loss: 0.059 | Acc: 98.077% \n",
      "[epoch:60, iter:5717] Loss: 0.059 | Acc: 98.113% \n",
      "[epoch:60, iter:5718] Loss: 0.058 | Acc: 98.148% \n",
      "[epoch:60, iter:5719] Loss: 0.057 | Acc: 98.182% \n",
      "[epoch:60, iter:5720] Loss: 0.057 | Acc: 98.214% \n",
      "[epoch:60, iter:5721] Loss: 0.057 | Acc: 98.136% \n",
      "[epoch:60, iter:5722] Loss: 0.064 | Acc: 97.953% \n",
      "[epoch:60, iter:5723] Loss: 0.063 | Acc: 97.987% \n",
      "[epoch:60, iter:5724] Loss: 0.063 | Acc: 98.021% \n",
      "[epoch:60, iter:5725] Loss: 0.062 | Acc: 98.053% \n",
      "[epoch:60, iter:5726] Loss: 0.062 | Acc: 98.085% \n",
      "[epoch:60, iter:5727] Loss: 0.061 | Acc: 98.115% \n",
      "[epoch:60, iter:5728] Loss: 0.060 | Acc: 98.145% \n",
      "[epoch:60, iter:5729] Loss: 0.060 | Acc: 98.077% \n",
      "[epoch:60, iter:5730] Loss: 0.061 | Acc: 98.011% \n",
      "[epoch:60, iter:5731] Loss: 0.060 | Acc: 98.041% \n",
      "[epoch:60, iter:5732] Loss: 0.060 | Acc: 98.070% \n",
      "[epoch:60, iter:5733] Loss: 0.060 | Acc: 98.007% \n",
      "[epoch:60, iter:5734] Loss: 0.060 | Acc: 97.946% \n",
      "[epoch:60, iter:5735] Loss: 0.059 | Acc: 97.975% \n",
      "[epoch:60, iter:5736] Loss: 0.060 | Acc: 97.917% \n",
      "[epoch:60, iter:5737] Loss: 0.063 | Acc: 97.860% \n",
      "[epoch:60, iter:5738] Loss: 0.062 | Acc: 97.804% \n",
      "[epoch:60, iter:5739] Loss: 0.062 | Acc: 97.750% \n",
      "[epoch:60, iter:5740] Loss: 0.062 | Acc: 97.780% \n",
      "[epoch:60, iter:5741] Loss: 0.061 | Acc: 97.808% \n",
      "[epoch:60, iter:5742] Loss: 0.061 | Acc: 97.837% \n",
      "[epoch:60, iter:5743] Loss: 0.060 | Acc: 97.864% \n",
      "[epoch:60, iter:5744] Loss: 0.059 | Acc: 97.891% \n",
      "[epoch:60, iter:5745] Loss: 0.061 | Acc: 97.840% \n",
      "[epoch:60, iter:5746] Loss: 0.060 | Acc: 97.866% \n",
      "[epoch:60, iter:5747] Loss: 0.060 | Acc: 97.892% \n",
      "[epoch:60, iter:5748] Loss: 0.059 | Acc: 97.917% \n",
      "[epoch:60, iter:5749] Loss: 0.058 | Acc: 97.941% \n",
      "[epoch:60, iter:5750] Loss: 0.059 | Acc: 97.965% \n",
      "[epoch:60, iter:5751] Loss: 0.063 | Acc: 97.773% \n",
      "[epoch:60, iter:5752] Loss: 0.062 | Acc: 97.798% \n",
      "[epoch:60, iter:5753] Loss: 0.063 | Acc: 97.753% \n",
      "[epoch:60, iter:5754] Loss: 0.064 | Acc: 97.708% \n",
      "[epoch:60, iter:5755] Loss: 0.065 | Acc: 97.665% \n",
      "[epoch:60, iter:5756] Loss: 0.065 | Acc: 97.690% \n",
      "[epoch:60, iter:5757] Loss: 0.067 | Acc: 97.648% \n",
      "[epoch:60, iter:5758] Loss: 0.066 | Acc: 97.673% \n",
      "[epoch:60, iter:5759] Loss: 0.066 | Acc: 97.697% \n",
      "[epoch:60, iter:5760] Loss: 0.065 | Acc: 97.711% \n",
      "Waiting Test!\n",
      "测试分类准确率为：75.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 61\n",
      "[epoch:61, iter:5761] Loss: 0.022 | Acc: 100.000% \n",
      "[epoch:61, iter:5762] Loss: 0.023 | Acc: 100.000% \n",
      "[epoch:61, iter:5763] Loss: 0.017 | Acc: 100.000% \n",
      "[epoch:61, iter:5764] Loss: 0.015 | Acc: 100.000% \n",
      "[epoch:61, iter:5765] Loss: 0.020 | Acc: 100.000% \n",
      "[epoch:61, iter:5766] Loss: 0.020 | Acc: 100.000% \n",
      "[epoch:61, iter:5767] Loss: 0.019 | Acc: 100.000% \n",
      "[epoch:61, iter:5768] Loss: 0.025 | Acc: 99.219% \n",
      "[epoch:61, iter:5769] Loss: 0.026 | Acc: 99.306% \n",
      "[epoch:61, iter:5770] Loss: 0.024 | Acc: 99.375% \n",
      "[epoch:61, iter:5771] Loss: 0.022 | Acc: 99.432% \n",
      "[epoch:61, iter:5772] Loss: 0.021 | Acc: 99.479% \n",
      "[epoch:61, iter:5773] Loss: 0.022 | Acc: 99.519% \n",
      "[epoch:61, iter:5774] Loss: 0.021 | Acc: 99.554% \n",
      "[epoch:61, iter:5775] Loss: 0.022 | Acc: 99.583% \n",
      "[epoch:61, iter:5776] Loss: 0.023 | Acc: 99.609% \n",
      "[epoch:61, iter:5777] Loss: 0.022 | Acc: 99.632% \n",
      "[epoch:61, iter:5778] Loss: 0.021 | Acc: 99.653% \n",
      "[epoch:61, iter:5779] Loss: 0.021 | Acc: 99.671% \n",
      "[epoch:61, iter:5780] Loss: 0.022 | Acc: 99.688% \n",
      "[epoch:61, iter:5781] Loss: 0.023 | Acc: 99.702% \n",
      "[epoch:61, iter:5782] Loss: 0.023 | Acc: 99.716% \n",
      "[epoch:61, iter:5783] Loss: 0.022 | Acc: 99.728% \n",
      "[epoch:61, iter:5784] Loss: 0.025 | Acc: 99.740% \n",
      "[epoch:61, iter:5785] Loss: 0.024 | Acc: 99.750% \n",
      "[epoch:61, iter:5786] Loss: 0.024 | Acc: 99.760% \n",
      "[epoch:61, iter:5787] Loss: 0.025 | Acc: 99.769% \n",
      "[epoch:61, iter:5788] Loss: 0.024 | Acc: 99.777% \n",
      "[epoch:61, iter:5789] Loss: 0.023 | Acc: 99.784% \n",
      "[epoch:61, iter:5790] Loss: 0.026 | Acc: 99.583% \n",
      "[epoch:61, iter:5791] Loss: 0.025 | Acc: 99.597% \n",
      "[epoch:61, iter:5792] Loss: 0.026 | Acc: 99.609% \n",
      "[epoch:61, iter:5793] Loss: 0.025 | Acc: 99.621% \n",
      "[epoch:61, iter:5794] Loss: 0.025 | Acc: 99.632% \n",
      "[epoch:61, iter:5795] Loss: 0.024 | Acc: 99.643% \n",
      "[epoch:61, iter:5796] Loss: 0.024 | Acc: 99.653% \n",
      "[epoch:61, iter:5797] Loss: 0.023 | Acc: 99.662% \n",
      "[epoch:61, iter:5798] Loss: 0.023 | Acc: 99.671% \n",
      "[epoch:61, iter:5799] Loss: 0.022 | Acc: 99.679% \n",
      "[epoch:61, iter:5800] Loss: 0.022 | Acc: 99.688% \n",
      "[epoch:61, iter:5801] Loss: 0.021 | Acc: 99.695% \n",
      "[epoch:61, iter:5802] Loss: 0.026 | Acc: 99.256% \n",
      "[epoch:61, iter:5803] Loss: 0.026 | Acc: 99.128% \n",
      "[epoch:61, iter:5804] Loss: 0.027 | Acc: 99.148% \n",
      "[epoch:61, iter:5805] Loss: 0.026 | Acc: 99.167% \n",
      "[epoch:61, iter:5806] Loss: 0.026 | Acc: 99.185% \n",
      "[epoch:61, iter:5807] Loss: 0.030 | Acc: 98.936% \n",
      "[epoch:61, iter:5808] Loss: 0.029 | Acc: 98.958% \n",
      "[epoch:61, iter:5809] Loss: 0.031 | Acc: 98.724% \n",
      "[epoch:61, iter:5810] Loss: 0.031 | Acc: 98.750% \n",
      "[epoch:61, iter:5811] Loss: 0.031 | Acc: 98.775% \n",
      "[epoch:61, iter:5812] Loss: 0.032 | Acc: 98.678% \n",
      "[epoch:61, iter:5813] Loss: 0.031 | Acc: 98.703% \n",
      "[epoch:61, iter:5814] Loss: 0.031 | Acc: 98.727% \n",
      "[epoch:61, iter:5815] Loss: 0.030 | Acc: 98.750% \n",
      "[epoch:61, iter:5816] Loss: 0.030 | Acc: 98.772% \n",
      "[epoch:61, iter:5817] Loss: 0.029 | Acc: 98.794% \n",
      "[epoch:61, iter:5818] Loss: 0.029 | Acc: 98.815% \n",
      "[epoch:61, iter:5819] Loss: 0.030 | Acc: 98.729% \n",
      "[epoch:61, iter:5820] Loss: 0.032 | Acc: 98.542% \n",
      "[epoch:61, iter:5821] Loss: 0.032 | Acc: 98.566% \n",
      "[epoch:61, iter:5822] Loss: 0.033 | Acc: 98.488% \n",
      "[epoch:61, iter:5823] Loss: 0.032 | Acc: 98.512% \n",
      "[epoch:61, iter:5824] Loss: 0.032 | Acc: 98.535% \n",
      "[epoch:61, iter:5825] Loss: 0.031 | Acc: 98.558% \n",
      "[epoch:61, iter:5826] Loss: 0.031 | Acc: 98.580% \n",
      "[epoch:61, iter:5827] Loss: 0.030 | Acc: 98.601% \n",
      "[epoch:61, iter:5828] Loss: 0.030 | Acc: 98.621% \n",
      "[epoch:61, iter:5829] Loss: 0.030 | Acc: 98.641% \n",
      "[epoch:61, iter:5830] Loss: 0.030 | Acc: 98.661% \n",
      "[epoch:61, iter:5831] Loss: 0.031 | Acc: 98.504% \n",
      "[epoch:61, iter:5832] Loss: 0.036 | Acc: 98.351% \n",
      "[epoch:61, iter:5833] Loss: 0.038 | Acc: 98.288% \n",
      "[epoch:61, iter:5834] Loss: 0.038 | Acc: 98.311% \n",
      "[epoch:61, iter:5835] Loss: 0.037 | Acc: 98.333% \n",
      "[epoch:61, iter:5836] Loss: 0.037 | Acc: 98.355% \n",
      "[epoch:61, iter:5837] Loss: 0.037 | Acc: 98.377% \n",
      "[epoch:61, iter:5838] Loss: 0.038 | Acc: 98.237% \n",
      "[epoch:61, iter:5839] Loss: 0.038 | Acc: 98.180% \n",
      "[epoch:61, iter:5840] Loss: 0.038 | Acc: 98.203% \n",
      "[epoch:61, iter:5841] Loss: 0.038 | Acc: 98.225% \n",
      "[epoch:61, iter:5842] Loss: 0.038 | Acc: 98.171% \n",
      "[epoch:61, iter:5843] Loss: 0.038 | Acc: 98.193% \n",
      "[epoch:61, iter:5844] Loss: 0.039 | Acc: 98.065% \n",
      "[epoch:61, iter:5845] Loss: 0.039 | Acc: 98.088% \n",
      "[epoch:61, iter:5846] Loss: 0.039 | Acc: 98.038% \n",
      "[epoch:61, iter:5847] Loss: 0.039 | Acc: 98.060% \n",
      "[epoch:61, iter:5848] Loss: 0.039 | Acc: 98.082% \n",
      "[epoch:61, iter:5849] Loss: 0.038 | Acc: 98.104% \n",
      "[epoch:61, iter:5850] Loss: 0.038 | Acc: 98.125% \n",
      "[epoch:61, iter:5851] Loss: 0.038 | Acc: 98.146% \n",
      "[epoch:61, iter:5852] Loss: 0.038 | Acc: 98.098% \n",
      "[epoch:61, iter:5853] Loss: 0.039 | Acc: 98.051% \n",
      "[epoch:61, iter:5854] Loss: 0.038 | Acc: 98.072% \n",
      "[epoch:61, iter:5855] Loss: 0.038 | Acc: 98.026% \n",
      "[epoch:61, iter:5856] Loss: 0.038 | Acc: 98.038% \n",
      "Waiting Test!\n",
      "测试分类准确率为：88.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 62\n",
      "[epoch:62, iter:5857] Loss: 0.015 | Acc: 100.000% \n",
      "[epoch:62, iter:5858] Loss: 0.034 | Acc: 100.000% \n",
      "[epoch:62, iter:5859] Loss: 0.029 | Acc: 100.000% \n",
      "[epoch:62, iter:5860] Loss: 0.022 | Acc: 100.000% \n",
      "[epoch:62, iter:5861] Loss: 0.018 | Acc: 100.000% \n",
      "[epoch:62, iter:5862] Loss: 0.016 | Acc: 100.000% \n",
      "[epoch:62, iter:5863] Loss: 0.014 | Acc: 100.000% \n",
      "[epoch:62, iter:5864] Loss: 0.013 | Acc: 100.000% \n",
      "[epoch:62, iter:5865] Loss: 0.016 | Acc: 100.000% \n",
      "[epoch:62, iter:5866] Loss: 0.015 | Acc: 100.000% \n",
      "[epoch:62, iter:5867] Loss: 0.014 | Acc: 100.000% \n",
      "[epoch:62, iter:5868] Loss: 0.017 | Acc: 100.000% \n",
      "[epoch:62, iter:5869] Loss: 0.019 | Acc: 99.519% \n",
      "[epoch:62, iter:5870] Loss: 0.018 | Acc: 99.554% \n",
      "[epoch:62, iter:5871] Loss: 0.017 | Acc: 99.583% \n",
      "[epoch:62, iter:5872] Loss: 0.016 | Acc: 99.609% \n",
      "[epoch:62, iter:5873] Loss: 0.017 | Acc: 99.632% \n",
      "[epoch:62, iter:5874] Loss: 0.017 | Acc: 99.653% \n",
      "[epoch:62, iter:5875] Loss: 0.019 | Acc: 99.342% \n",
      "[epoch:62, iter:5876] Loss: 0.018 | Acc: 99.375% \n",
      "[epoch:62, iter:5877] Loss: 0.017 | Acc: 99.405% \n",
      "[epoch:62, iter:5878] Loss: 0.017 | Acc: 99.432% \n",
      "[epoch:62, iter:5879] Loss: 0.016 | Acc: 99.457% \n",
      "[epoch:62, iter:5880] Loss: 0.016 | Acc: 99.479% \n",
      "[epoch:62, iter:5881] Loss: 0.021 | Acc: 99.250% \n",
      "[epoch:62, iter:5882] Loss: 0.020 | Acc: 99.279% \n",
      "[epoch:62, iter:5883] Loss: 0.020 | Acc: 99.306% \n",
      "[epoch:62, iter:5884] Loss: 0.021 | Acc: 99.107% \n",
      "[epoch:62, iter:5885] Loss: 0.024 | Acc: 98.922% \n",
      "[epoch:62, iter:5886] Loss: 0.023 | Acc: 98.958% \n",
      "[epoch:62, iter:5887] Loss: 0.024 | Acc: 98.790% \n",
      "[epoch:62, iter:5888] Loss: 0.023 | Acc: 98.828% \n",
      "[epoch:62, iter:5889] Loss: 0.023 | Acc: 98.864% \n",
      "[epoch:62, iter:5890] Loss: 0.025 | Acc: 98.713% \n",
      "[epoch:62, iter:5891] Loss: 0.024 | Acc: 98.750% \n",
      "[epoch:62, iter:5892] Loss: 0.024 | Acc: 98.785% \n",
      "[epoch:62, iter:5893] Loss: 0.025 | Acc: 98.649% \n",
      "[epoch:62, iter:5894] Loss: 0.026 | Acc: 98.520% \n",
      "[epoch:62, iter:5895] Loss: 0.026 | Acc: 98.558% \n",
      "[epoch:62, iter:5896] Loss: 0.025 | Acc: 98.594% \n",
      "[epoch:62, iter:5897] Loss: 0.025 | Acc: 98.628% \n",
      "[epoch:62, iter:5898] Loss: 0.033 | Acc: 98.512% \n",
      "[epoch:62, iter:5899] Loss: 0.033 | Acc: 98.547% \n",
      "[epoch:62, iter:5900] Loss: 0.033 | Acc: 98.580% \n",
      "[epoch:62, iter:5901] Loss: 0.033 | Acc: 98.611% \n",
      "[epoch:62, iter:5902] Loss: 0.032 | Acc: 98.641% \n",
      "[epoch:62, iter:5903] Loss: 0.033 | Acc: 98.537% \n",
      "[epoch:62, iter:5904] Loss: 0.032 | Acc: 98.568% \n",
      "[epoch:62, iter:5905] Loss: 0.032 | Acc: 98.597% \n",
      "[epoch:62, iter:5906] Loss: 0.034 | Acc: 98.500% \n",
      "[epoch:62, iter:5907] Loss: 0.034 | Acc: 98.529% \n",
      "[epoch:62, iter:5908] Loss: 0.034 | Acc: 98.438% \n",
      "[epoch:62, iter:5909] Loss: 0.034 | Acc: 98.467% \n",
      "[epoch:62, iter:5910] Loss: 0.036 | Acc: 98.380% \n",
      "[epoch:62, iter:5911] Loss: 0.036 | Acc: 98.295% \n",
      "[epoch:62, iter:5912] Loss: 0.036 | Acc: 98.326% \n",
      "[epoch:62, iter:5913] Loss: 0.036 | Acc: 98.355% \n",
      "[epoch:62, iter:5914] Loss: 0.035 | Acc: 98.384% \n",
      "[epoch:62, iter:5915] Loss: 0.039 | Acc: 98.305% \n",
      "[epoch:62, iter:5916] Loss: 0.039 | Acc: 98.333% \n",
      "[epoch:62, iter:5917] Loss: 0.039 | Acc: 98.361% \n",
      "[epoch:62, iter:5918] Loss: 0.038 | Acc: 98.387% \n",
      "[epoch:62, iter:5919] Loss: 0.038 | Acc: 98.413% \n",
      "[epoch:62, iter:5920] Loss: 0.038 | Acc: 98.438% \n",
      "[epoch:62, iter:5921] Loss: 0.037 | Acc: 98.462% \n",
      "[epoch:62, iter:5922] Loss: 0.037 | Acc: 98.485% \n",
      "[epoch:62, iter:5923] Loss: 0.036 | Acc: 98.507% \n",
      "[epoch:62, iter:5924] Loss: 0.036 | Acc: 98.529% \n",
      "[epoch:62, iter:5925] Loss: 0.036 | Acc: 98.551% \n",
      "[epoch:62, iter:5926] Loss: 0.036 | Acc: 98.571% \n",
      "[epoch:62, iter:5927] Loss: 0.035 | Acc: 98.592% \n",
      "[epoch:62, iter:5928] Loss: 0.035 | Acc: 98.611% \n",
      "[epoch:62, iter:5929] Loss: 0.035 | Acc: 98.630% \n",
      "[epoch:62, iter:5930] Loss: 0.035 | Acc: 98.649% \n",
      "[epoch:62, iter:5931] Loss: 0.036 | Acc: 98.583% \n",
      "[epoch:62, iter:5932] Loss: 0.036 | Acc: 98.520% \n",
      "[epoch:62, iter:5933] Loss: 0.035 | Acc: 98.539% \n",
      "[epoch:62, iter:5934] Loss: 0.037 | Acc: 98.478% \n",
      "[epoch:62, iter:5935] Loss: 0.037 | Acc: 98.497% \n",
      "[epoch:62, iter:5936] Loss: 0.040 | Acc: 98.359% \n",
      "[epoch:62, iter:5937] Loss: 0.040 | Acc: 98.380% \n",
      "[epoch:62, iter:5938] Loss: 0.039 | Acc: 98.399% \n",
      "[epoch:62, iter:5939] Loss: 0.039 | Acc: 98.419% \n",
      "[epoch:62, iter:5940] Loss: 0.039 | Acc: 98.438% \n",
      "[epoch:62, iter:5941] Loss: 0.038 | Acc: 98.456% \n",
      "[epoch:62, iter:5942] Loss: 0.038 | Acc: 98.474% \n",
      "[epoch:62, iter:5943] Loss: 0.038 | Acc: 98.491% \n",
      "[epoch:62, iter:5944] Loss: 0.037 | Acc: 98.509% \n",
      "[epoch:62, iter:5945] Loss: 0.039 | Acc: 98.385% \n",
      "[epoch:62, iter:5946] Loss: 0.038 | Acc: 98.403% \n",
      "[epoch:62, iter:5947] Loss: 0.038 | Acc: 98.352% \n",
      "[epoch:62, iter:5948] Loss: 0.038 | Acc: 98.370% \n",
      "[epoch:62, iter:5949] Loss: 0.039 | Acc: 98.320% \n",
      "[epoch:62, iter:5950] Loss: 0.039 | Acc: 98.338% \n",
      "[epoch:62, iter:5951] Loss: 0.039 | Acc: 98.289% \n",
      "[epoch:62, iter:5952] Loss: 0.039 | Acc: 98.300% \n",
      "Waiting Test!\n",
      "测试分类准确率为：83.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 63\n",
      "[epoch:63, iter:5953] Loss: 0.003 | Acc: 100.000% \n",
      "[epoch:63, iter:5954] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:63, iter:5955] Loss: 0.003 | Acc: 100.000% \n",
      "[epoch:63, iter:5956] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:63, iter:5957] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:63, iter:5958] Loss: 0.006 | Acc: 100.000% \n",
      "[epoch:63, iter:5959] Loss: 0.010 | Acc: 100.000% \n",
      "[epoch:63, iter:5960] Loss: 0.010 | Acc: 100.000% \n",
      "[epoch:63, iter:5961] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:63, iter:5962] Loss: 0.008 | Acc: 100.000% \n",
      "[epoch:63, iter:5963] Loss: 0.013 | Acc: 99.432% \n",
      "[epoch:63, iter:5964] Loss: 0.012 | Acc: 99.479% \n",
      "[epoch:63, iter:5965] Loss: 0.014 | Acc: 99.519% \n",
      "[epoch:63, iter:5966] Loss: 0.016 | Acc: 99.554% \n",
      "[epoch:63, iter:5967] Loss: 0.018 | Acc: 99.583% \n",
      "[epoch:63, iter:5968] Loss: 0.017 | Acc: 99.609% \n",
      "[epoch:63, iter:5969] Loss: 0.017 | Acc: 99.632% \n",
      "[epoch:63, iter:5970] Loss: 0.019 | Acc: 99.653% \n",
      "[epoch:63, iter:5971] Loss: 0.021 | Acc: 99.342% \n",
      "[epoch:63, iter:5972] Loss: 0.020 | Acc: 99.375% \n",
      "[epoch:63, iter:5973] Loss: 0.021 | Acc: 99.107% \n",
      "[epoch:63, iter:5974] Loss: 0.021 | Acc: 99.148% \n",
      "[epoch:63, iter:5975] Loss: 0.021 | Acc: 99.185% \n",
      "[epoch:63, iter:5976] Loss: 0.020 | Acc: 99.219% \n",
      "[epoch:63, iter:5977] Loss: 0.020 | Acc: 99.250% \n",
      "[epoch:63, iter:5978] Loss: 0.019 | Acc: 99.279% \n",
      "[epoch:63, iter:5979] Loss: 0.019 | Acc: 99.306% \n",
      "[epoch:63, iter:5980] Loss: 0.020 | Acc: 99.107% \n",
      "[epoch:63, iter:5981] Loss: 0.019 | Acc: 99.138% \n",
      "[epoch:63, iter:5982] Loss: 0.019 | Acc: 99.167% \n",
      "[epoch:63, iter:5983] Loss: 0.018 | Acc: 99.194% \n",
      "[epoch:63, iter:5984] Loss: 0.018 | Acc: 99.219% \n",
      "[epoch:63, iter:5985] Loss: 0.017 | Acc: 99.242% \n",
      "[epoch:63, iter:5986] Loss: 0.017 | Acc: 99.265% \n",
      "[epoch:63, iter:5987] Loss: 0.016 | Acc: 99.286% \n",
      "[epoch:63, iter:5988] Loss: 0.017 | Acc: 99.306% \n",
      "[epoch:63, iter:5989] Loss: 0.016 | Acc: 99.324% \n",
      "[epoch:63, iter:5990] Loss: 0.023 | Acc: 99.013% \n",
      "[epoch:63, iter:5991] Loss: 0.025 | Acc: 99.038% \n",
      "[epoch:63, iter:5992] Loss: 0.025 | Acc: 99.062% \n",
      "[epoch:63, iter:5993] Loss: 0.027 | Acc: 98.933% \n",
      "[epoch:63, iter:5994] Loss: 0.026 | Acc: 98.958% \n",
      "[epoch:63, iter:5995] Loss: 0.028 | Acc: 98.837% \n",
      "[epoch:63, iter:5996] Loss: 0.028 | Acc: 98.864% \n",
      "[epoch:63, iter:5997] Loss: 0.028 | Acc: 98.750% \n",
      "[epoch:63, iter:5998] Loss: 0.028 | Acc: 98.777% \n",
      "[epoch:63, iter:5999] Loss: 0.028 | Acc: 98.803% \n",
      "[epoch:63, iter:6000] Loss: 0.027 | Acc: 98.828% \n",
      "[epoch:63, iter:6001] Loss: 0.028 | Acc: 98.724% \n",
      "[epoch:63, iter:6002] Loss: 0.029 | Acc: 98.625% \n",
      "[epoch:63, iter:6003] Loss: 0.029 | Acc: 98.529% \n",
      "[epoch:63, iter:6004] Loss: 0.029 | Acc: 98.558% \n",
      "[epoch:63, iter:6005] Loss: 0.029 | Acc: 98.467% \n",
      "[epoch:63, iter:6006] Loss: 0.030 | Acc: 98.495% \n",
      "[epoch:63, iter:6007] Loss: 0.029 | Acc: 98.523% \n",
      "[epoch:63, iter:6008] Loss: 0.029 | Acc: 98.549% \n",
      "[epoch:63, iter:6009] Loss: 0.028 | Acc: 98.575% \n",
      "[epoch:63, iter:6010] Loss: 0.028 | Acc: 98.599% \n",
      "[epoch:63, iter:6011] Loss: 0.029 | Acc: 98.517% \n",
      "[epoch:63, iter:6012] Loss: 0.028 | Acc: 98.542% \n",
      "[epoch:63, iter:6013] Loss: 0.029 | Acc: 98.463% \n",
      "[epoch:63, iter:6014] Loss: 0.029 | Acc: 98.488% \n",
      "[epoch:63, iter:6015] Loss: 0.031 | Acc: 98.413% \n",
      "[epoch:63, iter:6016] Loss: 0.031 | Acc: 98.340% \n",
      "[epoch:63, iter:6017] Loss: 0.031 | Acc: 98.365% \n",
      "[epoch:63, iter:6018] Loss: 0.033 | Acc: 98.201% \n",
      "[epoch:63, iter:6019] Loss: 0.032 | Acc: 98.228% \n",
      "[epoch:63, iter:6020] Loss: 0.033 | Acc: 98.162% \n",
      "[epoch:63, iter:6021] Loss: 0.032 | Acc: 98.188% \n",
      "[epoch:63, iter:6022] Loss: 0.032 | Acc: 98.214% \n",
      "[epoch:63, iter:6023] Loss: 0.032 | Acc: 98.239% \n",
      "[epoch:63, iter:6024] Loss: 0.031 | Acc: 98.264% \n",
      "[epoch:63, iter:6025] Loss: 0.032 | Acc: 98.202% \n",
      "[epoch:63, iter:6026] Loss: 0.031 | Acc: 98.226% \n",
      "[epoch:63, iter:6027] Loss: 0.031 | Acc: 98.250% \n",
      "[epoch:63, iter:6028] Loss: 0.031 | Acc: 98.273% \n",
      "[epoch:63, iter:6029] Loss: 0.031 | Acc: 98.214% \n",
      "[epoch:63, iter:6030] Loss: 0.031 | Acc: 98.237% \n",
      "[epoch:63, iter:6031] Loss: 0.031 | Acc: 98.259% \n",
      "[epoch:63, iter:6032] Loss: 0.030 | Acc: 98.281% \n",
      "[epoch:63, iter:6033] Loss: 0.030 | Acc: 98.302% \n",
      "[epoch:63, iter:6034] Loss: 0.030 | Acc: 98.323% \n",
      "[epoch:63, iter:6035] Loss: 0.029 | Acc: 98.343% \n",
      "[epoch:63, iter:6036] Loss: 0.031 | Acc: 98.214% \n",
      "[epoch:63, iter:6037] Loss: 0.034 | Acc: 98.162% \n",
      "[epoch:63, iter:6038] Loss: 0.034 | Acc: 98.110% \n",
      "[epoch:63, iter:6039] Loss: 0.034 | Acc: 98.132% \n",
      "[epoch:63, iter:6040] Loss: 0.035 | Acc: 98.082% \n",
      "[epoch:63, iter:6041] Loss: 0.035 | Acc: 98.104% \n",
      "[epoch:63, iter:6042] Loss: 0.035 | Acc: 98.125% \n",
      "[epoch:63, iter:6043] Loss: 0.034 | Acc: 98.146% \n",
      "[epoch:63, iter:6044] Loss: 0.034 | Acc: 98.098% \n",
      "[epoch:63, iter:6045] Loss: 0.034 | Acc: 98.118% \n",
      "[epoch:63, iter:6046] Loss: 0.034 | Acc: 98.138% \n",
      "[epoch:63, iter:6047] Loss: 0.034 | Acc: 98.158% \n",
      "[epoch:63, iter:6048] Loss: 0.033 | Acc: 98.169% \n",
      "Waiting Test!\n",
      "测试分类准确率为：85.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 64\n",
      "[epoch:64, iter:6049] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:64, iter:6050] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:64, iter:6051] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:64, iter:6052] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:64, iter:6053] Loss: 0.024 | Acc: 98.750% \n",
      "[epoch:64, iter:6054] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:64, iter:6055] Loss: 0.018 | Acc: 99.107% \n",
      "[epoch:64, iter:6056] Loss: 0.023 | Acc: 98.438% \n",
      "[epoch:64, iter:6057] Loss: 0.025 | Acc: 97.917% \n",
      "[epoch:64, iter:6058] Loss: 0.023 | Acc: 98.125% \n",
      "[epoch:64, iter:6059] Loss: 0.021 | Acc: 98.295% \n",
      "[epoch:64, iter:6060] Loss: 0.024 | Acc: 97.917% \n",
      "[epoch:64, iter:6061] Loss: 0.022 | Acc: 98.077% \n",
      "[epoch:64, iter:6062] Loss: 0.021 | Acc: 98.214% \n",
      "[epoch:64, iter:6063] Loss: 0.026 | Acc: 97.917% \n",
      "[epoch:64, iter:6064] Loss: 0.025 | Acc: 98.047% \n",
      "[epoch:64, iter:6065] Loss: 0.029 | Acc: 97.426% \n",
      "[epoch:64, iter:6066] Loss: 0.031 | Acc: 97.222% \n",
      "[epoch:64, iter:6067] Loss: 0.029 | Acc: 97.368% \n",
      "[epoch:64, iter:6068] Loss: 0.028 | Acc: 97.500% \n",
      "[epoch:64, iter:6069] Loss: 0.029 | Acc: 97.619% \n",
      "[epoch:64, iter:6070] Loss: 0.028 | Acc: 97.727% \n",
      "[epoch:64, iter:6071] Loss: 0.027 | Acc: 97.826% \n",
      "[epoch:64, iter:6072] Loss: 0.027 | Acc: 97.917% \n",
      "[epoch:64, iter:6073] Loss: 0.029 | Acc: 97.750% \n",
      "[epoch:64, iter:6074] Loss: 0.030 | Acc: 97.596% \n",
      "[epoch:64, iter:6075] Loss: 0.029 | Acc: 97.685% \n",
      "[epoch:64, iter:6076] Loss: 0.028 | Acc: 97.768% \n",
      "[epoch:64, iter:6077] Loss: 0.028 | Acc: 97.845% \n",
      "[epoch:64, iter:6078] Loss: 0.030 | Acc: 97.917% \n",
      "[epoch:64, iter:6079] Loss: 0.030 | Acc: 97.782% \n",
      "[epoch:64, iter:6080] Loss: 0.030 | Acc: 97.852% \n",
      "[epoch:64, iter:6081] Loss: 0.033 | Acc: 97.538% \n",
      "[epoch:64, iter:6082] Loss: 0.032 | Acc: 97.610% \n",
      "[epoch:64, iter:6083] Loss: 0.031 | Acc: 97.679% \n",
      "[epoch:64, iter:6084] Loss: 0.031 | Acc: 97.743% \n",
      "[epoch:64, iter:6085] Loss: 0.030 | Acc: 97.804% \n",
      "[epoch:64, iter:6086] Loss: 0.030 | Acc: 97.862% \n",
      "[epoch:64, iter:6087] Loss: 0.029 | Acc: 97.917% \n",
      "[epoch:64, iter:6088] Loss: 0.028 | Acc: 97.969% \n",
      "[epoch:64, iter:6089] Loss: 0.028 | Acc: 98.018% \n",
      "[epoch:64, iter:6090] Loss: 0.027 | Acc: 98.065% \n",
      "[epoch:64, iter:6091] Loss: 0.026 | Acc: 98.110% \n",
      "[epoch:64, iter:6092] Loss: 0.026 | Acc: 98.153% \n",
      "[epoch:64, iter:6093] Loss: 0.025 | Acc: 98.194% \n",
      "[epoch:64, iter:6094] Loss: 0.025 | Acc: 98.234% \n",
      "[epoch:64, iter:6095] Loss: 0.024 | Acc: 98.271% \n",
      "[epoch:64, iter:6096] Loss: 0.024 | Acc: 98.307% \n",
      "[epoch:64, iter:6097] Loss: 0.026 | Acc: 98.214% \n",
      "[epoch:64, iter:6098] Loss: 0.026 | Acc: 98.250% \n",
      "[epoch:64, iter:6099] Loss: 0.026 | Acc: 98.284% \n",
      "[epoch:64, iter:6100] Loss: 0.027 | Acc: 98.197% \n",
      "[epoch:64, iter:6101] Loss: 0.026 | Acc: 98.231% \n",
      "[epoch:64, iter:6102] Loss: 0.026 | Acc: 98.264% \n",
      "[epoch:64, iter:6103] Loss: 0.027 | Acc: 98.182% \n",
      "[epoch:64, iter:6104] Loss: 0.027 | Acc: 98.214% \n",
      "[epoch:64, iter:6105] Loss: 0.026 | Acc: 98.246% \n",
      "[epoch:64, iter:6106] Loss: 0.026 | Acc: 98.276% \n",
      "[epoch:64, iter:6107] Loss: 0.025 | Acc: 98.305% \n",
      "[epoch:64, iter:6108] Loss: 0.027 | Acc: 98.229% \n",
      "[epoch:64, iter:6109] Loss: 0.026 | Acc: 98.258% \n",
      "[epoch:64, iter:6110] Loss: 0.026 | Acc: 98.286% \n",
      "[epoch:64, iter:6111] Loss: 0.026 | Acc: 98.214% \n",
      "[epoch:64, iter:6112] Loss: 0.028 | Acc: 98.145% \n",
      "[epoch:64, iter:6113] Loss: 0.028 | Acc: 98.173% \n",
      "[epoch:64, iter:6114] Loss: 0.028 | Acc: 98.201% \n",
      "[epoch:64, iter:6115] Loss: 0.027 | Acc: 98.228% \n",
      "[epoch:64, iter:6116] Loss: 0.027 | Acc: 98.254% \n",
      "[epoch:64, iter:6117] Loss: 0.027 | Acc: 98.279% \n",
      "[epoch:64, iter:6118] Loss: 0.027 | Acc: 98.214% \n",
      "[epoch:64, iter:6119] Loss: 0.027 | Acc: 98.239% \n",
      "[epoch:64, iter:6120] Loss: 0.028 | Acc: 98.177% \n",
      "[epoch:64, iter:6121] Loss: 0.027 | Acc: 98.202% \n",
      "[epoch:64, iter:6122] Loss: 0.027 | Acc: 98.226% \n",
      "[epoch:64, iter:6123] Loss: 0.027 | Acc: 98.250% \n",
      "[epoch:64, iter:6124] Loss: 0.027 | Acc: 98.273% \n",
      "[epoch:64, iter:6125] Loss: 0.027 | Acc: 98.295% \n",
      "[epoch:64, iter:6126] Loss: 0.030 | Acc: 98.237% \n",
      "[epoch:64, iter:6127] Loss: 0.030 | Acc: 98.259% \n",
      "[epoch:64, iter:6128] Loss: 0.030 | Acc: 98.281% \n",
      "[epoch:64, iter:6129] Loss: 0.030 | Acc: 98.225% \n",
      "[epoch:64, iter:6130] Loss: 0.030 | Acc: 98.247% \n",
      "[epoch:64, iter:6131] Loss: 0.030 | Acc: 98.193% \n",
      "[epoch:64, iter:6132] Loss: 0.030 | Acc: 98.214% \n",
      "[epoch:64, iter:6133] Loss: 0.030 | Acc: 98.235% \n",
      "[epoch:64, iter:6134] Loss: 0.030 | Acc: 98.256% \n",
      "[epoch:64, iter:6135] Loss: 0.029 | Acc: 98.276% \n",
      "[epoch:64, iter:6136] Loss: 0.030 | Acc: 98.224% \n",
      "[epoch:64, iter:6137] Loss: 0.029 | Acc: 98.244% \n",
      "[epoch:64, iter:6138] Loss: 0.029 | Acc: 98.264% \n",
      "[epoch:64, iter:6139] Loss: 0.031 | Acc: 98.214% \n",
      "[epoch:64, iter:6140] Loss: 0.031 | Acc: 98.234% \n",
      "[epoch:64, iter:6141] Loss: 0.031 | Acc: 98.253% \n",
      "[epoch:64, iter:6142] Loss: 0.030 | Acc: 98.271% \n",
      "[epoch:64, iter:6143] Loss: 0.030 | Acc: 98.289% \n",
      "[epoch:64, iter:6144] Loss: 0.030 | Acc: 98.300% \n",
      "Waiting Test!\n",
      "测试分类准确率为：73.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 65\n",
      "[epoch:65, iter:6145] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:65, iter:6146] Loss: 0.005 | Acc: 100.000% \n",
      "[epoch:65, iter:6147] Loss: 0.003 | Acc: 100.000% \n",
      "[epoch:65, iter:6148] Loss: 0.021 | Acc: 98.438% \n",
      "[epoch:65, iter:6149] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:65, iter:6150] Loss: 0.019 | Acc: 98.958% \n",
      "[epoch:65, iter:6151] Loss: 0.030 | Acc: 98.214% \n",
      "[epoch:65, iter:6152] Loss: 0.026 | Acc: 98.438% \n",
      "[epoch:65, iter:6153] Loss: 0.027 | Acc: 98.611% \n",
      "[epoch:65, iter:6154] Loss: 0.027 | Acc: 98.750% \n",
      "[epoch:65, iter:6155] Loss: 0.025 | Acc: 98.864% \n",
      "[epoch:65, iter:6156] Loss: 0.026 | Acc: 98.958% \n",
      "[epoch:65, iter:6157] Loss: 0.029 | Acc: 98.558% \n",
      "[epoch:65, iter:6158] Loss: 0.034 | Acc: 98.214% \n",
      "[epoch:65, iter:6159] Loss: 0.032 | Acc: 98.333% \n",
      "[epoch:65, iter:6160] Loss: 0.030 | Acc: 98.438% \n",
      "[epoch:65, iter:6161] Loss: 0.029 | Acc: 98.529% \n",
      "[epoch:65, iter:6162] Loss: 0.027 | Acc: 98.611% \n",
      "[epoch:65, iter:6163] Loss: 0.026 | Acc: 98.684% \n",
      "[epoch:65, iter:6164] Loss: 0.034 | Acc: 98.438% \n",
      "[epoch:65, iter:6165] Loss: 0.032 | Acc: 98.512% \n",
      "[epoch:65, iter:6166] Loss: 0.031 | Acc: 98.580% \n",
      "[epoch:65, iter:6167] Loss: 0.032 | Acc: 98.641% \n",
      "[epoch:65, iter:6168] Loss: 0.031 | Acc: 98.698% \n",
      "[epoch:65, iter:6169] Loss: 0.030 | Acc: 98.750% \n",
      "[epoch:65, iter:6170] Loss: 0.031 | Acc: 98.558% \n",
      "[epoch:65, iter:6171] Loss: 0.031 | Acc: 98.611% \n",
      "[epoch:65, iter:6172] Loss: 0.030 | Acc: 98.661% \n",
      "[epoch:65, iter:6173] Loss: 0.029 | Acc: 98.707% \n",
      "[epoch:65, iter:6174] Loss: 0.031 | Acc: 98.542% \n",
      "[epoch:65, iter:6175] Loss: 0.031 | Acc: 98.589% \n",
      "[epoch:65, iter:6176] Loss: 0.030 | Acc: 98.633% \n",
      "[epoch:65, iter:6177] Loss: 0.029 | Acc: 98.674% \n",
      "[epoch:65, iter:6178] Loss: 0.029 | Acc: 98.713% \n",
      "[epoch:65, iter:6179] Loss: 0.028 | Acc: 98.750% \n",
      "[epoch:65, iter:6180] Loss: 0.031 | Acc: 98.611% \n",
      "[epoch:65, iter:6181] Loss: 0.030 | Acc: 98.649% \n",
      "[epoch:65, iter:6182] Loss: 0.030 | Acc: 98.684% \n",
      "[epoch:65, iter:6183] Loss: 0.029 | Acc: 98.718% \n",
      "[epoch:65, iter:6184] Loss: 0.029 | Acc: 98.750% \n",
      "[epoch:65, iter:6185] Loss: 0.029 | Acc: 98.780% \n",
      "[epoch:65, iter:6186] Loss: 0.030 | Acc: 98.661% \n",
      "[epoch:65, iter:6187] Loss: 0.030 | Acc: 98.692% \n",
      "[epoch:65, iter:6188] Loss: 0.029 | Acc: 98.722% \n",
      "[epoch:65, iter:6189] Loss: 0.030 | Acc: 98.611% \n",
      "[epoch:65, iter:6190] Loss: 0.030 | Acc: 98.505% \n",
      "[epoch:65, iter:6191] Loss: 0.030 | Acc: 98.537% \n",
      "[epoch:65, iter:6192] Loss: 0.029 | Acc: 98.568% \n",
      "[epoch:65, iter:6193] Loss: 0.029 | Acc: 98.597% \n",
      "[epoch:65, iter:6194] Loss: 0.028 | Acc: 98.625% \n",
      "[epoch:65, iter:6195] Loss: 0.028 | Acc: 98.652% \n",
      "[epoch:65, iter:6196] Loss: 0.027 | Acc: 98.678% \n",
      "[epoch:65, iter:6197] Loss: 0.027 | Acc: 98.703% \n",
      "[epoch:65, iter:6198] Loss: 0.026 | Acc: 98.727% \n",
      "[epoch:65, iter:6199] Loss: 0.027 | Acc: 98.636% \n",
      "[epoch:65, iter:6200] Loss: 0.027 | Acc: 98.549% \n",
      "[epoch:65, iter:6201] Loss: 0.027 | Acc: 98.575% \n",
      "[epoch:65, iter:6202] Loss: 0.027 | Acc: 98.599% \n",
      "[epoch:65, iter:6203] Loss: 0.026 | Acc: 98.623% \n",
      "[epoch:65, iter:6204] Loss: 0.026 | Acc: 98.646% \n",
      "[epoch:65, iter:6205] Loss: 0.027 | Acc: 98.566% \n",
      "[epoch:65, iter:6206] Loss: 0.028 | Acc: 98.488% \n",
      "[epoch:65, iter:6207] Loss: 0.029 | Acc: 98.413% \n",
      "[epoch:65, iter:6208] Loss: 0.031 | Acc: 98.340% \n",
      "[epoch:65, iter:6209] Loss: 0.030 | Acc: 98.365% \n",
      "[epoch:65, iter:6210] Loss: 0.030 | Acc: 98.390% \n",
      "[epoch:65, iter:6211] Loss: 0.029 | Acc: 98.414% \n",
      "[epoch:65, iter:6212] Loss: 0.029 | Acc: 98.438% \n",
      "[epoch:65, iter:6213] Loss: 0.030 | Acc: 98.370% \n",
      "[epoch:65, iter:6214] Loss: 0.029 | Acc: 98.393% \n",
      "[epoch:65, iter:6215] Loss: 0.029 | Acc: 98.415% \n",
      "[epoch:65, iter:6216] Loss: 0.030 | Acc: 98.351% \n",
      "[epoch:65, iter:6217] Loss: 0.030 | Acc: 98.373% \n",
      "[epoch:65, iter:6218] Loss: 0.030 | Acc: 98.311% \n",
      "[epoch:65, iter:6219] Loss: 0.030 | Acc: 98.333% \n",
      "[epoch:65, iter:6220] Loss: 0.030 | Acc: 98.355% \n",
      "[epoch:65, iter:6221] Loss: 0.030 | Acc: 98.377% \n",
      "[epoch:65, iter:6222] Loss: 0.031 | Acc: 98.237% \n",
      "[epoch:65, iter:6223] Loss: 0.031 | Acc: 98.259% \n",
      "[epoch:65, iter:6224] Loss: 0.030 | Acc: 98.281% \n",
      "[epoch:65, iter:6225] Loss: 0.031 | Acc: 98.225% \n",
      "[epoch:65, iter:6226] Loss: 0.030 | Acc: 98.247% \n",
      "[epoch:65, iter:6227] Loss: 0.031 | Acc: 98.193% \n",
      "[epoch:65, iter:6228] Loss: 0.031 | Acc: 98.214% \n",
      "[epoch:65, iter:6229] Loss: 0.030 | Acc: 98.235% \n",
      "[epoch:65, iter:6230] Loss: 0.030 | Acc: 98.256% \n",
      "[epoch:65, iter:6231] Loss: 0.030 | Acc: 98.276% \n",
      "[epoch:65, iter:6232] Loss: 0.031 | Acc: 98.224% \n",
      "[epoch:65, iter:6233] Loss: 0.030 | Acc: 98.244% \n",
      "[epoch:65, iter:6234] Loss: 0.033 | Acc: 98.194% \n",
      "[epoch:65, iter:6235] Loss: 0.032 | Acc: 98.214% \n",
      "[epoch:65, iter:6236] Loss: 0.032 | Acc: 98.234% \n",
      "[epoch:65, iter:6237] Loss: 0.032 | Acc: 98.253% \n",
      "[epoch:65, iter:6238] Loss: 0.032 | Acc: 98.205% \n",
      "[epoch:65, iter:6239] Loss: 0.032 | Acc: 98.224% \n",
      "[epoch:65, iter:6240] Loss: 0.032 | Acc: 98.234% \n",
      "Waiting Test!\n",
      "测试分类准确率为：82.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 66\n",
      "[epoch:66, iter:6241] Loss: 0.046 | Acc: 93.750% \n",
      "[epoch:66, iter:6242] Loss: 0.023 | Acc: 96.875% \n",
      "[epoch:66, iter:6243] Loss: 0.016 | Acc: 97.917% \n",
      "[epoch:66, iter:6244] Loss: 0.012 | Acc: 98.438% \n",
      "[epoch:66, iter:6245] Loss: 0.010 | Acc: 98.750% \n",
      "[epoch:66, iter:6246] Loss: 0.008 | Acc: 98.958% \n",
      "[epoch:66, iter:6247] Loss: 0.007 | Acc: 99.107% \n",
      "[epoch:66, iter:6248] Loss: 0.006 | Acc: 99.219% \n",
      "[epoch:66, iter:6249] Loss: 0.007 | Acc: 99.306% \n",
      "[epoch:66, iter:6250] Loss: 0.006 | Acc: 99.375% \n",
      "[epoch:66, iter:6251] Loss: 0.006 | Acc: 99.432% \n",
      "[epoch:66, iter:6252] Loss: 0.006 | Acc: 99.479% \n",
      "[epoch:66, iter:6253] Loss: 0.010 | Acc: 99.038% \n",
      "[epoch:66, iter:6254] Loss: 0.009 | Acc: 99.107% \n",
      "[epoch:66, iter:6255] Loss: 0.009 | Acc: 99.167% \n",
      "[epoch:66, iter:6256] Loss: 0.010 | Acc: 99.219% \n",
      "[epoch:66, iter:6257] Loss: 0.010 | Acc: 99.265% \n",
      "[epoch:66, iter:6258] Loss: 0.010 | Acc: 99.306% \n",
      "[epoch:66, iter:6259] Loss: 0.009 | Acc: 99.342% \n",
      "[epoch:66, iter:6260] Loss: 0.011 | Acc: 99.375% \n",
      "[epoch:66, iter:6261] Loss: 0.012 | Acc: 99.107% \n",
      "[epoch:66, iter:6262] Loss: 0.014 | Acc: 98.864% \n",
      "[epoch:66, iter:6263] Loss: 0.013 | Acc: 98.913% \n",
      "[epoch:66, iter:6264] Loss: 0.013 | Acc: 98.958% \n",
      "[epoch:66, iter:6265] Loss: 0.017 | Acc: 98.500% \n",
      "[epoch:66, iter:6266] Loss: 0.017 | Acc: 98.558% \n",
      "[epoch:66, iter:6267] Loss: 0.017 | Acc: 98.611% \n",
      "[epoch:66, iter:6268] Loss: 0.019 | Acc: 98.438% \n",
      "[epoch:66, iter:6269] Loss: 0.018 | Acc: 98.491% \n",
      "[epoch:66, iter:6270] Loss: 0.019 | Acc: 98.542% \n",
      "[epoch:66, iter:6271] Loss: 0.019 | Acc: 98.589% \n",
      "[epoch:66, iter:6272] Loss: 0.023 | Acc: 98.438% \n",
      "[epoch:66, iter:6273] Loss: 0.024 | Acc: 98.295% \n",
      "[epoch:66, iter:6274] Loss: 0.024 | Acc: 98.162% \n",
      "[epoch:66, iter:6275] Loss: 0.024 | Acc: 98.214% \n",
      "[epoch:66, iter:6276] Loss: 0.024 | Acc: 98.264% \n",
      "[epoch:66, iter:6277] Loss: 0.023 | Acc: 98.311% \n",
      "[epoch:66, iter:6278] Loss: 0.022 | Acc: 98.355% \n",
      "[epoch:66, iter:6279] Loss: 0.022 | Acc: 98.397% \n",
      "[epoch:66, iter:6280] Loss: 0.021 | Acc: 98.438% \n",
      "[epoch:66, iter:6281] Loss: 0.021 | Acc: 98.476% \n",
      "[epoch:66, iter:6282] Loss: 0.020 | Acc: 98.512% \n",
      "[epoch:66, iter:6283] Loss: 0.020 | Acc: 98.547% \n",
      "[epoch:66, iter:6284] Loss: 0.021 | Acc: 98.438% \n",
      "[epoch:66, iter:6285] Loss: 0.020 | Acc: 98.472% \n",
      "[epoch:66, iter:6286] Loss: 0.020 | Acc: 98.505% \n",
      "[epoch:66, iter:6287] Loss: 0.020 | Acc: 98.537% \n",
      "[epoch:66, iter:6288] Loss: 0.019 | Acc: 98.568% \n",
      "[epoch:66, iter:6289] Loss: 0.020 | Acc: 98.597% \n",
      "[epoch:66, iter:6290] Loss: 0.020 | Acc: 98.625% \n",
      "[epoch:66, iter:6291] Loss: 0.021 | Acc: 98.529% \n",
      "[epoch:66, iter:6292] Loss: 0.020 | Acc: 98.558% \n",
      "[epoch:66, iter:6293] Loss: 0.020 | Acc: 98.585% \n",
      "[epoch:66, iter:6294] Loss: 0.021 | Acc: 98.495% \n",
      "[epoch:66, iter:6295] Loss: 0.023 | Acc: 98.295% \n",
      "[epoch:66, iter:6296] Loss: 0.023 | Acc: 98.326% \n",
      "[epoch:66, iter:6297] Loss: 0.023 | Acc: 98.355% \n",
      "[epoch:66, iter:6298] Loss: 0.022 | Acc: 98.384% \n",
      "[epoch:66, iter:6299] Loss: 0.022 | Acc: 98.411% \n",
      "[epoch:66, iter:6300] Loss: 0.022 | Acc: 98.438% \n",
      "[epoch:66, iter:6301] Loss: 0.021 | Acc: 98.463% \n",
      "[epoch:66, iter:6302] Loss: 0.021 | Acc: 98.488% \n",
      "[epoch:66, iter:6303] Loss: 0.021 | Acc: 98.413% \n",
      "[epoch:66, iter:6304] Loss: 0.023 | Acc: 98.242% \n",
      "[epoch:66, iter:6305] Loss: 0.023 | Acc: 98.269% \n",
      "[epoch:66, iter:6306] Loss: 0.023 | Acc: 98.295% \n",
      "[epoch:66, iter:6307] Loss: 0.023 | Acc: 98.228% \n",
      "[epoch:66, iter:6308] Loss: 0.023 | Acc: 98.254% \n",
      "[epoch:66, iter:6309] Loss: 0.022 | Acc: 98.279% \n",
      "[epoch:66, iter:6310] Loss: 0.022 | Acc: 98.304% \n",
      "[epoch:66, iter:6311] Loss: 0.022 | Acc: 98.327% \n",
      "[epoch:66, iter:6312] Loss: 0.022 | Acc: 98.351% \n",
      "[epoch:66, iter:6313] Loss: 0.022 | Acc: 98.373% \n",
      "[epoch:66, iter:6314] Loss: 0.022 | Acc: 98.311% \n",
      "[epoch:66, iter:6315] Loss: 0.023 | Acc: 98.333% \n",
      "[epoch:66, iter:6316] Loss: 0.022 | Acc: 98.355% \n",
      "[epoch:66, iter:6317] Loss: 0.023 | Acc: 98.295% \n",
      "[epoch:66, iter:6318] Loss: 0.023 | Acc: 98.317% \n",
      "[epoch:66, iter:6319] Loss: 0.023 | Acc: 98.259% \n",
      "[epoch:66, iter:6320] Loss: 0.023 | Acc: 98.281% \n",
      "[epoch:66, iter:6321] Loss: 0.022 | Acc: 98.302% \n",
      "[epoch:66, iter:6322] Loss: 0.022 | Acc: 98.323% \n",
      "[epoch:66, iter:6323] Loss: 0.022 | Acc: 98.343% \n",
      "[epoch:66, iter:6324] Loss: 0.022 | Acc: 98.363% \n",
      "[epoch:66, iter:6325] Loss: 0.021 | Acc: 98.382% \n",
      "[epoch:66, iter:6326] Loss: 0.022 | Acc: 98.401% \n",
      "[epoch:66, iter:6327] Loss: 0.021 | Acc: 98.420% \n",
      "[epoch:66, iter:6328] Loss: 0.021 | Acc: 98.438% \n",
      "[epoch:66, iter:6329] Loss: 0.021 | Acc: 98.455% \n",
      "[epoch:66, iter:6330] Loss: 0.021 | Acc: 98.403% \n",
      "[epoch:66, iter:6331] Loss: 0.022 | Acc: 98.352% \n",
      "[epoch:66, iter:6332] Loss: 0.022 | Acc: 98.302% \n",
      "[epoch:66, iter:6333] Loss: 0.022 | Acc: 98.320% \n",
      "[epoch:66, iter:6334] Loss: 0.022 | Acc: 98.338% \n",
      "[epoch:66, iter:6335] Loss: 0.022 | Acc: 98.355% \n",
      "[epoch:66, iter:6336] Loss: 0.021 | Acc: 98.365% \n",
      "Waiting Test!\n",
      "测试分类准确率为：87.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 67\n",
      "[epoch:67, iter:6337] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:67, iter:6338] Loss: 0.020 | Acc: 100.000% \n",
      "[epoch:67, iter:6339] Loss: 0.029 | Acc: 97.917% \n",
      "[epoch:67, iter:6340] Loss: 0.038 | Acc: 98.438% \n",
      "[epoch:67, iter:6341] Loss: 0.030 | Acc: 98.750% \n",
      "[epoch:67, iter:6342] Loss: 0.025 | Acc: 98.958% \n",
      "[epoch:67, iter:6343] Loss: 0.022 | Acc: 99.107% \n",
      "[epoch:67, iter:6344] Loss: 0.019 | Acc: 99.219% \n",
      "[epoch:67, iter:6345] Loss: 0.026 | Acc: 99.306% \n",
      "[epoch:67, iter:6346] Loss: 0.024 | Acc: 99.375% \n",
      "[epoch:67, iter:6347] Loss: 0.021 | Acc: 99.432% \n",
      "[epoch:67, iter:6348] Loss: 0.020 | Acc: 99.479% \n",
      "[epoch:67, iter:6349] Loss: 0.035 | Acc: 99.038% \n",
      "[epoch:67, iter:6350] Loss: 0.041 | Acc: 98.661% \n",
      "[epoch:67, iter:6351] Loss: 0.038 | Acc: 98.750% \n",
      "[epoch:67, iter:6352] Loss: 0.036 | Acc: 98.828% \n",
      "[epoch:67, iter:6353] Loss: 0.033 | Acc: 98.897% \n",
      "[epoch:67, iter:6354] Loss: 0.033 | Acc: 98.958% \n",
      "[epoch:67, iter:6355] Loss: 0.031 | Acc: 99.013% \n",
      "[epoch:67, iter:6356] Loss: 0.033 | Acc: 99.062% \n",
      "[epoch:67, iter:6357] Loss: 0.033 | Acc: 99.107% \n",
      "[epoch:67, iter:6358] Loss: 0.031 | Acc: 99.148% \n",
      "[epoch:67, iter:6359] Loss: 0.032 | Acc: 98.913% \n",
      "[epoch:67, iter:6360] Loss: 0.031 | Acc: 98.958% \n",
      "[epoch:67, iter:6361] Loss: 0.030 | Acc: 99.000% \n",
      "[epoch:67, iter:6362] Loss: 0.029 | Acc: 99.038% \n",
      "[epoch:67, iter:6363] Loss: 0.028 | Acc: 99.074% \n",
      "[epoch:67, iter:6364] Loss: 0.032 | Acc: 98.884% \n",
      "[epoch:67, iter:6365] Loss: 0.032 | Acc: 98.707% \n",
      "[epoch:67, iter:6366] Loss: 0.032 | Acc: 98.750% \n",
      "[epoch:67, iter:6367] Loss: 0.033 | Acc: 98.589% \n",
      "[epoch:67, iter:6368] Loss: 0.032 | Acc: 98.633% \n",
      "[epoch:67, iter:6369] Loss: 0.032 | Acc: 98.485% \n",
      "[epoch:67, iter:6370] Loss: 0.033 | Acc: 98.346% \n",
      "[epoch:67, iter:6371] Loss: 0.032 | Acc: 98.393% \n",
      "[epoch:67, iter:6372] Loss: 0.031 | Acc: 98.438% \n",
      "[epoch:67, iter:6373] Loss: 0.031 | Acc: 98.480% \n",
      "[epoch:67, iter:6374] Loss: 0.030 | Acc: 98.520% \n",
      "[epoch:67, iter:6375] Loss: 0.029 | Acc: 98.558% \n",
      "[epoch:67, iter:6376] Loss: 0.030 | Acc: 98.438% \n",
      "[epoch:67, iter:6377] Loss: 0.030 | Acc: 98.476% \n",
      "[epoch:67, iter:6378] Loss: 0.031 | Acc: 98.363% \n",
      "[epoch:67, iter:6379] Loss: 0.030 | Acc: 98.401% \n",
      "[epoch:67, iter:6380] Loss: 0.029 | Acc: 98.438% \n",
      "[epoch:67, iter:6381] Loss: 0.030 | Acc: 98.333% \n",
      "[epoch:67, iter:6382] Loss: 0.029 | Acc: 98.370% \n",
      "[epoch:67, iter:6383] Loss: 0.029 | Acc: 98.404% \n",
      "[epoch:67, iter:6384] Loss: 0.030 | Acc: 98.307% \n",
      "[epoch:67, iter:6385] Loss: 0.029 | Acc: 98.342% \n",
      "[epoch:67, iter:6386] Loss: 0.029 | Acc: 98.375% \n",
      "[epoch:67, iter:6387] Loss: 0.028 | Acc: 98.407% \n",
      "[epoch:67, iter:6388] Loss: 0.027 | Acc: 98.438% \n",
      "[epoch:67, iter:6389] Loss: 0.028 | Acc: 98.349% \n",
      "[epoch:67, iter:6390] Loss: 0.028 | Acc: 98.380% \n",
      "[epoch:67, iter:6391] Loss: 0.027 | Acc: 98.409% \n",
      "[epoch:67, iter:6392] Loss: 0.028 | Acc: 98.326% \n",
      "[epoch:67, iter:6393] Loss: 0.029 | Acc: 98.246% \n",
      "[epoch:67, iter:6394] Loss: 0.028 | Acc: 98.276% \n",
      "[epoch:67, iter:6395] Loss: 0.028 | Acc: 98.305% \n",
      "[epoch:67, iter:6396] Loss: 0.028 | Acc: 98.333% \n",
      "[epoch:67, iter:6397] Loss: 0.027 | Acc: 98.361% \n",
      "[epoch:67, iter:6398] Loss: 0.027 | Acc: 98.387% \n",
      "[epoch:67, iter:6399] Loss: 0.026 | Acc: 98.413% \n",
      "[epoch:67, iter:6400] Loss: 0.026 | Acc: 98.438% \n",
      "[epoch:67, iter:6401] Loss: 0.026 | Acc: 98.462% \n",
      "[epoch:67, iter:6402] Loss: 0.026 | Acc: 98.485% \n",
      "[epoch:67, iter:6403] Loss: 0.025 | Acc: 98.507% \n",
      "[epoch:67, iter:6404] Loss: 0.025 | Acc: 98.529% \n",
      "[epoch:67, iter:6405] Loss: 0.025 | Acc: 98.551% \n",
      "[epoch:67, iter:6406] Loss: 0.025 | Acc: 98.571% \n",
      "[epoch:67, iter:6407] Loss: 0.025 | Acc: 98.592% \n",
      "[epoch:67, iter:6408] Loss: 0.024 | Acc: 98.611% \n",
      "[epoch:67, iter:6409] Loss: 0.024 | Acc: 98.630% \n",
      "[epoch:67, iter:6410] Loss: 0.024 | Acc: 98.649% \n",
      "[epoch:67, iter:6411] Loss: 0.024 | Acc: 98.583% \n",
      "[epoch:67, iter:6412] Loss: 0.024 | Acc: 98.602% \n",
      "[epoch:67, iter:6413] Loss: 0.024 | Acc: 98.620% \n",
      "[epoch:67, iter:6414] Loss: 0.024 | Acc: 98.638% \n",
      "[epoch:67, iter:6415] Loss: 0.024 | Acc: 98.655% \n",
      "[epoch:67, iter:6416] Loss: 0.025 | Acc: 98.594% \n",
      "[epoch:67, iter:6417] Loss: 0.025 | Acc: 98.611% \n",
      "[epoch:67, iter:6418] Loss: 0.024 | Acc: 98.628% \n",
      "[epoch:67, iter:6419] Loss: 0.024 | Acc: 98.645% \n",
      "[epoch:67, iter:6420] Loss: 0.024 | Acc: 98.661% \n",
      "[epoch:67, iter:6421] Loss: 0.027 | Acc: 98.529% \n",
      "[epoch:67, iter:6422] Loss: 0.027 | Acc: 98.547% \n",
      "[epoch:67, iter:6423] Loss: 0.027 | Acc: 98.491% \n",
      "[epoch:67, iter:6424] Loss: 0.027 | Acc: 98.509% \n",
      "[epoch:67, iter:6425] Loss: 0.029 | Acc: 98.455% \n",
      "[epoch:67, iter:6426] Loss: 0.029 | Acc: 98.472% \n",
      "[epoch:67, iter:6427] Loss: 0.029 | Acc: 98.489% \n",
      "[epoch:67, iter:6428] Loss: 0.029 | Acc: 98.505% \n",
      "[epoch:67, iter:6429] Loss: 0.028 | Acc: 98.522% \n",
      "[epoch:67, iter:6430] Loss: 0.029 | Acc: 98.471% \n",
      "[epoch:67, iter:6431] Loss: 0.029 | Acc: 98.487% \n",
      "[epoch:67, iter:6432] Loss: 0.029 | Acc: 98.496% \n",
      "Waiting Test!\n",
      "测试分类准确率为：81.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 68\n",
      "[epoch:68, iter:6433] Loss: 0.024 | Acc: 100.000% \n",
      "[epoch:68, iter:6434] Loss: 0.013 | Acc: 100.000% \n",
      "[epoch:68, iter:6435] Loss: 0.029 | Acc: 100.000% \n",
      "[epoch:68, iter:6436] Loss: 0.023 | Acc: 100.000% \n",
      "[epoch:68, iter:6437] Loss: 0.020 | Acc: 100.000% \n",
      "[epoch:68, iter:6438] Loss: 0.029 | Acc: 100.000% \n",
      "[epoch:68, iter:6439] Loss: 0.025 | Acc: 100.000% \n",
      "[epoch:68, iter:6440] Loss: 0.023 | Acc: 100.000% \n",
      "[epoch:68, iter:6441] Loss: 0.026 | Acc: 100.000% \n",
      "[epoch:68, iter:6442] Loss: 0.026 | Acc: 100.000% \n",
      "[epoch:68, iter:6443] Loss: 0.027 | Acc: 100.000% \n",
      "[epoch:68, iter:6444] Loss: 0.025 | Acc: 100.000% \n",
      "[epoch:68, iter:6445] Loss: 0.024 | Acc: 100.000% \n",
      "[epoch:68, iter:6446] Loss: 0.028 | Acc: 99.554% \n",
      "[epoch:68, iter:6447] Loss: 0.027 | Acc: 99.583% \n",
      "[epoch:68, iter:6448] Loss: 0.027 | Acc: 99.609% \n",
      "[epoch:68, iter:6449] Loss: 0.032 | Acc: 98.897% \n",
      "[epoch:68, iter:6450] Loss: 0.032 | Acc: 98.958% \n",
      "[epoch:68, iter:6451] Loss: 0.033 | Acc: 98.684% \n",
      "[epoch:68, iter:6452] Loss: 0.032 | Acc: 98.750% \n",
      "[epoch:68, iter:6453] Loss: 0.036 | Acc: 98.512% \n",
      "[epoch:68, iter:6454] Loss: 0.036 | Acc: 98.580% \n",
      "[epoch:68, iter:6455] Loss: 0.034 | Acc: 98.641% \n",
      "[epoch:68, iter:6456] Loss: 0.033 | Acc: 98.698% \n",
      "[epoch:68, iter:6457] Loss: 0.034 | Acc: 98.750% \n",
      "[epoch:68, iter:6458] Loss: 0.033 | Acc: 98.798% \n",
      "[epoch:68, iter:6459] Loss: 0.031 | Acc: 98.843% \n",
      "[epoch:68, iter:6460] Loss: 0.031 | Acc: 98.884% \n",
      "[epoch:68, iter:6461] Loss: 0.030 | Acc: 98.922% \n",
      "[epoch:68, iter:6462] Loss: 0.029 | Acc: 98.958% \n",
      "[epoch:68, iter:6463] Loss: 0.028 | Acc: 98.992% \n",
      "[epoch:68, iter:6464] Loss: 0.028 | Acc: 99.023% \n",
      "[epoch:68, iter:6465] Loss: 0.028 | Acc: 99.053% \n",
      "[epoch:68, iter:6466] Loss: 0.033 | Acc: 98.713% \n",
      "[epoch:68, iter:6467] Loss: 0.036 | Acc: 98.393% \n",
      "[epoch:68, iter:6468] Loss: 0.037 | Acc: 98.438% \n",
      "[epoch:68, iter:6469] Loss: 0.037 | Acc: 98.480% \n",
      "[epoch:68, iter:6470] Loss: 0.037 | Acc: 98.520% \n",
      "[epoch:68, iter:6471] Loss: 0.036 | Acc: 98.558% \n",
      "[epoch:68, iter:6472] Loss: 0.035 | Acc: 98.594% \n",
      "[epoch:68, iter:6473] Loss: 0.035 | Acc: 98.628% \n",
      "[epoch:68, iter:6474] Loss: 0.035 | Acc: 98.661% \n",
      "[epoch:68, iter:6475] Loss: 0.034 | Acc: 98.692% \n",
      "[epoch:68, iter:6476] Loss: 0.034 | Acc: 98.580% \n",
      "[epoch:68, iter:6477] Loss: 0.034 | Acc: 98.611% \n",
      "[epoch:68, iter:6478] Loss: 0.034 | Acc: 98.641% \n",
      "[epoch:68, iter:6479] Loss: 0.033 | Acc: 98.670% \n",
      "[epoch:68, iter:6480] Loss: 0.036 | Acc: 98.568% \n",
      "[epoch:68, iter:6481] Loss: 0.035 | Acc: 98.597% \n",
      "[epoch:68, iter:6482] Loss: 0.034 | Acc: 98.625% \n",
      "[epoch:68, iter:6483] Loss: 0.037 | Acc: 98.529% \n",
      "[epoch:68, iter:6484] Loss: 0.036 | Acc: 98.558% \n",
      "[epoch:68, iter:6485] Loss: 0.036 | Acc: 98.585% \n",
      "[epoch:68, iter:6486] Loss: 0.035 | Acc: 98.611% \n",
      "[epoch:68, iter:6487] Loss: 0.035 | Acc: 98.636% \n",
      "[epoch:68, iter:6488] Loss: 0.035 | Acc: 98.549% \n",
      "[epoch:68, iter:6489] Loss: 0.035 | Acc: 98.575% \n",
      "[epoch:68, iter:6490] Loss: 0.034 | Acc: 98.599% \n",
      "[epoch:68, iter:6491] Loss: 0.034 | Acc: 98.623% \n",
      "[epoch:68, iter:6492] Loss: 0.033 | Acc: 98.646% \n",
      "[epoch:68, iter:6493] Loss: 0.034 | Acc: 98.566% \n",
      "[epoch:68, iter:6494] Loss: 0.033 | Acc: 98.589% \n",
      "[epoch:68, iter:6495] Loss: 0.033 | Acc: 98.611% \n",
      "[epoch:68, iter:6496] Loss: 0.032 | Acc: 98.633% \n",
      "[epoch:68, iter:6497] Loss: 0.035 | Acc: 98.462% \n",
      "[epoch:68, iter:6498] Loss: 0.035 | Acc: 98.485% \n",
      "[epoch:68, iter:6499] Loss: 0.035 | Acc: 98.507% \n",
      "[epoch:68, iter:6500] Loss: 0.035 | Acc: 98.438% \n",
      "[epoch:68, iter:6501] Loss: 0.035 | Acc: 98.460% \n",
      "[epoch:68, iter:6502] Loss: 0.034 | Acc: 98.482% \n",
      "[epoch:68, iter:6503] Loss: 0.034 | Acc: 98.504% \n",
      "[epoch:68, iter:6504] Loss: 0.034 | Acc: 98.524% \n",
      "[epoch:68, iter:6505] Loss: 0.034 | Acc: 98.459% \n",
      "[epoch:68, iter:6506] Loss: 0.034 | Acc: 98.480% \n",
      "[epoch:68, iter:6507] Loss: 0.034 | Acc: 98.500% \n",
      "[epoch:68, iter:6508] Loss: 0.034 | Acc: 98.520% \n",
      "[epoch:68, iter:6509] Loss: 0.034 | Acc: 98.539% \n",
      "[epoch:68, iter:6510] Loss: 0.034 | Acc: 98.558% \n",
      "[epoch:68, iter:6511] Loss: 0.034 | Acc: 98.497% \n",
      "[epoch:68, iter:6512] Loss: 0.035 | Acc: 98.438% \n",
      "[epoch:68, iter:6513] Loss: 0.034 | Acc: 98.457% \n",
      "[epoch:68, iter:6514] Loss: 0.035 | Acc: 98.399% \n",
      "[epoch:68, iter:6515] Loss: 0.035 | Acc: 98.419% \n",
      "[epoch:68, iter:6516] Loss: 0.034 | Acc: 98.438% \n",
      "[epoch:68, iter:6517] Loss: 0.037 | Acc: 98.309% \n",
      "[epoch:68, iter:6518] Loss: 0.036 | Acc: 98.328% \n",
      "[epoch:68, iter:6519] Loss: 0.036 | Acc: 98.348% \n",
      "[epoch:68, iter:6520] Loss: 0.036 | Acc: 98.366% \n",
      "[epoch:68, iter:6521] Loss: 0.036 | Acc: 98.315% \n",
      "[epoch:68, iter:6522] Loss: 0.037 | Acc: 98.264% \n",
      "[epoch:68, iter:6523] Loss: 0.037 | Acc: 98.283% \n",
      "[epoch:68, iter:6524] Loss: 0.036 | Acc: 98.302% \n",
      "[epoch:68, iter:6525] Loss: 0.036 | Acc: 98.320% \n",
      "[epoch:68, iter:6526] Loss: 0.037 | Acc: 98.205% \n",
      "[epoch:68, iter:6527] Loss: 0.036 | Acc: 98.224% \n",
      "[epoch:68, iter:6528] Loss: 0.036 | Acc: 98.234% \n",
      "Waiting Test!\n",
      "测试分类准确率为：80.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 69\n",
      "[epoch:69, iter:6529] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:69, iter:6530] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:69, iter:6531] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:69, iter:6532] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:69, iter:6533] Loss: 0.005 | Acc: 100.000% \n",
      "[epoch:69, iter:6534] Loss: 0.005 | Acc: 100.000% \n",
      "[epoch:69, iter:6535] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:69, iter:6536] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:69, iter:6537] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:69, iter:6538] Loss: 0.015 | Acc: 98.750% \n",
      "[epoch:69, iter:6539] Loss: 0.018 | Acc: 98.295% \n",
      "[epoch:69, iter:6540] Loss: 0.020 | Acc: 97.917% \n",
      "[epoch:69, iter:6541] Loss: 0.022 | Acc: 98.077% \n",
      "[epoch:69, iter:6542] Loss: 0.021 | Acc: 98.214% \n",
      "[epoch:69, iter:6543] Loss: 0.042 | Acc: 97.917% \n",
      "[epoch:69, iter:6544] Loss: 0.040 | Acc: 98.047% \n",
      "[epoch:69, iter:6545] Loss: 0.045 | Acc: 97.794% \n",
      "[epoch:69, iter:6546] Loss: 0.043 | Acc: 97.917% \n",
      "[epoch:69, iter:6547] Loss: 0.044 | Acc: 97.697% \n",
      "[epoch:69, iter:6548] Loss: 0.052 | Acc: 97.188% \n",
      "[epoch:69, iter:6549] Loss: 0.051 | Acc: 97.321% \n",
      "[epoch:69, iter:6550] Loss: 0.049 | Acc: 97.443% \n",
      "[epoch:69, iter:6551] Loss: 0.047 | Acc: 97.554% \n",
      "[epoch:69, iter:6552] Loss: 0.047 | Acc: 97.656% \n",
      "[epoch:69, iter:6553] Loss: 0.046 | Acc: 97.750% \n",
      "[epoch:69, iter:6554] Loss: 0.044 | Acc: 97.837% \n",
      "[epoch:69, iter:6555] Loss: 0.045 | Acc: 97.685% \n",
      "[epoch:69, iter:6556] Loss: 0.044 | Acc: 97.768% \n",
      "[epoch:69, iter:6557] Loss: 0.043 | Acc: 97.845% \n",
      "[epoch:69, iter:6558] Loss: 0.043 | Acc: 97.708% \n",
      "[epoch:69, iter:6559] Loss: 0.042 | Acc: 97.782% \n",
      "[epoch:69, iter:6560] Loss: 0.041 | Acc: 97.852% \n",
      "[epoch:69, iter:6561] Loss: 0.041 | Acc: 97.917% \n",
      "[epoch:69, iter:6562] Loss: 0.045 | Acc: 97.610% \n",
      "[epoch:69, iter:6563] Loss: 0.044 | Acc: 97.679% \n",
      "[epoch:69, iter:6564] Loss: 0.044 | Acc: 97.743% \n",
      "[epoch:69, iter:6565] Loss: 0.044 | Acc: 97.804% \n",
      "[epoch:69, iter:6566] Loss: 0.044 | Acc: 97.697% \n",
      "[epoch:69, iter:6567] Loss: 0.061 | Acc: 97.596% \n",
      "[epoch:69, iter:6568] Loss: 0.061 | Acc: 97.500% \n",
      "[epoch:69, iter:6569] Loss: 0.060 | Acc: 97.561% \n",
      "[epoch:69, iter:6570] Loss: 0.059 | Acc: 97.619% \n",
      "[epoch:69, iter:6571] Loss: 0.057 | Acc: 97.674% \n",
      "[epoch:69, iter:6572] Loss: 0.058 | Acc: 97.585% \n",
      "[epoch:69, iter:6573] Loss: 0.057 | Acc: 97.639% \n",
      "[epoch:69, iter:6574] Loss: 0.056 | Acc: 97.690% \n",
      "[epoch:69, iter:6575] Loss: 0.055 | Acc: 97.739% \n",
      "[epoch:69, iter:6576] Loss: 0.055 | Acc: 97.656% \n",
      "[epoch:69, iter:6577] Loss: 0.056 | Acc: 97.577% \n",
      "[epoch:69, iter:6578] Loss: 0.056 | Acc: 97.625% \n",
      "[epoch:69, iter:6579] Loss: 0.056 | Acc: 97.672% \n",
      "[epoch:69, iter:6580] Loss: 0.055 | Acc: 97.716% \n",
      "[epoch:69, iter:6581] Loss: 0.055 | Acc: 97.759% \n",
      "[epoch:69, iter:6582] Loss: 0.054 | Acc: 97.801% \n",
      "[epoch:69, iter:6583] Loss: 0.053 | Acc: 97.841% \n",
      "[epoch:69, iter:6584] Loss: 0.052 | Acc: 97.879% \n",
      "[epoch:69, iter:6585] Loss: 0.052 | Acc: 97.917% \n",
      "[epoch:69, iter:6586] Loss: 0.051 | Acc: 97.953% \n",
      "[epoch:69, iter:6587] Loss: 0.050 | Acc: 97.987% \n",
      "[epoch:69, iter:6588] Loss: 0.050 | Acc: 98.021% \n",
      "[epoch:69, iter:6589] Loss: 0.051 | Acc: 97.951% \n",
      "[epoch:69, iter:6590] Loss: 0.050 | Acc: 97.984% \n",
      "[epoch:69, iter:6591] Loss: 0.050 | Acc: 98.016% \n",
      "[epoch:69, iter:6592] Loss: 0.050 | Acc: 97.949% \n",
      "[epoch:69, iter:6593] Loss: 0.049 | Acc: 97.981% \n",
      "[epoch:69, iter:6594] Loss: 0.048 | Acc: 98.011% \n",
      "[epoch:69, iter:6595] Loss: 0.048 | Acc: 98.041% \n",
      "[epoch:69, iter:6596] Loss: 0.047 | Acc: 98.070% \n",
      "[epoch:69, iter:6597] Loss: 0.046 | Acc: 98.098% \n",
      "[epoch:69, iter:6598] Loss: 0.048 | Acc: 98.036% \n",
      "[epoch:69, iter:6599] Loss: 0.048 | Acc: 97.975% \n",
      "[epoch:69, iter:6600] Loss: 0.048 | Acc: 98.003% \n",
      "[epoch:69, iter:6601] Loss: 0.049 | Acc: 97.945% \n",
      "[epoch:69, iter:6602] Loss: 0.048 | Acc: 97.973% \n",
      "[epoch:69, iter:6603] Loss: 0.048 | Acc: 98.000% \n",
      "[epoch:69, iter:6604] Loss: 0.047 | Acc: 98.026% \n",
      "[epoch:69, iter:6605] Loss: 0.049 | Acc: 97.890% \n",
      "[epoch:69, iter:6606] Loss: 0.049 | Acc: 97.917% \n",
      "[epoch:69, iter:6607] Loss: 0.048 | Acc: 97.943% \n",
      "[epoch:69, iter:6608] Loss: 0.048 | Acc: 97.969% \n",
      "[epoch:69, iter:6609] Loss: 0.048 | Acc: 97.840% \n",
      "[epoch:69, iter:6610] Loss: 0.048 | Acc: 97.866% \n",
      "[epoch:69, iter:6611] Loss: 0.048 | Acc: 97.816% \n",
      "[epoch:69, iter:6612] Loss: 0.048 | Acc: 97.768% \n",
      "[epoch:69, iter:6613] Loss: 0.048 | Acc: 97.794% \n",
      "[epoch:69, iter:6614] Loss: 0.048 | Acc: 97.747% \n",
      "[epoch:69, iter:6615] Loss: 0.048 | Acc: 97.773% \n",
      "[epoch:69, iter:6616] Loss: 0.047 | Acc: 97.798% \n",
      "[epoch:69, iter:6617] Loss: 0.047 | Acc: 97.823% \n",
      "[epoch:69, iter:6618] Loss: 0.049 | Acc: 97.778% \n",
      "[epoch:69, iter:6619] Loss: 0.049 | Acc: 97.734% \n",
      "[epoch:69, iter:6620] Loss: 0.048 | Acc: 97.758% \n",
      "[epoch:69, iter:6621] Loss: 0.048 | Acc: 97.782% \n",
      "[epoch:69, iter:6622] Loss: 0.048 | Acc: 97.806% \n",
      "[epoch:69, iter:6623] Loss: 0.047 | Acc: 97.829% \n",
      "[epoch:69, iter:6624] Loss: 0.047 | Acc: 97.842% \n",
      "Waiting Test!\n",
      "测试分类准确率为：85.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 70\n",
      "[epoch:70, iter:6625] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:70, iter:6626] Loss: 0.031 | Acc: 96.875% \n",
      "[epoch:70, iter:6627] Loss: 0.033 | Acc: 97.917% \n",
      "[epoch:70, iter:6628] Loss: 0.027 | Acc: 98.438% \n",
      "[epoch:70, iter:6629] Loss: 0.022 | Acc: 98.750% \n",
      "[epoch:70, iter:6630] Loss: 0.019 | Acc: 98.958% \n",
      "[epoch:70, iter:6631] Loss: 0.018 | Acc: 99.107% \n",
      "[epoch:70, iter:6632] Loss: 0.017 | Acc: 99.219% \n",
      "[epoch:70, iter:6633] Loss: 0.015 | Acc: 99.306% \n",
      "[epoch:70, iter:6634] Loss: 0.014 | Acc: 99.375% \n",
      "[epoch:70, iter:6635] Loss: 0.013 | Acc: 99.432% \n",
      "[epoch:70, iter:6636] Loss: 0.012 | Acc: 99.479% \n",
      "[epoch:70, iter:6637] Loss: 0.011 | Acc: 99.519% \n",
      "[epoch:70, iter:6638] Loss: 0.013 | Acc: 99.554% \n",
      "[epoch:70, iter:6639] Loss: 0.012 | Acc: 99.583% \n",
      "[epoch:70, iter:6640] Loss: 0.015 | Acc: 99.219% \n",
      "[epoch:70, iter:6641] Loss: 0.014 | Acc: 99.265% \n",
      "[epoch:70, iter:6642] Loss: 0.013 | Acc: 99.306% \n",
      "[epoch:70, iter:6643] Loss: 0.013 | Acc: 99.342% \n",
      "[epoch:70, iter:6644] Loss: 0.013 | Acc: 99.375% \n",
      "[epoch:70, iter:6645] Loss: 0.013 | Acc: 99.405% \n",
      "[epoch:70, iter:6646] Loss: 0.013 | Acc: 99.432% \n",
      "[epoch:70, iter:6647] Loss: 0.013 | Acc: 99.457% \n",
      "[epoch:70, iter:6648] Loss: 0.012 | Acc: 99.479% \n",
      "[epoch:70, iter:6649] Loss: 0.012 | Acc: 99.500% \n",
      "[epoch:70, iter:6650] Loss: 0.021 | Acc: 99.038% \n",
      "[epoch:70, iter:6651] Loss: 0.022 | Acc: 98.843% \n",
      "[epoch:70, iter:6652] Loss: 0.021 | Acc: 98.884% \n",
      "[epoch:70, iter:6653] Loss: 0.020 | Acc: 98.922% \n",
      "[epoch:70, iter:6654] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:70, iter:6655] Loss: 0.019 | Acc: 98.992% \n",
      "[epoch:70, iter:6656] Loss: 0.021 | Acc: 98.828% \n",
      "[epoch:70, iter:6657] Loss: 0.020 | Acc: 98.864% \n",
      "[epoch:70, iter:6658] Loss: 0.020 | Acc: 98.897% \n",
      "[epoch:70, iter:6659] Loss: 0.020 | Acc: 98.929% \n",
      "[epoch:70, iter:6660] Loss: 0.022 | Acc: 98.785% \n",
      "[epoch:70, iter:6661] Loss: 0.021 | Acc: 98.818% \n",
      "[epoch:70, iter:6662] Loss: 0.021 | Acc: 98.849% \n",
      "[epoch:70, iter:6663] Loss: 0.029 | Acc: 98.718% \n",
      "[epoch:70, iter:6664] Loss: 0.028 | Acc: 98.750% \n",
      "[epoch:70, iter:6665] Loss: 0.028 | Acc: 98.780% \n",
      "[epoch:70, iter:6666] Loss: 0.027 | Acc: 98.810% \n",
      "[epoch:70, iter:6667] Loss: 0.027 | Acc: 98.837% \n",
      "[epoch:70, iter:6668] Loss: 0.027 | Acc: 98.722% \n",
      "[epoch:70, iter:6669] Loss: 0.027 | Acc: 98.750% \n",
      "[epoch:70, iter:6670] Loss: 0.026 | Acc: 98.777% \n",
      "[epoch:70, iter:6671] Loss: 0.026 | Acc: 98.670% \n",
      "[epoch:70, iter:6672] Loss: 0.026 | Acc: 98.698% \n",
      "[epoch:70, iter:6673] Loss: 0.026 | Acc: 98.724% \n",
      "[epoch:70, iter:6674] Loss: 0.025 | Acc: 98.750% \n",
      "[epoch:70, iter:6675] Loss: 0.026 | Acc: 98.652% \n",
      "[epoch:70, iter:6676] Loss: 0.026 | Acc: 98.678% \n",
      "[epoch:70, iter:6677] Loss: 0.025 | Acc: 98.703% \n",
      "[epoch:70, iter:6678] Loss: 0.025 | Acc: 98.727% \n",
      "[epoch:70, iter:6679] Loss: 0.025 | Acc: 98.750% \n",
      "[epoch:70, iter:6680] Loss: 0.025 | Acc: 98.772% \n",
      "[epoch:70, iter:6681] Loss: 0.026 | Acc: 98.684% \n",
      "[epoch:70, iter:6682] Loss: 0.026 | Acc: 98.707% \n",
      "[epoch:70, iter:6683] Loss: 0.025 | Acc: 98.729% \n",
      "[epoch:70, iter:6684] Loss: 0.026 | Acc: 98.646% \n",
      "[epoch:70, iter:6685] Loss: 0.027 | Acc: 98.566% \n",
      "[epoch:70, iter:6686] Loss: 0.027 | Acc: 98.589% \n",
      "[epoch:70, iter:6687] Loss: 0.027 | Acc: 98.611% \n",
      "[epoch:70, iter:6688] Loss: 0.027 | Acc: 98.633% \n",
      "[epoch:70, iter:6689] Loss: 0.026 | Acc: 98.654% \n",
      "[epoch:70, iter:6690] Loss: 0.029 | Acc: 98.580% \n",
      "[epoch:70, iter:6691] Loss: 0.029 | Acc: 98.601% \n",
      "[epoch:70, iter:6692] Loss: 0.028 | Acc: 98.621% \n",
      "[epoch:70, iter:6693] Loss: 0.029 | Acc: 98.551% \n",
      "[epoch:70, iter:6694] Loss: 0.029 | Acc: 98.482% \n",
      "[epoch:70, iter:6695] Loss: 0.031 | Acc: 98.415% \n",
      "[epoch:70, iter:6696] Loss: 0.031 | Acc: 98.438% \n",
      "[epoch:70, iter:6697] Loss: 0.031 | Acc: 98.459% \n",
      "[epoch:70, iter:6698] Loss: 0.031 | Acc: 98.395% \n",
      "[epoch:70, iter:6699] Loss: 0.031 | Acc: 98.417% \n",
      "[epoch:70, iter:6700] Loss: 0.031 | Acc: 98.355% \n",
      "[epoch:70, iter:6701] Loss: 0.031 | Acc: 98.377% \n",
      "[epoch:70, iter:6702] Loss: 0.031 | Acc: 98.397% \n",
      "[epoch:70, iter:6703] Loss: 0.030 | Acc: 98.418% \n",
      "[epoch:70, iter:6704] Loss: 0.030 | Acc: 98.438% \n",
      "[epoch:70, iter:6705] Loss: 0.030 | Acc: 98.457% \n",
      "[epoch:70, iter:6706] Loss: 0.031 | Acc: 98.399% \n",
      "[epoch:70, iter:6707] Loss: 0.031 | Acc: 98.419% \n",
      "[epoch:70, iter:6708] Loss: 0.030 | Acc: 98.438% \n",
      "[epoch:70, iter:6709] Loss: 0.031 | Acc: 98.382% \n",
      "[epoch:70, iter:6710] Loss: 0.030 | Acc: 98.401% \n",
      "[epoch:70, iter:6711] Loss: 0.030 | Acc: 98.420% \n",
      "[epoch:70, iter:6712] Loss: 0.030 | Acc: 98.438% \n",
      "[epoch:70, iter:6713] Loss: 0.030 | Acc: 98.455% \n",
      "[epoch:70, iter:6714] Loss: 0.030 | Acc: 98.472% \n",
      "[epoch:70, iter:6715] Loss: 0.030 | Acc: 98.489% \n",
      "[epoch:70, iter:6716] Loss: 0.030 | Acc: 98.505% \n",
      "[epoch:70, iter:6717] Loss: 0.030 | Acc: 98.522% \n",
      "[epoch:70, iter:6718] Loss: 0.030 | Acc: 98.537% \n",
      "[epoch:70, iter:6719] Loss: 0.030 | Acc: 98.553% \n",
      "[epoch:70, iter:6720] Loss: 0.029 | Acc: 98.561% \n",
      "Waiting Test!\n",
      "测试分类准确率为：88.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 71\n",
      "[epoch:71, iter:6721] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:71, iter:6722] Loss: 0.024 | Acc: 96.875% \n",
      "[epoch:71, iter:6723] Loss: 0.017 | Acc: 97.917% \n",
      "[epoch:71, iter:6724] Loss: 0.013 | Acc: 98.438% \n",
      "[epoch:71, iter:6725] Loss: 0.011 | Acc: 98.750% \n",
      "[epoch:71, iter:6726] Loss: 0.022 | Acc: 98.958% \n",
      "[epoch:71, iter:6727] Loss: 0.021 | Acc: 99.107% \n",
      "[epoch:71, iter:6728] Loss: 0.019 | Acc: 99.219% \n",
      "[epoch:71, iter:6729] Loss: 0.017 | Acc: 99.306% \n",
      "[epoch:71, iter:6730] Loss: 0.023 | Acc: 98.750% \n",
      "[epoch:71, iter:6731] Loss: 0.025 | Acc: 98.295% \n",
      "[epoch:71, iter:6732] Loss: 0.023 | Acc: 98.438% \n",
      "[epoch:71, iter:6733] Loss: 0.021 | Acc: 98.558% \n",
      "[epoch:71, iter:6734] Loss: 0.020 | Acc: 98.661% \n",
      "[epoch:71, iter:6735] Loss: 0.019 | Acc: 98.750% \n",
      "[epoch:71, iter:6736] Loss: 0.018 | Acc: 98.828% \n",
      "[epoch:71, iter:6737] Loss: 0.017 | Acc: 98.897% \n",
      "[epoch:71, iter:6738] Loss: 0.016 | Acc: 98.958% \n",
      "[epoch:71, iter:6739] Loss: 0.018 | Acc: 98.684% \n",
      "[epoch:71, iter:6740] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:71, iter:6741] Loss: 0.019 | Acc: 98.512% \n",
      "[epoch:71, iter:6742] Loss: 0.022 | Acc: 98.295% \n",
      "[epoch:71, iter:6743] Loss: 0.023 | Acc: 98.098% \n",
      "[epoch:71, iter:6744] Loss: 0.022 | Acc: 98.177% \n",
      "[epoch:71, iter:6745] Loss: 0.022 | Acc: 98.250% \n",
      "[epoch:71, iter:6746] Loss: 0.021 | Acc: 98.317% \n",
      "[epoch:71, iter:6747] Loss: 0.020 | Acc: 98.380% \n",
      "[epoch:71, iter:6748] Loss: 0.020 | Acc: 98.438% \n",
      "[epoch:71, iter:6749] Loss: 0.021 | Acc: 98.276% \n",
      "[epoch:71, iter:6750] Loss: 0.020 | Acc: 98.333% \n",
      "[epoch:71, iter:6751] Loss: 0.022 | Acc: 98.185% \n",
      "[epoch:71, iter:6752] Loss: 0.021 | Acc: 98.242% \n",
      "[epoch:71, iter:6753] Loss: 0.024 | Acc: 97.917% \n",
      "[epoch:71, iter:6754] Loss: 0.024 | Acc: 97.978% \n",
      "[epoch:71, iter:6755] Loss: 0.023 | Acc: 98.036% \n",
      "[epoch:71, iter:6756] Loss: 0.023 | Acc: 98.090% \n",
      "[epoch:71, iter:6757] Loss: 0.022 | Acc: 98.142% \n",
      "[epoch:71, iter:6758] Loss: 0.022 | Acc: 98.191% \n",
      "[epoch:71, iter:6759] Loss: 0.021 | Acc: 98.237% \n",
      "[epoch:71, iter:6760] Loss: 0.021 | Acc: 98.281% \n",
      "[epoch:71, iter:6761] Loss: 0.020 | Acc: 98.323% \n",
      "[epoch:71, iter:6762] Loss: 0.020 | Acc: 98.363% \n",
      "[epoch:71, iter:6763] Loss: 0.019 | Acc: 98.401% \n",
      "[epoch:71, iter:6764] Loss: 0.019 | Acc: 98.438% \n",
      "[epoch:71, iter:6765] Loss: 0.018 | Acc: 98.472% \n",
      "[epoch:71, iter:6766] Loss: 0.018 | Acc: 98.505% \n",
      "[epoch:71, iter:6767] Loss: 0.018 | Acc: 98.537% \n",
      "[epoch:71, iter:6768] Loss: 0.017 | Acc: 98.568% \n",
      "[epoch:71, iter:6769] Loss: 0.017 | Acc: 98.597% \n",
      "[epoch:71, iter:6770] Loss: 0.017 | Acc: 98.625% \n",
      "[epoch:71, iter:6771] Loss: 0.017 | Acc: 98.652% \n",
      "[epoch:71, iter:6772] Loss: 0.017 | Acc: 98.558% \n",
      "[epoch:71, iter:6773] Loss: 0.018 | Acc: 98.585% \n",
      "[epoch:71, iter:6774] Loss: 0.018 | Acc: 98.611% \n",
      "[epoch:71, iter:6775] Loss: 0.018 | Acc: 98.636% \n",
      "[epoch:71, iter:6776] Loss: 0.018 | Acc: 98.549% \n",
      "[epoch:71, iter:6777] Loss: 0.018 | Acc: 98.575% \n",
      "[epoch:71, iter:6778] Loss: 0.019 | Acc: 98.491% \n",
      "[epoch:71, iter:6779] Loss: 0.019 | Acc: 98.517% \n",
      "[epoch:71, iter:6780] Loss: 0.018 | Acc: 98.542% \n",
      "[epoch:71, iter:6781] Loss: 0.018 | Acc: 98.566% \n",
      "[epoch:71, iter:6782] Loss: 0.019 | Acc: 98.589% \n",
      "[epoch:71, iter:6783] Loss: 0.018 | Acc: 98.611% \n",
      "[epoch:71, iter:6784] Loss: 0.018 | Acc: 98.633% \n",
      "[epoch:71, iter:6785] Loss: 0.018 | Acc: 98.654% \n",
      "[epoch:71, iter:6786] Loss: 0.018 | Acc: 98.674% \n",
      "[epoch:71, iter:6787] Loss: 0.017 | Acc: 98.694% \n",
      "[epoch:71, iter:6788] Loss: 0.017 | Acc: 98.713% \n",
      "[epoch:71, iter:6789] Loss: 0.017 | Acc: 98.732% \n",
      "[epoch:71, iter:6790] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:71, iter:6791] Loss: 0.017 | Acc: 98.768% \n",
      "[epoch:71, iter:6792] Loss: 0.016 | Acc: 98.785% \n",
      "[epoch:71, iter:6793] Loss: 0.017 | Acc: 98.801% \n",
      "[epoch:71, iter:6794] Loss: 0.016 | Acc: 98.818% \n",
      "[epoch:71, iter:6795] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:71, iter:6796] Loss: 0.017 | Acc: 98.766% \n",
      "[epoch:71, iter:6797] Loss: 0.017 | Acc: 98.782% \n",
      "[epoch:71, iter:6798] Loss: 0.017 | Acc: 98.798% \n",
      "[epoch:71, iter:6799] Loss: 0.017 | Acc: 98.813% \n",
      "[epoch:71, iter:6800] Loss: 0.017 | Acc: 98.828% \n",
      "[epoch:71, iter:6801] Loss: 0.018 | Acc: 98.688% \n",
      "[epoch:71, iter:6802] Loss: 0.018 | Acc: 98.704% \n",
      "[epoch:71, iter:6803] Loss: 0.018 | Acc: 98.720% \n",
      "[epoch:71, iter:6804] Loss: 0.017 | Acc: 98.735% \n",
      "[epoch:71, iter:6805] Loss: 0.018 | Acc: 98.676% \n",
      "[epoch:71, iter:6806] Loss: 0.018 | Acc: 98.692% \n",
      "[epoch:71, iter:6807] Loss: 0.018 | Acc: 98.707% \n",
      "[epoch:71, iter:6808] Loss: 0.018 | Acc: 98.722% \n",
      "[epoch:71, iter:6809] Loss: 0.019 | Acc: 98.666% \n",
      "[epoch:71, iter:6810] Loss: 0.018 | Acc: 98.681% \n",
      "[epoch:71, iter:6811] Loss: 0.019 | Acc: 98.695% \n",
      "[epoch:71, iter:6812] Loss: 0.020 | Acc: 98.573% \n",
      "[epoch:71, iter:6813] Loss: 0.020 | Acc: 98.522% \n",
      "[epoch:71, iter:6814] Loss: 0.020 | Acc: 98.537% \n",
      "[epoch:71, iter:6815] Loss: 0.020 | Acc: 98.553% \n",
      "[epoch:71, iter:6816] Loss: 0.020 | Acc: 98.561% \n",
      "Waiting Test!\n",
      "测试分类准确率为：89.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 72\n",
      "[epoch:72, iter:6817] Loss: 0.147 | Acc: 93.750% \n",
      "[epoch:72, iter:6818] Loss: 0.073 | Acc: 96.875% \n",
      "[epoch:72, iter:6819] Loss: 0.049 | Acc: 97.917% \n",
      "[epoch:72, iter:6820] Loss: 0.046 | Acc: 98.438% \n",
      "[epoch:72, iter:6821] Loss: 0.037 | Acc: 98.750% \n",
      "[epoch:72, iter:6822] Loss: 0.031 | Acc: 98.958% \n",
      "[epoch:72, iter:6823] Loss: 0.027 | Acc: 99.107% \n",
      "[epoch:72, iter:6824] Loss: 0.023 | Acc: 99.219% \n",
      "[epoch:72, iter:6825] Loss: 0.026 | Acc: 98.611% \n",
      "[epoch:72, iter:6826] Loss: 0.023 | Acc: 98.750% \n",
      "[epoch:72, iter:6827] Loss: 0.021 | Acc: 98.864% \n",
      "[epoch:72, iter:6828] Loss: 0.024 | Acc: 98.958% \n",
      "[epoch:72, iter:6829] Loss: 0.022 | Acc: 99.038% \n",
      "[epoch:72, iter:6830] Loss: 0.021 | Acc: 99.107% \n",
      "[epoch:72, iter:6831] Loss: 0.019 | Acc: 99.167% \n",
      "[epoch:72, iter:6832] Loss: 0.018 | Acc: 99.219% \n",
      "[epoch:72, iter:6833] Loss: 0.017 | Acc: 99.265% \n",
      "[epoch:72, iter:6834] Loss: 0.016 | Acc: 99.306% \n",
      "[epoch:72, iter:6835] Loss: 0.017 | Acc: 99.342% \n",
      "[epoch:72, iter:6836] Loss: 0.016 | Acc: 99.375% \n",
      "[epoch:72, iter:6837] Loss: 0.016 | Acc: 99.405% \n",
      "[epoch:72, iter:6838] Loss: 0.017 | Acc: 99.432% \n",
      "[epoch:72, iter:6839] Loss: 0.016 | Acc: 99.457% \n",
      "[epoch:72, iter:6840] Loss: 0.015 | Acc: 99.479% \n",
      "[epoch:72, iter:6841] Loss: 0.017 | Acc: 99.250% \n",
      "[epoch:72, iter:6842] Loss: 0.018 | Acc: 99.038% \n",
      "[epoch:72, iter:6843] Loss: 0.018 | Acc: 99.074% \n",
      "[epoch:72, iter:6844] Loss: 0.017 | Acc: 99.107% \n",
      "[epoch:72, iter:6845] Loss: 0.016 | Acc: 99.138% \n",
      "[epoch:72, iter:6846] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:72, iter:6847] Loss: 0.021 | Acc: 98.790% \n",
      "[epoch:72, iter:6848] Loss: 0.022 | Acc: 98.633% \n",
      "[epoch:72, iter:6849] Loss: 0.021 | Acc: 98.674% \n",
      "[epoch:72, iter:6850] Loss: 0.021 | Acc: 98.713% \n",
      "[epoch:72, iter:6851] Loss: 0.021 | Acc: 98.750% \n",
      "[epoch:72, iter:6852] Loss: 0.020 | Acc: 98.785% \n",
      "[epoch:72, iter:6853] Loss: 0.020 | Acc: 98.818% \n",
      "[epoch:72, iter:6854] Loss: 0.019 | Acc: 98.849% \n",
      "[epoch:72, iter:6855] Loss: 0.020 | Acc: 98.878% \n",
      "[epoch:72, iter:6856] Loss: 0.019 | Acc: 98.906% \n",
      "[epoch:72, iter:6857] Loss: 0.020 | Acc: 98.933% \n",
      "[epoch:72, iter:6858] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:72, iter:6859] Loss: 0.019 | Acc: 98.983% \n",
      "[epoch:72, iter:6860] Loss: 0.020 | Acc: 98.864% \n",
      "[epoch:72, iter:6861] Loss: 0.020 | Acc: 98.889% \n",
      "[epoch:72, iter:6862] Loss: 0.021 | Acc: 98.777% \n",
      "[epoch:72, iter:6863] Loss: 0.021 | Acc: 98.803% \n",
      "[epoch:72, iter:6864] Loss: 0.021 | Acc: 98.828% \n",
      "[epoch:72, iter:6865] Loss: 0.020 | Acc: 98.852% \n",
      "[epoch:72, iter:6866] Loss: 0.020 | Acc: 98.875% \n",
      "[epoch:72, iter:6867] Loss: 0.019 | Acc: 98.897% \n",
      "[epoch:72, iter:6868] Loss: 0.019 | Acc: 98.918% \n",
      "[epoch:72, iter:6869] Loss: 0.019 | Acc: 98.939% \n",
      "[epoch:72, iter:6870] Loss: 0.020 | Acc: 98.843% \n",
      "[epoch:72, iter:6871] Loss: 0.020 | Acc: 98.864% \n",
      "[epoch:72, iter:6872] Loss: 0.019 | Acc: 98.884% \n",
      "[epoch:72, iter:6873] Loss: 0.020 | Acc: 98.904% \n",
      "[epoch:72, iter:6874] Loss: 0.021 | Acc: 98.922% \n",
      "[epoch:72, iter:6875] Loss: 0.021 | Acc: 98.835% \n",
      "[epoch:72, iter:6876] Loss: 0.021 | Acc: 98.854% \n",
      "[epoch:72, iter:6877] Loss: 0.021 | Acc: 98.873% \n",
      "[epoch:72, iter:6878] Loss: 0.020 | Acc: 98.891% \n",
      "[epoch:72, iter:6879] Loss: 0.021 | Acc: 98.810% \n",
      "[epoch:72, iter:6880] Loss: 0.021 | Acc: 98.828% \n",
      "[epoch:72, iter:6881] Loss: 0.020 | Acc: 98.846% \n",
      "[epoch:72, iter:6882] Loss: 0.020 | Acc: 98.864% \n",
      "[epoch:72, iter:6883] Loss: 0.020 | Acc: 98.881% \n",
      "[epoch:72, iter:6884] Loss: 0.020 | Acc: 98.805% \n",
      "[epoch:72, iter:6885] Loss: 0.020 | Acc: 98.822% \n",
      "[epoch:72, iter:6886] Loss: 0.020 | Acc: 98.839% \n",
      "[epoch:72, iter:6887] Loss: 0.020 | Acc: 98.768% \n",
      "[epoch:72, iter:6888] Loss: 0.020 | Acc: 98.785% \n",
      "[epoch:72, iter:6889] Loss: 0.019 | Acc: 98.801% \n",
      "[epoch:72, iter:6890] Loss: 0.020 | Acc: 98.818% \n",
      "[epoch:72, iter:6891] Loss: 0.020 | Acc: 98.833% \n",
      "[epoch:72, iter:6892] Loss: 0.019 | Acc: 98.849% \n",
      "[epoch:72, iter:6893] Loss: 0.020 | Acc: 98.782% \n",
      "[epoch:72, iter:6894] Loss: 0.020 | Acc: 98.798% \n",
      "[epoch:72, iter:6895] Loss: 0.019 | Acc: 98.813% \n",
      "[epoch:72, iter:6896] Loss: 0.019 | Acc: 98.828% \n",
      "[epoch:72, iter:6897] Loss: 0.019 | Acc: 98.843% \n",
      "[epoch:72, iter:6898] Loss: 0.019 | Acc: 98.857% \n",
      "[epoch:72, iter:6899] Loss: 0.019 | Acc: 98.870% \n",
      "[epoch:72, iter:6900] Loss: 0.019 | Acc: 98.810% \n",
      "[epoch:72, iter:6901] Loss: 0.020 | Acc: 98.676% \n",
      "[epoch:72, iter:6902] Loss: 0.020 | Acc: 98.692% \n",
      "[epoch:72, iter:6903] Loss: 0.020 | Acc: 98.707% \n",
      "[epoch:72, iter:6904] Loss: 0.019 | Acc: 98.722% \n",
      "[epoch:72, iter:6905] Loss: 0.019 | Acc: 98.736% \n",
      "[epoch:72, iter:6906] Loss: 0.019 | Acc: 98.750% \n",
      "[epoch:72, iter:6907] Loss: 0.019 | Acc: 98.764% \n",
      "[epoch:72, iter:6908] Loss: 0.019 | Acc: 98.709% \n",
      "[epoch:72, iter:6909] Loss: 0.019 | Acc: 98.723% \n",
      "[epoch:72, iter:6910] Loss: 0.019 | Acc: 98.737% \n",
      "[epoch:72, iter:6911] Loss: 0.019 | Acc: 98.750% \n",
      "[epoch:72, iter:6912] Loss: 0.018 | Acc: 98.757% \n",
      "Waiting Test!\n",
      "测试分类准确率为：86.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 73\n",
      "[epoch:73, iter:6913] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:73, iter:6914] Loss: 0.030 | Acc: 96.875% \n",
      "[epoch:73, iter:6915] Loss: 0.020 | Acc: 97.917% \n",
      "[epoch:73, iter:6916] Loss: 0.015 | Acc: 98.438% \n",
      "[epoch:73, iter:6917] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:73, iter:6918] Loss: 0.015 | Acc: 98.958% \n",
      "[epoch:73, iter:6919] Loss: 0.017 | Acc: 99.107% \n",
      "[epoch:73, iter:6920] Loss: 0.015 | Acc: 99.219% \n",
      "[epoch:73, iter:6921] Loss: 0.017 | Acc: 99.306% \n",
      "[epoch:73, iter:6922] Loss: 0.015 | Acc: 99.375% \n",
      "[epoch:73, iter:6923] Loss: 0.018 | Acc: 99.432% \n",
      "[epoch:73, iter:6924] Loss: 0.016 | Acc: 99.479% \n",
      "[epoch:73, iter:6925] Loss: 0.015 | Acc: 99.519% \n",
      "[epoch:73, iter:6926] Loss: 0.017 | Acc: 99.554% \n",
      "[epoch:73, iter:6927] Loss: 0.017 | Acc: 99.583% \n",
      "[epoch:73, iter:6928] Loss: 0.016 | Acc: 99.609% \n",
      "[epoch:73, iter:6929] Loss: 0.016 | Acc: 99.632% \n",
      "[epoch:73, iter:6930] Loss: 0.015 | Acc: 99.653% \n",
      "[epoch:73, iter:6931] Loss: 0.016 | Acc: 99.671% \n",
      "[epoch:73, iter:6932] Loss: 0.015 | Acc: 99.688% \n",
      "[epoch:73, iter:6933] Loss: 0.015 | Acc: 99.702% \n",
      "[epoch:73, iter:6934] Loss: 0.014 | Acc: 99.716% \n",
      "[epoch:73, iter:6935] Loss: 0.013 | Acc: 99.728% \n",
      "[epoch:73, iter:6936] Loss: 0.013 | Acc: 99.740% \n",
      "[epoch:73, iter:6937] Loss: 0.012 | Acc: 99.750% \n",
      "[epoch:73, iter:6938] Loss: 0.013 | Acc: 99.760% \n",
      "[epoch:73, iter:6939] Loss: 0.012 | Acc: 99.769% \n",
      "[epoch:73, iter:6940] Loss: 0.012 | Acc: 99.777% \n",
      "[epoch:73, iter:6941] Loss: 0.013 | Acc: 99.784% \n",
      "[epoch:73, iter:6942] Loss: 0.014 | Acc: 99.792% \n",
      "[epoch:73, iter:6943] Loss: 0.013 | Acc: 99.798% \n",
      "[epoch:73, iter:6944] Loss: 0.013 | Acc: 99.805% \n",
      "[epoch:73, iter:6945] Loss: 0.012 | Acc: 99.811% \n",
      "[epoch:73, iter:6946] Loss: 0.014 | Acc: 99.632% \n",
      "[epoch:73, iter:6947] Loss: 0.014 | Acc: 99.643% \n",
      "[epoch:73, iter:6948] Loss: 0.013 | Acc: 99.653% \n",
      "[epoch:73, iter:6949] Loss: 0.013 | Acc: 99.662% \n",
      "[epoch:73, iter:6950] Loss: 0.013 | Acc: 99.671% \n",
      "[epoch:73, iter:6951] Loss: 0.014 | Acc: 99.679% \n",
      "[epoch:73, iter:6952] Loss: 0.013 | Acc: 99.688% \n",
      "[epoch:73, iter:6953] Loss: 0.015 | Acc: 99.543% \n",
      "[epoch:73, iter:6954] Loss: 0.015 | Acc: 99.554% \n",
      "[epoch:73, iter:6955] Loss: 0.016 | Acc: 99.564% \n",
      "[epoch:73, iter:6956] Loss: 0.015 | Acc: 99.574% \n",
      "[epoch:73, iter:6957] Loss: 0.017 | Acc: 99.444% \n",
      "[epoch:73, iter:6958] Loss: 0.016 | Acc: 99.457% \n",
      "[epoch:73, iter:6959] Loss: 0.016 | Acc: 99.468% \n",
      "[epoch:73, iter:6960] Loss: 0.016 | Acc: 99.479% \n",
      "[epoch:73, iter:6961] Loss: 0.015 | Acc: 99.490% \n",
      "[epoch:73, iter:6962] Loss: 0.016 | Acc: 99.375% \n",
      "[epoch:73, iter:6963] Loss: 0.017 | Acc: 99.265% \n",
      "[epoch:73, iter:6964] Loss: 0.018 | Acc: 99.159% \n",
      "[epoch:73, iter:6965] Loss: 0.018 | Acc: 99.175% \n",
      "[epoch:73, iter:6966] Loss: 0.018 | Acc: 99.074% \n",
      "[epoch:73, iter:6967] Loss: 0.019 | Acc: 98.977% \n",
      "[epoch:73, iter:6968] Loss: 0.019 | Acc: 98.996% \n",
      "[epoch:73, iter:6969] Loss: 0.019 | Acc: 99.013% \n",
      "[epoch:73, iter:6970] Loss: 0.018 | Acc: 99.030% \n",
      "[epoch:73, iter:6971] Loss: 0.019 | Acc: 98.941% \n",
      "[epoch:73, iter:6972] Loss: 0.019 | Acc: 98.958% \n",
      "[epoch:73, iter:6973] Loss: 0.019 | Acc: 98.975% \n",
      "[epoch:73, iter:6974] Loss: 0.019 | Acc: 98.992% \n",
      "[epoch:73, iter:6975] Loss: 0.018 | Acc: 99.008% \n",
      "[epoch:73, iter:6976] Loss: 0.018 | Acc: 99.023% \n",
      "[epoch:73, iter:6977] Loss: 0.018 | Acc: 99.038% \n",
      "[epoch:73, iter:6978] Loss: 0.017 | Acc: 99.053% \n",
      "[epoch:73, iter:6979] Loss: 0.017 | Acc: 99.067% \n",
      "[epoch:73, iter:6980] Loss: 0.017 | Acc: 99.081% \n",
      "[epoch:73, iter:6981] Loss: 0.017 | Acc: 99.094% \n",
      "[epoch:73, iter:6982] Loss: 0.018 | Acc: 99.018% \n",
      "[epoch:73, iter:6983] Loss: 0.019 | Acc: 98.944% \n",
      "[epoch:73, iter:6984] Loss: 0.019 | Acc: 98.958% \n",
      "[epoch:73, iter:6985] Loss: 0.019 | Acc: 98.973% \n",
      "[epoch:73, iter:6986] Loss: 0.018 | Acc: 98.986% \n",
      "[epoch:73, iter:6987] Loss: 0.018 | Acc: 99.000% \n",
      "[epoch:73, iter:6988] Loss: 0.018 | Acc: 99.013% \n",
      "[epoch:73, iter:6989] Loss: 0.018 | Acc: 99.026% \n",
      "[epoch:73, iter:6990] Loss: 0.018 | Acc: 99.038% \n",
      "[epoch:73, iter:6991] Loss: 0.017 | Acc: 99.051% \n",
      "[epoch:73, iter:6992] Loss: 0.018 | Acc: 98.984% \n",
      "[epoch:73, iter:6993] Loss: 0.017 | Acc: 98.997% \n",
      "[epoch:73, iter:6994] Loss: 0.017 | Acc: 99.009% \n",
      "[epoch:73, iter:6995] Loss: 0.017 | Acc: 99.021% \n",
      "[epoch:73, iter:6996] Loss: 0.017 | Acc: 99.033% \n",
      "[epoch:73, iter:6997] Loss: 0.018 | Acc: 98.971% \n",
      "[epoch:73, iter:6998] Loss: 0.017 | Acc: 98.983% \n",
      "[epoch:73, iter:6999] Loss: 0.017 | Acc: 98.994% \n",
      "[epoch:73, iter:7000] Loss: 0.017 | Acc: 99.006% \n",
      "[epoch:73, iter:7001] Loss: 0.017 | Acc: 99.017% \n",
      "[epoch:73, iter:7002] Loss: 0.017 | Acc: 99.028% \n",
      "[epoch:73, iter:7003] Loss: 0.016 | Acc: 99.038% \n",
      "[epoch:73, iter:7004] Loss: 0.016 | Acc: 99.049% \n",
      "[epoch:73, iter:7005] Loss: 0.016 | Acc: 99.059% \n",
      "[epoch:73, iter:7006] Loss: 0.017 | Acc: 99.003% \n",
      "[epoch:73, iter:7007] Loss: 0.017 | Acc: 98.947% \n",
      "[epoch:73, iter:7008] Loss: 0.017 | Acc: 98.954% \n",
      "Waiting Test!\n",
      "测试分类准确率为：87.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 74\n",
      "[epoch:74, iter:7009] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:74, iter:7010] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:74, iter:7011] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:74, iter:7012] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:74, iter:7013] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:74, iter:7014] Loss: 0.029 | Acc: 97.917% \n",
      "[epoch:74, iter:7015] Loss: 0.025 | Acc: 98.214% \n",
      "[epoch:74, iter:7016] Loss: 0.022 | Acc: 98.438% \n",
      "[epoch:74, iter:7017] Loss: 0.019 | Acc: 98.611% \n",
      "[epoch:74, iter:7018] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:74, iter:7019] Loss: 0.016 | Acc: 98.864% \n",
      "[epoch:74, iter:7020] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:74, iter:7021] Loss: 0.016 | Acc: 99.038% \n",
      "[epoch:74, iter:7022] Loss: 0.015 | Acc: 99.107% \n",
      "[epoch:74, iter:7023] Loss: 0.016 | Acc: 99.167% \n",
      "[epoch:74, iter:7024] Loss: 0.018 | Acc: 99.219% \n",
      "[epoch:74, iter:7025] Loss: 0.017 | Acc: 99.265% \n",
      "[epoch:74, iter:7026] Loss: 0.018 | Acc: 98.958% \n",
      "[epoch:74, iter:7027] Loss: 0.017 | Acc: 99.013% \n",
      "[epoch:74, iter:7028] Loss: 0.016 | Acc: 99.062% \n",
      "[epoch:74, iter:7029] Loss: 0.016 | Acc: 99.107% \n",
      "[epoch:74, iter:7030] Loss: 0.015 | Acc: 99.148% \n",
      "[epoch:74, iter:7031] Loss: 0.018 | Acc: 98.913% \n",
      "[epoch:74, iter:7032] Loss: 0.018 | Acc: 98.958% \n",
      "[epoch:74, iter:7033] Loss: 0.018 | Acc: 99.000% \n",
      "[epoch:74, iter:7034] Loss: 0.017 | Acc: 99.038% \n",
      "[epoch:74, iter:7035] Loss: 0.017 | Acc: 99.074% \n",
      "[epoch:74, iter:7036] Loss: 0.016 | Acc: 99.107% \n",
      "[epoch:74, iter:7037] Loss: 0.017 | Acc: 98.922% \n",
      "[epoch:74, iter:7038] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:74, iter:7039] Loss: 0.017 | Acc: 98.992% \n",
      "[epoch:74, iter:7040] Loss: 0.018 | Acc: 99.023% \n",
      "[epoch:74, iter:7041] Loss: 0.018 | Acc: 99.053% \n",
      "[epoch:74, iter:7042] Loss: 0.017 | Acc: 99.081% \n",
      "[epoch:74, iter:7043] Loss: 0.017 | Acc: 99.107% \n",
      "[epoch:74, iter:7044] Loss: 0.016 | Acc: 99.132% \n",
      "[epoch:74, iter:7045] Loss: 0.017 | Acc: 99.155% \n",
      "[epoch:74, iter:7046] Loss: 0.019 | Acc: 99.013% \n",
      "[epoch:74, iter:7047] Loss: 0.018 | Acc: 99.038% \n",
      "[epoch:74, iter:7048] Loss: 0.018 | Acc: 99.062% \n",
      "[epoch:74, iter:7049] Loss: 0.018 | Acc: 99.085% \n",
      "[epoch:74, iter:7050] Loss: 0.021 | Acc: 98.810% \n",
      "[epoch:74, iter:7051] Loss: 0.020 | Acc: 98.837% \n",
      "[epoch:74, iter:7052] Loss: 0.020 | Acc: 98.864% \n",
      "[epoch:74, iter:7053] Loss: 0.019 | Acc: 98.889% \n",
      "[epoch:74, iter:7054] Loss: 0.019 | Acc: 98.913% \n",
      "[epoch:74, iter:7055] Loss: 0.020 | Acc: 98.936% \n",
      "[epoch:74, iter:7056] Loss: 0.019 | Acc: 98.958% \n",
      "[epoch:74, iter:7057] Loss: 0.019 | Acc: 98.980% \n",
      "[epoch:74, iter:7058] Loss: 0.019 | Acc: 99.000% \n",
      "[epoch:74, iter:7059] Loss: 0.018 | Acc: 99.020% \n",
      "[epoch:74, iter:7060] Loss: 0.018 | Acc: 99.038% \n",
      "[epoch:74, iter:7061] Loss: 0.019 | Acc: 98.939% \n",
      "[epoch:74, iter:7062] Loss: 0.019 | Acc: 98.958% \n",
      "[epoch:74, iter:7063] Loss: 0.018 | Acc: 98.977% \n",
      "[epoch:74, iter:7064] Loss: 0.018 | Acc: 98.996% \n",
      "[epoch:74, iter:7065] Loss: 0.018 | Acc: 99.013% \n",
      "[epoch:74, iter:7066] Loss: 0.018 | Acc: 99.030% \n",
      "[epoch:74, iter:7067] Loss: 0.017 | Acc: 99.047% \n",
      "[epoch:74, iter:7068] Loss: 0.018 | Acc: 99.062% \n",
      "[epoch:74, iter:7069] Loss: 0.018 | Acc: 98.975% \n",
      "[epoch:74, iter:7070] Loss: 0.018 | Acc: 98.992% \n",
      "[epoch:74, iter:7071] Loss: 0.018 | Acc: 99.008% \n",
      "[epoch:74, iter:7072] Loss: 0.018 | Acc: 99.023% \n",
      "[epoch:74, iter:7073] Loss: 0.018 | Acc: 99.038% \n",
      "[epoch:74, iter:7074] Loss: 0.018 | Acc: 99.053% \n",
      "[epoch:74, iter:7075] Loss: 0.017 | Acc: 99.067% \n",
      "[epoch:74, iter:7076] Loss: 0.018 | Acc: 98.989% \n",
      "[epoch:74, iter:7077] Loss: 0.018 | Acc: 99.004% \n",
      "[epoch:74, iter:7078] Loss: 0.018 | Acc: 99.018% \n",
      "[epoch:74, iter:7079] Loss: 0.018 | Acc: 99.032% \n",
      "[epoch:74, iter:7080] Loss: 0.019 | Acc: 98.958% \n",
      "[epoch:74, iter:7081] Loss: 0.019 | Acc: 98.973% \n",
      "[epoch:74, iter:7082] Loss: 0.018 | Acc: 98.986% \n",
      "[epoch:74, iter:7083] Loss: 0.018 | Acc: 99.000% \n",
      "[epoch:74, iter:7084] Loss: 0.018 | Acc: 99.013% \n",
      "[epoch:74, iter:7085] Loss: 0.018 | Acc: 99.026% \n",
      "[epoch:74, iter:7086] Loss: 0.018 | Acc: 98.958% \n",
      "[epoch:74, iter:7087] Loss: 0.019 | Acc: 98.892% \n",
      "[epoch:74, iter:7088] Loss: 0.019 | Acc: 98.906% \n",
      "[epoch:74, iter:7089] Loss: 0.019 | Acc: 98.920% \n",
      "[epoch:74, iter:7090] Loss: 0.019 | Acc: 98.933% \n",
      "[epoch:74, iter:7091] Loss: 0.018 | Acc: 98.946% \n",
      "[epoch:74, iter:7092] Loss: 0.018 | Acc: 98.958% \n",
      "[epoch:74, iter:7093] Loss: 0.018 | Acc: 98.971% \n",
      "[epoch:74, iter:7094] Loss: 0.018 | Acc: 98.983% \n",
      "[epoch:74, iter:7095] Loss: 0.018 | Acc: 98.994% \n",
      "[epoch:74, iter:7096] Loss: 0.017 | Acc: 99.006% \n",
      "[epoch:74, iter:7097] Loss: 0.018 | Acc: 98.947% \n",
      "[epoch:74, iter:7098] Loss: 0.018 | Acc: 98.958% \n",
      "[epoch:74, iter:7099] Loss: 0.017 | Acc: 98.970% \n",
      "[epoch:74, iter:7100] Loss: 0.018 | Acc: 98.913% \n",
      "[epoch:74, iter:7101] Loss: 0.018 | Acc: 98.925% \n",
      "[epoch:74, iter:7102] Loss: 0.017 | Acc: 98.936% \n",
      "[epoch:74, iter:7103] Loss: 0.018 | Acc: 98.882% \n",
      "[epoch:74, iter:7104] Loss: 0.019 | Acc: 98.823% \n",
      "Waiting Test!\n",
      "测试分类准确率为：92.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 75\n",
      "[epoch:75, iter:7105] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:75, iter:7106] Loss: 0.021 | Acc: 100.000% \n",
      "[epoch:75, iter:7107] Loss: 0.014 | Acc: 100.000% \n",
      "[epoch:75, iter:7108] Loss: 0.011 | Acc: 100.000% \n",
      "[epoch:75, iter:7109] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:75, iter:7110] Loss: 0.015 | Acc: 98.958% \n",
      "[epoch:75, iter:7111] Loss: 0.013 | Acc: 99.107% \n",
      "[epoch:75, iter:7112] Loss: 0.016 | Acc: 99.219% \n",
      "[epoch:75, iter:7113] Loss: 0.015 | Acc: 99.306% \n",
      "[epoch:75, iter:7114] Loss: 0.014 | Acc: 99.375% \n",
      "[epoch:75, iter:7115] Loss: 0.012 | Acc: 99.432% \n",
      "[epoch:75, iter:7116] Loss: 0.011 | Acc: 99.479% \n",
      "[epoch:75, iter:7117] Loss: 0.010 | Acc: 99.519% \n",
      "[epoch:75, iter:7118] Loss: 0.010 | Acc: 99.554% \n",
      "[epoch:75, iter:7119] Loss: 0.011 | Acc: 99.583% \n",
      "[epoch:75, iter:7120] Loss: 0.013 | Acc: 99.609% \n",
      "[epoch:75, iter:7121] Loss: 0.013 | Acc: 99.632% \n",
      "[epoch:75, iter:7122] Loss: 0.012 | Acc: 99.653% \n",
      "[epoch:75, iter:7123] Loss: 0.011 | Acc: 99.671% \n",
      "[epoch:75, iter:7124] Loss: 0.011 | Acc: 99.688% \n",
      "[epoch:75, iter:7125] Loss: 0.010 | Acc: 99.702% \n",
      "[epoch:75, iter:7126] Loss: 0.010 | Acc: 99.716% \n",
      "[epoch:75, iter:7127] Loss: 0.009 | Acc: 99.728% \n",
      "[epoch:75, iter:7128] Loss: 0.011 | Acc: 99.740% \n",
      "[epoch:75, iter:7129] Loss: 0.011 | Acc: 99.750% \n",
      "[epoch:75, iter:7130] Loss: 0.010 | Acc: 99.760% \n",
      "[epoch:75, iter:7131] Loss: 0.010 | Acc: 99.769% \n",
      "[epoch:75, iter:7132] Loss: 0.009 | Acc: 99.777% \n",
      "[epoch:75, iter:7133] Loss: 0.009 | Acc: 99.784% \n",
      "[epoch:75, iter:7134] Loss: 0.009 | Acc: 99.792% \n",
      "[epoch:75, iter:7135] Loss: 0.010 | Acc: 99.597% \n",
      "[epoch:75, iter:7136] Loss: 0.010 | Acc: 99.609% \n",
      "[epoch:75, iter:7137] Loss: 0.009 | Acc: 99.621% \n",
      "[epoch:75, iter:7138] Loss: 0.009 | Acc: 99.632% \n",
      "[epoch:75, iter:7139] Loss: 0.009 | Acc: 99.643% \n",
      "[epoch:75, iter:7140] Loss: 0.009 | Acc: 99.653% \n",
      "[epoch:75, iter:7141] Loss: 0.011 | Acc: 99.493% \n",
      "[epoch:75, iter:7142] Loss: 0.010 | Acc: 99.507% \n",
      "[epoch:75, iter:7143] Loss: 0.010 | Acc: 99.519% \n",
      "[epoch:75, iter:7144] Loss: 0.010 | Acc: 99.531% \n",
      "[epoch:75, iter:7145] Loss: 0.011 | Acc: 99.390% \n",
      "[epoch:75, iter:7146] Loss: 0.011 | Acc: 99.405% \n",
      "[epoch:75, iter:7147] Loss: 0.011 | Acc: 99.419% \n",
      "[epoch:75, iter:7148] Loss: 0.011 | Acc: 99.432% \n",
      "[epoch:75, iter:7149] Loss: 0.011 | Acc: 99.444% \n",
      "[epoch:75, iter:7150] Loss: 0.010 | Acc: 99.457% \n",
      "[epoch:75, iter:7151] Loss: 0.010 | Acc: 99.468% \n",
      "[epoch:75, iter:7152] Loss: 0.011 | Acc: 99.479% \n",
      "[epoch:75, iter:7153] Loss: 0.011 | Acc: 99.490% \n",
      "[epoch:75, iter:7154] Loss: 0.011 | Acc: 99.500% \n",
      "[epoch:75, iter:7155] Loss: 0.010 | Acc: 99.510% \n",
      "[epoch:75, iter:7156] Loss: 0.012 | Acc: 99.399% \n",
      "[epoch:75, iter:7157] Loss: 0.011 | Acc: 99.410% \n",
      "[epoch:75, iter:7158] Loss: 0.011 | Acc: 99.421% \n",
      "[epoch:75, iter:7159] Loss: 0.011 | Acc: 99.432% \n",
      "[epoch:75, iter:7160] Loss: 0.011 | Acc: 99.442% \n",
      "[epoch:75, iter:7161] Loss: 0.011 | Acc: 99.452% \n",
      "[epoch:75, iter:7162] Loss: 0.010 | Acc: 99.461% \n",
      "[epoch:75, iter:7163] Loss: 0.010 | Acc: 99.470% \n",
      "[epoch:75, iter:7164] Loss: 0.010 | Acc: 99.479% \n",
      "[epoch:75, iter:7165] Loss: 0.011 | Acc: 99.385% \n",
      "[epoch:75, iter:7166] Loss: 0.011 | Acc: 99.395% \n",
      "[epoch:75, iter:7167] Loss: 0.011 | Acc: 99.405% \n",
      "[epoch:75, iter:7168] Loss: 0.011 | Acc: 99.414% \n",
      "[epoch:75, iter:7169] Loss: 0.012 | Acc: 99.327% \n",
      "[epoch:75, iter:7170] Loss: 0.012 | Acc: 99.337% \n",
      "[epoch:75, iter:7171] Loss: 0.012 | Acc: 99.347% \n",
      "[epoch:75, iter:7172] Loss: 0.012 | Acc: 99.357% \n",
      "[epoch:75, iter:7173] Loss: 0.012 | Acc: 99.366% \n",
      "[epoch:75, iter:7174] Loss: 0.012 | Acc: 99.375% \n",
      "[epoch:75, iter:7175] Loss: 0.013 | Acc: 99.296% \n",
      "[epoch:75, iter:7176] Loss: 0.013 | Acc: 99.306% \n",
      "[epoch:75, iter:7177] Loss: 0.012 | Acc: 99.315% \n",
      "[epoch:75, iter:7178] Loss: 0.012 | Acc: 99.324% \n",
      "[epoch:75, iter:7179] Loss: 0.012 | Acc: 99.333% \n",
      "[epoch:75, iter:7180] Loss: 0.012 | Acc: 99.342% \n",
      "[epoch:75, iter:7181] Loss: 0.012 | Acc: 99.351% \n",
      "[epoch:75, iter:7182] Loss: 0.012 | Acc: 99.359% \n",
      "[epoch:75, iter:7183] Loss: 0.013 | Acc: 99.288% \n",
      "[epoch:75, iter:7184] Loss: 0.013 | Acc: 99.297% \n",
      "[epoch:75, iter:7185] Loss: 0.013 | Acc: 99.228% \n",
      "[epoch:75, iter:7186] Loss: 0.013 | Acc: 99.238% \n",
      "[epoch:75, iter:7187] Loss: 0.013 | Acc: 99.247% \n",
      "[epoch:75, iter:7188] Loss: 0.013 | Acc: 99.256% \n",
      "[epoch:75, iter:7189] Loss: 0.013 | Acc: 99.265% \n",
      "[epoch:75, iter:7190] Loss: 0.014 | Acc: 99.201% \n",
      "[epoch:75, iter:7191] Loss: 0.014 | Acc: 99.138% \n",
      "[epoch:75, iter:7192] Loss: 0.015 | Acc: 99.077% \n",
      "[epoch:75, iter:7193] Loss: 0.016 | Acc: 99.017% \n",
      "[epoch:75, iter:7194] Loss: 0.016 | Acc: 99.028% \n",
      "[epoch:75, iter:7195] Loss: 0.015 | Acc: 99.038% \n",
      "[epoch:75, iter:7196] Loss: 0.016 | Acc: 98.981% \n",
      "[epoch:75, iter:7197] Loss: 0.017 | Acc: 98.992% \n",
      "[epoch:75, iter:7198] Loss: 0.017 | Acc: 99.003% \n",
      "[epoch:75, iter:7199] Loss: 0.017 | Acc: 99.013% \n",
      "[epoch:75, iter:7200] Loss: 0.018 | Acc: 98.954% \n",
      "Waiting Test!\n",
      "测试分类准确率为：93.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 76\n",
      "[epoch:76, iter:7201] Loss: 0.003 | Acc: 100.000% \n",
      "[epoch:76, iter:7202] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:76, iter:7203] Loss: 0.021 | Acc: 97.917% \n",
      "[epoch:76, iter:7204] Loss: 0.037 | Acc: 96.875% \n",
      "[epoch:76, iter:7205] Loss: 0.034 | Acc: 97.500% \n",
      "[epoch:76, iter:7206] Loss: 0.029 | Acc: 97.917% \n",
      "[epoch:76, iter:7207] Loss: 0.024 | Acc: 98.214% \n",
      "[epoch:76, iter:7208] Loss: 0.021 | Acc: 98.438% \n",
      "[epoch:76, iter:7209] Loss: 0.019 | Acc: 98.611% \n",
      "[epoch:76, iter:7210] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:76, iter:7211] Loss: 0.016 | Acc: 98.864% \n",
      "[epoch:76, iter:7212] Loss: 0.014 | Acc: 98.958% \n",
      "[epoch:76, iter:7213] Loss: 0.014 | Acc: 99.038% \n",
      "[epoch:76, iter:7214] Loss: 0.013 | Acc: 99.107% \n",
      "[epoch:76, iter:7215] Loss: 0.012 | Acc: 99.167% \n",
      "[epoch:76, iter:7216] Loss: 0.011 | Acc: 99.219% \n",
      "[epoch:76, iter:7217] Loss: 0.013 | Acc: 99.265% \n",
      "[epoch:76, iter:7218] Loss: 0.012 | Acc: 99.306% \n",
      "[epoch:76, iter:7219] Loss: 0.012 | Acc: 99.342% \n",
      "[epoch:76, iter:7220] Loss: 0.011 | Acc: 99.375% \n",
      "[epoch:76, iter:7221] Loss: 0.010 | Acc: 99.405% \n",
      "[epoch:76, iter:7222] Loss: 0.010 | Acc: 99.432% \n",
      "[epoch:76, iter:7223] Loss: 0.010 | Acc: 99.457% \n",
      "[epoch:76, iter:7224] Loss: 0.009 | Acc: 99.479% \n",
      "[epoch:76, iter:7225] Loss: 0.011 | Acc: 99.250% \n",
      "[epoch:76, iter:7226] Loss: 0.011 | Acc: 99.279% \n",
      "[epoch:76, iter:7227] Loss: 0.013 | Acc: 99.074% \n",
      "[epoch:76, iter:7228] Loss: 0.012 | Acc: 99.107% \n",
      "[epoch:76, iter:7229] Loss: 0.018 | Acc: 98.922% \n",
      "[epoch:76, iter:7230] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:76, iter:7231] Loss: 0.017 | Acc: 98.992% \n",
      "[epoch:76, iter:7232] Loss: 0.016 | Acc: 99.023% \n",
      "[epoch:76, iter:7233] Loss: 0.016 | Acc: 99.053% \n",
      "[epoch:76, iter:7234] Loss: 0.015 | Acc: 99.081% \n",
      "[epoch:76, iter:7235] Loss: 0.016 | Acc: 98.929% \n",
      "[epoch:76, iter:7236] Loss: 0.016 | Acc: 98.958% \n",
      "[epoch:76, iter:7237] Loss: 0.016 | Acc: 98.986% \n",
      "[epoch:76, iter:7238] Loss: 0.015 | Acc: 99.013% \n",
      "[epoch:76, iter:7239] Loss: 0.017 | Acc: 99.038% \n",
      "[epoch:76, iter:7240] Loss: 0.017 | Acc: 99.062% \n",
      "[epoch:76, iter:7241] Loss: 0.016 | Acc: 99.085% \n",
      "[epoch:76, iter:7242] Loss: 0.016 | Acc: 99.107% \n",
      "[epoch:76, iter:7243] Loss: 0.017 | Acc: 98.983% \n",
      "[epoch:76, iter:7244] Loss: 0.018 | Acc: 98.864% \n",
      "[epoch:76, iter:7245] Loss: 0.018 | Acc: 98.889% \n",
      "[epoch:76, iter:7246] Loss: 0.017 | Acc: 98.913% \n",
      "[epoch:76, iter:7247] Loss: 0.020 | Acc: 98.803% \n",
      "[epoch:76, iter:7248] Loss: 0.020 | Acc: 98.828% \n",
      "[epoch:76, iter:7249] Loss: 0.020 | Acc: 98.852% \n",
      "[epoch:76, iter:7250] Loss: 0.019 | Acc: 98.875% \n",
      "[epoch:76, iter:7251] Loss: 0.019 | Acc: 98.897% \n",
      "[epoch:76, iter:7252] Loss: 0.019 | Acc: 98.918% \n",
      "[epoch:76, iter:7253] Loss: 0.020 | Acc: 98.821% \n",
      "[epoch:76, iter:7254] Loss: 0.020 | Acc: 98.727% \n",
      "[epoch:76, iter:7255] Loss: 0.020 | Acc: 98.750% \n",
      "[epoch:76, iter:7256] Loss: 0.022 | Acc: 98.549% \n",
      "[epoch:76, iter:7257] Loss: 0.022 | Acc: 98.465% \n",
      "[epoch:76, iter:7258] Loss: 0.022 | Acc: 98.491% \n",
      "[epoch:76, iter:7259] Loss: 0.022 | Acc: 98.411% \n",
      "[epoch:76, iter:7260] Loss: 0.023 | Acc: 98.333% \n",
      "[epoch:76, iter:7261] Loss: 0.022 | Acc: 98.361% \n",
      "[epoch:76, iter:7262] Loss: 0.023 | Acc: 98.387% \n",
      "[epoch:76, iter:7263] Loss: 0.022 | Acc: 98.413% \n",
      "[epoch:76, iter:7264] Loss: 0.022 | Acc: 98.438% \n",
      "[epoch:76, iter:7265] Loss: 0.022 | Acc: 98.462% \n",
      "[epoch:76, iter:7266] Loss: 0.021 | Acc: 98.485% \n",
      "[epoch:76, iter:7267] Loss: 0.021 | Acc: 98.507% \n",
      "[epoch:76, iter:7268] Loss: 0.021 | Acc: 98.529% \n",
      "[epoch:76, iter:7269] Loss: 0.020 | Acc: 98.551% \n",
      "[epoch:76, iter:7270] Loss: 0.021 | Acc: 98.571% \n",
      "[epoch:76, iter:7271] Loss: 0.021 | Acc: 98.504% \n",
      "[epoch:76, iter:7272] Loss: 0.021 | Acc: 98.524% \n",
      "[epoch:76, iter:7273] Loss: 0.021 | Acc: 98.545% \n",
      "[epoch:76, iter:7274] Loss: 0.021 | Acc: 98.564% \n",
      "[epoch:76, iter:7275] Loss: 0.020 | Acc: 98.583% \n",
      "[epoch:76, iter:7276] Loss: 0.020 | Acc: 98.602% \n",
      "[epoch:76, iter:7277] Loss: 0.020 | Acc: 98.620% \n",
      "[epoch:76, iter:7278] Loss: 0.020 | Acc: 98.558% \n",
      "[epoch:76, iter:7279] Loss: 0.020 | Acc: 98.576% \n",
      "[epoch:76, iter:7280] Loss: 0.020 | Acc: 98.594% \n",
      "[epoch:76, iter:7281] Loss: 0.020 | Acc: 98.611% \n",
      "[epoch:76, iter:7282] Loss: 0.020 | Acc: 98.628% \n",
      "[epoch:76, iter:7283] Loss: 0.019 | Acc: 98.645% \n",
      "[epoch:76, iter:7284] Loss: 0.019 | Acc: 98.661% \n",
      "[epoch:76, iter:7285] Loss: 0.019 | Acc: 98.676% \n",
      "[epoch:76, iter:7286] Loss: 0.019 | Acc: 98.692% \n",
      "[epoch:76, iter:7287] Loss: 0.019 | Acc: 98.707% \n",
      "[epoch:76, iter:7288] Loss: 0.018 | Acc: 98.722% \n",
      "[epoch:76, iter:7289] Loss: 0.018 | Acc: 98.736% \n",
      "[epoch:76, iter:7290] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:76, iter:7291] Loss: 0.019 | Acc: 98.764% \n",
      "[epoch:76, iter:7292] Loss: 0.018 | Acc: 98.777% \n",
      "[epoch:76, iter:7293] Loss: 0.018 | Acc: 98.790% \n",
      "[epoch:76, iter:7294] Loss: 0.018 | Acc: 98.803% \n",
      "[epoch:76, iter:7295] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:76, iter:7296] Loss: 0.018 | Acc: 98.757% \n",
      "Waiting Test!\n",
      "测试分类准确率为：91.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 77\n",
      "[epoch:77, iter:7297] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7298] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7299] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7300] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7301] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7302] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7303] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7304] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7305] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7306] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7307] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7308] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7309] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7310] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7311] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7312] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7313] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7314] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:77, iter:7315] Loss: 0.003 | Acc: 100.000% \n",
      "[epoch:77, iter:7316] Loss: 0.006 | Acc: 99.688% \n",
      "[epoch:77, iter:7317] Loss: 0.005 | Acc: 99.702% \n",
      "[epoch:77, iter:7318] Loss: 0.005 | Acc: 99.716% \n",
      "[epoch:77, iter:7319] Loss: 0.005 | Acc: 99.728% \n",
      "[epoch:77, iter:7320] Loss: 0.006 | Acc: 99.740% \n",
      "[epoch:77, iter:7321] Loss: 0.007 | Acc: 99.750% \n",
      "[epoch:77, iter:7322] Loss: 0.007 | Acc: 99.760% \n",
      "[epoch:77, iter:7323] Loss: 0.007 | Acc: 99.769% \n",
      "[epoch:77, iter:7324] Loss: 0.010 | Acc: 99.330% \n",
      "[epoch:77, iter:7325] Loss: 0.010 | Acc: 99.353% \n",
      "[epoch:77, iter:7326] Loss: 0.010 | Acc: 99.375% \n",
      "[epoch:77, iter:7327] Loss: 0.009 | Acc: 99.395% \n",
      "[epoch:77, iter:7328] Loss: 0.009 | Acc: 99.414% \n",
      "[epoch:77, iter:7329] Loss: 0.009 | Acc: 99.432% \n",
      "[epoch:77, iter:7330] Loss: 0.008 | Acc: 99.449% \n",
      "[epoch:77, iter:7331] Loss: 0.010 | Acc: 99.286% \n",
      "[epoch:77, iter:7332] Loss: 0.010 | Acc: 99.306% \n",
      "[epoch:77, iter:7333] Loss: 0.009 | Acc: 99.324% \n",
      "[epoch:77, iter:7334] Loss: 0.010 | Acc: 99.342% \n",
      "[epoch:77, iter:7335] Loss: 0.010 | Acc: 99.359% \n",
      "[epoch:77, iter:7336] Loss: 0.011 | Acc: 99.219% \n",
      "[epoch:77, iter:7337] Loss: 0.011 | Acc: 99.238% \n",
      "[epoch:77, iter:7338] Loss: 0.013 | Acc: 99.107% \n",
      "[epoch:77, iter:7339] Loss: 0.014 | Acc: 98.983% \n",
      "[epoch:77, iter:7340] Loss: 0.015 | Acc: 98.864% \n",
      "[epoch:77, iter:7341] Loss: 0.015 | Acc: 98.889% \n",
      "[epoch:77, iter:7342] Loss: 0.015 | Acc: 98.913% \n",
      "[epoch:77, iter:7343] Loss: 0.014 | Acc: 98.936% \n",
      "[epoch:77, iter:7344] Loss: 0.016 | Acc: 98.828% \n",
      "[epoch:77, iter:7345] Loss: 0.016 | Acc: 98.852% \n",
      "[epoch:77, iter:7346] Loss: 0.016 | Acc: 98.875% \n",
      "[epoch:77, iter:7347] Loss: 0.016 | Acc: 98.897% \n",
      "[epoch:77, iter:7348] Loss: 0.016 | Acc: 98.918% \n",
      "[epoch:77, iter:7349] Loss: 0.016 | Acc: 98.939% \n",
      "[epoch:77, iter:7350] Loss: 0.015 | Acc: 98.958% \n",
      "[epoch:77, iter:7351] Loss: 0.016 | Acc: 98.864% \n",
      "[epoch:77, iter:7352] Loss: 0.016 | Acc: 98.884% \n",
      "[epoch:77, iter:7353] Loss: 0.015 | Acc: 98.904% \n",
      "[epoch:77, iter:7354] Loss: 0.016 | Acc: 98.922% \n",
      "[epoch:77, iter:7355] Loss: 0.017 | Acc: 98.835% \n",
      "[epoch:77, iter:7356] Loss: 0.016 | Acc: 98.854% \n",
      "[epoch:77, iter:7357] Loss: 0.017 | Acc: 98.770% \n",
      "[epoch:77, iter:7358] Loss: 0.017 | Acc: 98.790% \n",
      "[epoch:77, iter:7359] Loss: 0.016 | Acc: 98.810% \n",
      "[epoch:77, iter:7360] Loss: 0.016 | Acc: 98.828% \n",
      "[epoch:77, iter:7361] Loss: 0.016 | Acc: 98.846% \n",
      "[epoch:77, iter:7362] Loss: 0.016 | Acc: 98.864% \n",
      "[epoch:77, iter:7363] Loss: 0.016 | Acc: 98.881% \n",
      "[epoch:77, iter:7364] Loss: 0.016 | Acc: 98.897% \n",
      "[epoch:77, iter:7365] Loss: 0.016 | Acc: 98.913% \n",
      "[epoch:77, iter:7366] Loss: 0.016 | Acc: 98.929% \n",
      "[epoch:77, iter:7367] Loss: 0.016 | Acc: 98.944% \n",
      "[epoch:77, iter:7368] Loss: 0.016 | Acc: 98.958% \n",
      "[epoch:77, iter:7369] Loss: 0.016 | Acc: 98.973% \n",
      "[epoch:77, iter:7370] Loss: 0.016 | Acc: 98.986% \n",
      "[epoch:77, iter:7371] Loss: 0.016 | Acc: 98.917% \n",
      "[epoch:77, iter:7372] Loss: 0.016 | Acc: 98.931% \n",
      "[epoch:77, iter:7373] Loss: 0.016 | Acc: 98.864% \n",
      "[epoch:77, iter:7374] Loss: 0.016 | Acc: 98.878% \n",
      "[epoch:77, iter:7375] Loss: 0.016 | Acc: 98.892% \n",
      "[epoch:77, iter:7376] Loss: 0.016 | Acc: 98.906% \n",
      "[epoch:77, iter:7377] Loss: 0.017 | Acc: 98.843% \n",
      "[epoch:77, iter:7378] Loss: 0.017 | Acc: 98.857% \n",
      "[epoch:77, iter:7379] Loss: 0.018 | Acc: 98.795% \n",
      "[epoch:77, iter:7380] Loss: 0.018 | Acc: 98.810% \n",
      "[epoch:77, iter:7381] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:77, iter:7382] Loss: 0.018 | Acc: 98.765% \n",
      "[epoch:77, iter:7383] Loss: 0.018 | Acc: 98.779% \n",
      "[epoch:77, iter:7384] Loss: 0.018 | Acc: 98.793% \n",
      "[epoch:77, iter:7385] Loss: 0.018 | Acc: 98.806% \n",
      "[epoch:77, iter:7386] Loss: 0.018 | Acc: 98.819% \n",
      "[epoch:77, iter:7387] Loss: 0.017 | Acc: 98.832% \n",
      "[epoch:77, iter:7388] Loss: 0.017 | Acc: 98.845% \n",
      "[epoch:77, iter:7389] Loss: 0.017 | Acc: 98.858% \n",
      "[epoch:77, iter:7390] Loss: 0.017 | Acc: 98.870% \n",
      "[epoch:77, iter:7391] Loss: 0.017 | Acc: 98.882% \n",
      "[epoch:77, iter:7392] Loss: 0.017 | Acc: 98.888% \n",
      "Waiting Test!\n",
      "测试分类准确率为：88.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 78\n",
      "[epoch:78, iter:7393] Loss: 0.045 | Acc: 93.750% \n",
      "[epoch:78, iter:7394] Loss: 0.023 | Acc: 96.875% \n",
      "[epoch:78, iter:7395] Loss: 0.015 | Acc: 97.917% \n",
      "[epoch:78, iter:7396] Loss: 0.034 | Acc: 95.312% \n",
      "[epoch:78, iter:7397] Loss: 0.044 | Acc: 95.000% \n",
      "[epoch:78, iter:7398] Loss: 0.037 | Acc: 95.833% \n",
      "[epoch:78, iter:7399] Loss: 0.032 | Acc: 96.429% \n",
      "[epoch:78, iter:7400] Loss: 0.028 | Acc: 96.875% \n",
      "[epoch:78, iter:7401] Loss: 0.025 | Acc: 97.222% \n",
      "[epoch:78, iter:7402] Loss: 0.022 | Acc: 97.500% \n",
      "[epoch:78, iter:7403] Loss: 0.020 | Acc: 97.727% \n",
      "[epoch:78, iter:7404] Loss: 0.019 | Acc: 97.917% \n",
      "[epoch:78, iter:7405] Loss: 0.017 | Acc: 98.077% \n",
      "[epoch:78, iter:7406] Loss: 0.023 | Acc: 97.321% \n",
      "[epoch:78, iter:7407] Loss: 0.022 | Acc: 97.500% \n",
      "[epoch:78, iter:7408] Loss: 0.021 | Acc: 97.656% \n",
      "[epoch:78, iter:7409] Loss: 0.019 | Acc: 97.794% \n",
      "[epoch:78, iter:7410] Loss: 0.021 | Acc: 97.569% \n",
      "[epoch:78, iter:7411] Loss: 0.020 | Acc: 97.697% \n",
      "[epoch:78, iter:7412] Loss: 0.019 | Acc: 97.812% \n",
      "[epoch:78, iter:7413] Loss: 0.018 | Acc: 97.917% \n",
      "[epoch:78, iter:7414] Loss: 0.017 | Acc: 98.011% \n",
      "[epoch:78, iter:7415] Loss: 0.018 | Acc: 98.098% \n",
      "[epoch:78, iter:7416] Loss: 0.018 | Acc: 98.177% \n",
      "[epoch:78, iter:7417] Loss: 0.018 | Acc: 98.250% \n",
      "[epoch:78, iter:7418] Loss: 0.017 | Acc: 98.317% \n",
      "[epoch:78, iter:7419] Loss: 0.016 | Acc: 98.380% \n",
      "[epoch:78, iter:7420] Loss: 0.016 | Acc: 98.438% \n",
      "[epoch:78, iter:7421] Loss: 0.015 | Acc: 98.491% \n",
      "[epoch:78, iter:7422] Loss: 0.015 | Acc: 98.542% \n",
      "[epoch:78, iter:7423] Loss: 0.014 | Acc: 98.589% \n",
      "[epoch:78, iter:7424] Loss: 0.014 | Acc: 98.633% \n",
      "[epoch:78, iter:7425] Loss: 0.017 | Acc: 98.295% \n",
      "[epoch:78, iter:7426] Loss: 0.017 | Acc: 98.346% \n",
      "[epoch:78, iter:7427] Loss: 0.016 | Acc: 98.393% \n",
      "[epoch:78, iter:7428] Loss: 0.017 | Acc: 98.438% \n",
      "[epoch:78, iter:7429] Loss: 0.016 | Acc: 98.480% \n",
      "[epoch:78, iter:7430] Loss: 0.016 | Acc: 98.520% \n",
      "[epoch:78, iter:7431] Loss: 0.016 | Acc: 98.558% \n",
      "[epoch:78, iter:7432] Loss: 0.015 | Acc: 98.594% \n",
      "[epoch:78, iter:7433] Loss: 0.016 | Acc: 98.476% \n",
      "[epoch:78, iter:7434] Loss: 0.016 | Acc: 98.512% \n",
      "[epoch:78, iter:7435] Loss: 0.016 | Acc: 98.547% \n",
      "[epoch:78, iter:7436] Loss: 0.015 | Acc: 98.580% \n",
      "[epoch:78, iter:7437] Loss: 0.015 | Acc: 98.611% \n",
      "[epoch:78, iter:7438] Loss: 0.015 | Acc: 98.641% \n",
      "[epoch:78, iter:7439] Loss: 0.014 | Acc: 98.670% \n",
      "[epoch:78, iter:7440] Loss: 0.014 | Acc: 98.698% \n",
      "[epoch:78, iter:7441] Loss: 0.014 | Acc: 98.724% \n",
      "[epoch:78, iter:7442] Loss: 0.014 | Acc: 98.750% \n",
      "[epoch:78, iter:7443] Loss: 0.014 | Acc: 98.775% \n",
      "[epoch:78, iter:7444] Loss: 0.014 | Acc: 98.798% \n",
      "[epoch:78, iter:7445] Loss: 0.014 | Acc: 98.821% \n",
      "[epoch:78, iter:7446] Loss: 0.013 | Acc: 98.843% \n",
      "[epoch:78, iter:7447] Loss: 0.015 | Acc: 98.750% \n",
      "[epoch:78, iter:7448] Loss: 0.015 | Acc: 98.772% \n",
      "[epoch:78, iter:7449] Loss: 0.015 | Acc: 98.794% \n",
      "[epoch:78, iter:7450] Loss: 0.015 | Acc: 98.815% \n",
      "[epoch:78, iter:7451] Loss: 0.015 | Acc: 98.835% \n",
      "[epoch:78, iter:7452] Loss: 0.014 | Acc: 98.854% \n",
      "[epoch:78, iter:7453] Loss: 0.015 | Acc: 98.770% \n",
      "[epoch:78, iter:7454] Loss: 0.015 | Acc: 98.790% \n",
      "[epoch:78, iter:7455] Loss: 0.016 | Acc: 98.710% \n",
      "[epoch:78, iter:7456] Loss: 0.016 | Acc: 98.730% \n",
      "[epoch:78, iter:7457] Loss: 0.016 | Acc: 98.750% \n",
      "[epoch:78, iter:7458] Loss: 0.016 | Acc: 98.674% \n",
      "[epoch:78, iter:7459] Loss: 0.018 | Acc: 98.601% \n",
      "[epoch:78, iter:7460] Loss: 0.017 | Acc: 98.621% \n",
      "[epoch:78, iter:7461] Loss: 0.017 | Acc: 98.641% \n",
      "[epoch:78, iter:7462] Loss: 0.017 | Acc: 98.571% \n",
      "[epoch:78, iter:7463] Loss: 0.017 | Acc: 98.592% \n",
      "[epoch:78, iter:7464] Loss: 0.017 | Acc: 98.611% \n",
      "[epoch:78, iter:7465] Loss: 0.017 | Acc: 98.630% \n",
      "[epoch:78, iter:7466] Loss: 0.017 | Acc: 98.649% \n",
      "[epoch:78, iter:7467] Loss: 0.018 | Acc: 98.583% \n",
      "[epoch:78, iter:7468] Loss: 0.018 | Acc: 98.602% \n",
      "[epoch:78, iter:7469] Loss: 0.018 | Acc: 98.620% \n",
      "[epoch:78, iter:7470] Loss: 0.018 | Acc: 98.638% \n",
      "[epoch:78, iter:7471] Loss: 0.018 | Acc: 98.655% \n",
      "[epoch:78, iter:7472] Loss: 0.018 | Acc: 98.672% \n",
      "[epoch:78, iter:7473] Loss: 0.018 | Acc: 98.688% \n",
      "[epoch:78, iter:7474] Loss: 0.017 | Acc: 98.704% \n",
      "[epoch:78, iter:7475] Loss: 0.017 | Acc: 98.720% \n",
      "[epoch:78, iter:7476] Loss: 0.017 | Acc: 98.735% \n",
      "[epoch:78, iter:7477] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:78, iter:7478] Loss: 0.017 | Acc: 98.765% \n",
      "[epoch:78, iter:7479] Loss: 0.016 | Acc: 98.779% \n",
      "[epoch:78, iter:7480] Loss: 0.016 | Acc: 98.793% \n",
      "[epoch:78, iter:7481] Loss: 0.016 | Acc: 98.806% \n",
      "[epoch:78, iter:7482] Loss: 0.016 | Acc: 98.819% \n",
      "[epoch:78, iter:7483] Loss: 0.016 | Acc: 98.832% \n",
      "[epoch:78, iter:7484] Loss: 0.016 | Acc: 98.845% \n",
      "[epoch:78, iter:7485] Loss: 0.016 | Acc: 98.790% \n",
      "[epoch:78, iter:7486] Loss: 0.017 | Acc: 98.737% \n",
      "[epoch:78, iter:7487] Loss: 0.016 | Acc: 98.750% \n",
      "[epoch:78, iter:7488] Loss: 0.017 | Acc: 98.692% \n",
      "Waiting Test!\n",
      "测试分类准确率为：91.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 79\n",
      "[epoch:79, iter:7489] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:79, iter:7490] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:79, iter:7491] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:79, iter:7492] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:79, iter:7493] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:79, iter:7494] Loss: 0.007 | Acc: 100.000% \n",
      "[epoch:79, iter:7495] Loss: 0.012 | Acc: 100.000% \n",
      "[epoch:79, iter:7496] Loss: 0.011 | Acc: 100.000% \n",
      "[epoch:79, iter:7497] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:79, iter:7498] Loss: 0.008 | Acc: 100.000% \n",
      "[epoch:79, iter:7499] Loss: 0.008 | Acc: 100.000% \n",
      "[epoch:79, iter:7500] Loss: 0.007 | Acc: 100.000% \n",
      "[epoch:79, iter:7501] Loss: 0.007 | Acc: 100.000% \n",
      "[epoch:79, iter:7502] Loss: 0.010 | Acc: 99.554% \n",
      "[epoch:79, iter:7503] Loss: 0.011 | Acc: 99.583% \n",
      "[epoch:79, iter:7504] Loss: 0.016 | Acc: 99.219% \n",
      "[epoch:79, iter:7505] Loss: 0.015 | Acc: 99.265% \n",
      "[epoch:79, iter:7506] Loss: 0.014 | Acc: 99.306% \n",
      "[epoch:79, iter:7507] Loss: 0.016 | Acc: 99.013% \n",
      "[epoch:79, iter:7508] Loss: 0.015 | Acc: 99.062% \n",
      "[epoch:79, iter:7509] Loss: 0.014 | Acc: 99.107% \n",
      "[epoch:79, iter:7510] Loss: 0.016 | Acc: 98.864% \n",
      "[epoch:79, iter:7511] Loss: 0.015 | Acc: 98.913% \n",
      "[epoch:79, iter:7512] Loss: 0.018 | Acc: 98.698% \n",
      "[epoch:79, iter:7513] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:79, iter:7514] Loss: 0.017 | Acc: 98.798% \n",
      "[epoch:79, iter:7515] Loss: 0.017 | Acc: 98.843% \n",
      "[epoch:79, iter:7516] Loss: 0.016 | Acc: 98.884% \n",
      "[epoch:79, iter:7517] Loss: 0.016 | Acc: 98.922% \n",
      "[epoch:79, iter:7518] Loss: 0.015 | Acc: 98.958% \n",
      "[epoch:79, iter:7519] Loss: 0.019 | Acc: 98.589% \n",
      "[epoch:79, iter:7520] Loss: 0.018 | Acc: 98.633% \n",
      "[epoch:79, iter:7521] Loss: 0.018 | Acc: 98.674% \n",
      "[epoch:79, iter:7522] Loss: 0.017 | Acc: 98.713% \n",
      "[epoch:79, iter:7523] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:79, iter:7524] Loss: 0.016 | Acc: 98.785% \n",
      "[epoch:79, iter:7525] Loss: 0.020 | Acc: 98.649% \n",
      "[epoch:79, iter:7526] Loss: 0.019 | Acc: 98.684% \n",
      "[epoch:79, iter:7527] Loss: 0.019 | Acc: 98.718% \n",
      "[epoch:79, iter:7528] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:79, iter:7529] Loss: 0.018 | Acc: 98.780% \n",
      "[epoch:79, iter:7530] Loss: 0.018 | Acc: 98.810% \n",
      "[epoch:79, iter:7531] Loss: 0.018 | Acc: 98.837% \n",
      "[epoch:79, iter:7532] Loss: 0.017 | Acc: 98.864% \n",
      "[epoch:79, iter:7533] Loss: 0.017 | Acc: 98.889% \n",
      "[epoch:79, iter:7534] Loss: 0.017 | Acc: 98.913% \n",
      "[epoch:79, iter:7535] Loss: 0.016 | Acc: 98.936% \n",
      "[epoch:79, iter:7536] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:79, iter:7537] Loss: 0.017 | Acc: 98.980% \n",
      "[epoch:79, iter:7538] Loss: 0.017 | Acc: 99.000% \n",
      "[epoch:79, iter:7539] Loss: 0.017 | Acc: 99.020% \n",
      "[epoch:79, iter:7540] Loss: 0.018 | Acc: 98.918% \n",
      "[epoch:79, iter:7541] Loss: 0.018 | Acc: 98.939% \n",
      "[epoch:79, iter:7542] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:79, iter:7543] Loss: 0.017 | Acc: 98.977% \n",
      "[epoch:79, iter:7544] Loss: 0.017 | Acc: 98.996% \n",
      "[epoch:79, iter:7545] Loss: 0.016 | Acc: 99.013% \n",
      "[epoch:79, iter:7546] Loss: 0.016 | Acc: 99.030% \n",
      "[epoch:79, iter:7547] Loss: 0.017 | Acc: 99.047% \n",
      "[epoch:79, iter:7548] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:79, iter:7549] Loss: 0.017 | Acc: 98.975% \n",
      "[epoch:79, iter:7550] Loss: 0.017 | Acc: 98.992% \n",
      "[epoch:79, iter:7551] Loss: 0.017 | Acc: 99.008% \n",
      "[epoch:79, iter:7552] Loss: 0.018 | Acc: 98.926% \n",
      "[epoch:79, iter:7553] Loss: 0.018 | Acc: 98.942% \n",
      "[epoch:79, iter:7554] Loss: 0.018 | Acc: 98.864% \n",
      "[epoch:79, iter:7555] Loss: 0.018 | Acc: 98.881% \n",
      "[epoch:79, iter:7556] Loss: 0.019 | Acc: 98.805% \n",
      "[epoch:79, iter:7557] Loss: 0.019 | Acc: 98.822% \n",
      "[epoch:79, iter:7558] Loss: 0.019 | Acc: 98.839% \n",
      "[epoch:79, iter:7559] Loss: 0.019 | Acc: 98.856% \n",
      "[epoch:79, iter:7560] Loss: 0.019 | Acc: 98.872% \n",
      "[epoch:79, iter:7561] Loss: 0.018 | Acc: 98.887% \n",
      "[epoch:79, iter:7562] Loss: 0.018 | Acc: 98.902% \n",
      "[epoch:79, iter:7563] Loss: 0.018 | Acc: 98.917% \n",
      "[epoch:79, iter:7564] Loss: 0.018 | Acc: 98.849% \n",
      "[epoch:79, iter:7565] Loss: 0.018 | Acc: 98.864% \n",
      "[epoch:79, iter:7566] Loss: 0.018 | Acc: 98.878% \n",
      "[epoch:79, iter:7567] Loss: 0.018 | Acc: 98.892% \n",
      "[epoch:79, iter:7568] Loss: 0.018 | Acc: 98.906% \n",
      "[epoch:79, iter:7569] Loss: 0.018 | Acc: 98.920% \n",
      "[epoch:79, iter:7570] Loss: 0.017 | Acc: 98.933% \n",
      "[epoch:79, iter:7571] Loss: 0.018 | Acc: 98.870% \n",
      "[epoch:79, iter:7572] Loss: 0.018 | Acc: 98.884% \n",
      "[epoch:79, iter:7573] Loss: 0.018 | Acc: 98.897% \n",
      "[epoch:79, iter:7574] Loss: 0.018 | Acc: 98.837% \n",
      "[epoch:79, iter:7575] Loss: 0.018 | Acc: 98.851% \n",
      "[epoch:79, iter:7576] Loss: 0.018 | Acc: 98.864% \n",
      "[epoch:79, iter:7577] Loss: 0.017 | Acc: 98.876% \n",
      "[epoch:79, iter:7578] Loss: 0.017 | Acc: 98.889% \n",
      "[epoch:79, iter:7579] Loss: 0.017 | Acc: 98.901% \n",
      "[epoch:79, iter:7580] Loss: 0.017 | Acc: 98.913% \n",
      "[epoch:79, iter:7581] Loss: 0.017 | Acc: 98.925% \n",
      "[epoch:79, iter:7582] Loss: 0.016 | Acc: 98.936% \n",
      "[epoch:79, iter:7583] Loss: 0.017 | Acc: 98.947% \n",
      "[epoch:79, iter:7584] Loss: 0.018 | Acc: 98.954% \n",
      "Waiting Test!\n",
      "测试分类准确率为：83.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 80\n",
      "[epoch:80, iter:7585] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:80, iter:7586] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:80, iter:7587] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:80, iter:7588] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:80, iter:7589] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:80, iter:7590] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:80, iter:7591] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:80, iter:7592] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:80, iter:7593] Loss: 0.013 | Acc: 99.306% \n",
      "[epoch:80, iter:7594] Loss: 0.027 | Acc: 98.125% \n",
      "[epoch:80, iter:7595] Loss: 0.025 | Acc: 98.295% \n",
      "[epoch:80, iter:7596] Loss: 0.028 | Acc: 97.917% \n",
      "[epoch:80, iter:7597] Loss: 0.031 | Acc: 97.596% \n",
      "[epoch:80, iter:7598] Loss: 0.032 | Acc: 97.321% \n",
      "[epoch:80, iter:7599] Loss: 0.031 | Acc: 97.500% \n",
      "[epoch:80, iter:7600] Loss: 0.029 | Acc: 97.656% \n",
      "[epoch:80, iter:7601] Loss: 0.028 | Acc: 97.794% \n",
      "[epoch:80, iter:7602] Loss: 0.027 | Acc: 97.917% \n",
      "[epoch:80, iter:7603] Loss: 0.026 | Acc: 98.026% \n",
      "[epoch:80, iter:7604] Loss: 0.025 | Acc: 98.125% \n",
      "[epoch:80, iter:7605] Loss: 0.024 | Acc: 98.214% \n",
      "[epoch:80, iter:7606] Loss: 0.023 | Acc: 98.295% \n",
      "[epoch:80, iter:7607] Loss: 0.025 | Acc: 98.098% \n",
      "[epoch:80, iter:7608] Loss: 0.035 | Acc: 97.917% \n",
      "[epoch:80, iter:7609] Loss: 0.034 | Acc: 98.000% \n",
      "[epoch:80, iter:7610] Loss: 0.033 | Acc: 98.077% \n",
      "[epoch:80, iter:7611] Loss: 0.033 | Acc: 98.148% \n",
      "[epoch:80, iter:7612] Loss: 0.032 | Acc: 98.214% \n",
      "[epoch:80, iter:7613] Loss: 0.033 | Acc: 98.276% \n",
      "[epoch:80, iter:7614] Loss: 0.032 | Acc: 98.333% \n",
      "[epoch:80, iter:7615] Loss: 0.031 | Acc: 98.387% \n",
      "[epoch:80, iter:7616] Loss: 0.032 | Acc: 98.242% \n",
      "[epoch:80, iter:7617] Loss: 0.031 | Acc: 98.295% \n",
      "[epoch:80, iter:7618] Loss: 0.031 | Acc: 98.346% \n",
      "[epoch:80, iter:7619] Loss: 0.031 | Acc: 98.393% \n",
      "[epoch:80, iter:7620] Loss: 0.032 | Acc: 98.438% \n",
      "[epoch:80, iter:7621] Loss: 0.031 | Acc: 98.480% \n",
      "[epoch:80, iter:7622] Loss: 0.031 | Acc: 98.520% \n",
      "[epoch:80, iter:7623] Loss: 0.030 | Acc: 98.558% \n",
      "[epoch:80, iter:7624] Loss: 0.032 | Acc: 98.438% \n",
      "[epoch:80, iter:7625] Loss: 0.031 | Acc: 98.476% \n",
      "[epoch:80, iter:7626] Loss: 0.030 | Acc: 98.512% \n",
      "[epoch:80, iter:7627] Loss: 0.031 | Acc: 98.547% \n",
      "[epoch:80, iter:7628] Loss: 0.030 | Acc: 98.580% \n",
      "[epoch:80, iter:7629] Loss: 0.031 | Acc: 98.611% \n",
      "[epoch:80, iter:7630] Loss: 0.032 | Acc: 98.505% \n",
      "[epoch:80, iter:7631] Loss: 0.031 | Acc: 98.537% \n",
      "[epoch:80, iter:7632] Loss: 0.030 | Acc: 98.568% \n",
      "[epoch:80, iter:7633] Loss: 0.030 | Acc: 98.597% \n",
      "[epoch:80, iter:7634] Loss: 0.030 | Acc: 98.625% \n",
      "[epoch:80, iter:7635] Loss: 0.029 | Acc: 98.652% \n",
      "[epoch:80, iter:7636] Loss: 0.030 | Acc: 98.678% \n",
      "[epoch:80, iter:7637] Loss: 0.030 | Acc: 98.585% \n",
      "[epoch:80, iter:7638] Loss: 0.032 | Acc: 98.495% \n",
      "[epoch:80, iter:7639] Loss: 0.031 | Acc: 98.523% \n",
      "[epoch:80, iter:7640] Loss: 0.031 | Acc: 98.549% \n",
      "[epoch:80, iter:7641] Loss: 0.030 | Acc: 98.575% \n",
      "[epoch:80, iter:7642] Loss: 0.030 | Acc: 98.599% \n",
      "[epoch:80, iter:7643] Loss: 0.031 | Acc: 98.517% \n",
      "[epoch:80, iter:7644] Loss: 0.030 | Acc: 98.542% \n",
      "[epoch:80, iter:7645] Loss: 0.030 | Acc: 98.566% \n",
      "[epoch:80, iter:7646] Loss: 0.033 | Acc: 98.387% \n",
      "[epoch:80, iter:7647] Loss: 0.032 | Acc: 98.413% \n",
      "[epoch:80, iter:7648] Loss: 0.032 | Acc: 98.438% \n",
      "[epoch:80, iter:7649] Loss: 0.033 | Acc: 98.365% \n",
      "[epoch:80, iter:7650] Loss: 0.034 | Acc: 98.295% \n",
      "[epoch:80, iter:7651] Loss: 0.034 | Acc: 98.228% \n",
      "[epoch:80, iter:7652] Loss: 0.033 | Acc: 98.254% \n",
      "[epoch:80, iter:7653] Loss: 0.033 | Acc: 98.279% \n",
      "[epoch:80, iter:7654] Loss: 0.032 | Acc: 98.304% \n",
      "[epoch:80, iter:7655] Loss: 0.032 | Acc: 98.327% \n",
      "[epoch:80, iter:7656] Loss: 0.032 | Acc: 98.351% \n",
      "[epoch:80, iter:7657] Loss: 0.031 | Acc: 98.373% \n",
      "[epoch:80, iter:7658] Loss: 0.032 | Acc: 98.311% \n",
      "[epoch:80, iter:7659] Loss: 0.032 | Acc: 98.333% \n",
      "[epoch:80, iter:7660] Loss: 0.031 | Acc: 98.355% \n",
      "[epoch:80, iter:7661] Loss: 0.031 | Acc: 98.377% \n",
      "[epoch:80, iter:7662] Loss: 0.032 | Acc: 98.317% \n",
      "[epoch:80, iter:7663] Loss: 0.031 | Acc: 98.339% \n",
      "[epoch:80, iter:7664] Loss: 0.031 | Acc: 98.359% \n",
      "[epoch:80, iter:7665] Loss: 0.031 | Acc: 98.380% \n",
      "[epoch:80, iter:7666] Loss: 0.031 | Acc: 98.399% \n",
      "[epoch:80, iter:7667] Loss: 0.030 | Acc: 98.419% \n",
      "[epoch:80, iter:7668] Loss: 0.030 | Acc: 98.438% \n",
      "[epoch:80, iter:7669] Loss: 0.033 | Acc: 98.382% \n",
      "[epoch:80, iter:7670] Loss: 0.033 | Acc: 98.401% \n",
      "[epoch:80, iter:7671] Loss: 0.033 | Acc: 98.348% \n",
      "[epoch:80, iter:7672] Loss: 0.033 | Acc: 98.366% \n",
      "[epoch:80, iter:7673] Loss: 0.035 | Acc: 98.244% \n",
      "[epoch:80, iter:7674] Loss: 0.035 | Acc: 98.264% \n",
      "[epoch:80, iter:7675] Loss: 0.034 | Acc: 98.283% \n",
      "[epoch:80, iter:7676] Loss: 0.034 | Acc: 98.302% \n",
      "[epoch:80, iter:7677] Loss: 0.038 | Acc: 98.253% \n",
      "[epoch:80, iter:7678] Loss: 0.038 | Acc: 98.271% \n",
      "[epoch:80, iter:7679] Loss: 0.039 | Acc: 98.224% \n",
      "[epoch:80, iter:7680] Loss: 0.039 | Acc: 98.234% \n",
      "Waiting Test!\n",
      "测试分类准确率为：76.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 81\n",
      "[epoch:81, iter:7681] Loss: 0.018 | Acc: 100.000% \n",
      "[epoch:81, iter:7682] Loss: 0.011 | Acc: 100.000% \n",
      "[epoch:81, iter:7683] Loss: 0.042 | Acc: 97.917% \n",
      "[epoch:81, iter:7684] Loss: 0.031 | Acc: 98.438% \n",
      "[epoch:81, iter:7685] Loss: 0.032 | Acc: 98.750% \n",
      "[epoch:81, iter:7686] Loss: 0.028 | Acc: 98.958% \n",
      "[epoch:81, iter:7687] Loss: 0.033 | Acc: 98.214% \n",
      "[epoch:81, iter:7688] Loss: 0.031 | Acc: 98.438% \n",
      "[epoch:81, iter:7689] Loss: 0.029 | Acc: 98.611% \n",
      "[epoch:81, iter:7690] Loss: 0.066 | Acc: 97.500% \n",
      "[epoch:81, iter:7691] Loss: 0.061 | Acc: 97.727% \n",
      "[epoch:81, iter:7692] Loss: 0.059 | Acc: 97.917% \n",
      "[epoch:81, iter:7693] Loss: 0.056 | Acc: 98.077% \n",
      "[epoch:81, iter:7694] Loss: 0.054 | Acc: 98.214% \n",
      "[epoch:81, iter:7695] Loss: 0.057 | Acc: 97.917% \n",
      "[epoch:81, iter:7696] Loss: 0.054 | Acc: 98.047% \n",
      "[epoch:81, iter:7697] Loss: 0.063 | Acc: 97.426% \n",
      "[epoch:81, iter:7698] Loss: 0.063 | Acc: 97.569% \n",
      "[epoch:81, iter:7699] Loss: 0.062 | Acc: 97.697% \n",
      "[epoch:81, iter:7700] Loss: 0.059 | Acc: 97.812% \n",
      "[epoch:81, iter:7701] Loss: 0.058 | Acc: 97.917% \n",
      "[epoch:81, iter:7702] Loss: 0.056 | Acc: 98.011% \n",
      "[epoch:81, iter:7703] Loss: 0.054 | Acc: 98.098% \n",
      "[epoch:81, iter:7704] Loss: 0.053 | Acc: 98.177% \n",
      "[epoch:81, iter:7705] Loss: 0.056 | Acc: 98.000% \n",
      "[epoch:81, iter:7706] Loss: 0.058 | Acc: 97.596% \n",
      "[epoch:81, iter:7707] Loss: 0.063 | Acc: 97.222% \n",
      "[epoch:81, iter:7708] Loss: 0.062 | Acc: 97.321% \n",
      "[epoch:81, iter:7709] Loss: 0.064 | Acc: 97.198% \n",
      "[epoch:81, iter:7710] Loss: 0.063 | Acc: 97.292% \n",
      "[epoch:81, iter:7711] Loss: 0.062 | Acc: 97.379% \n",
      "[epoch:81, iter:7712] Loss: 0.062 | Acc: 97.266% \n",
      "[epoch:81, iter:7713] Loss: 0.063 | Acc: 97.348% \n",
      "[epoch:81, iter:7714] Loss: 0.066 | Acc: 97.243% \n",
      "[epoch:81, iter:7715] Loss: 0.070 | Acc: 97.143% \n",
      "[epoch:81, iter:7716] Loss: 0.070 | Acc: 97.222% \n",
      "[epoch:81, iter:7717] Loss: 0.068 | Acc: 97.297% \n",
      "[epoch:81, iter:7718] Loss: 0.067 | Acc: 97.368% \n",
      "[epoch:81, iter:7719] Loss: 0.066 | Acc: 97.436% \n",
      "[epoch:81, iter:7720] Loss: 0.074 | Acc: 97.344% \n",
      "[epoch:81, iter:7721] Loss: 0.077 | Acc: 97.104% \n",
      "[epoch:81, iter:7722] Loss: 0.083 | Acc: 97.024% \n",
      "[epoch:81, iter:7723] Loss: 0.081 | Acc: 97.093% \n",
      "[epoch:81, iter:7724] Loss: 0.081 | Acc: 97.017% \n",
      "[epoch:81, iter:7725] Loss: 0.080 | Acc: 96.944% \n",
      "[epoch:81, iter:7726] Loss: 0.080 | Acc: 96.875% \n",
      "[epoch:81, iter:7727] Loss: 0.082 | Acc: 96.676% \n",
      "[epoch:81, iter:7728] Loss: 0.081 | Acc: 96.745% \n",
      "[epoch:81, iter:7729] Loss: 0.080 | Acc: 96.811% \n",
      "[epoch:81, iter:7730] Loss: 0.085 | Acc: 96.750% \n",
      "[epoch:81, iter:7731] Loss: 0.084 | Acc: 96.814% \n",
      "[epoch:81, iter:7732] Loss: 0.083 | Acc: 96.755% \n",
      "[epoch:81, iter:7733] Loss: 0.083 | Acc: 96.816% \n",
      "[epoch:81, iter:7734] Loss: 0.082 | Acc: 96.875% \n",
      "[epoch:81, iter:7735] Loss: 0.081 | Acc: 96.932% \n",
      "[epoch:81, iter:7736] Loss: 0.082 | Acc: 96.875% \n",
      "[epoch:81, iter:7737] Loss: 0.084 | Acc: 96.820% \n",
      "[epoch:81, iter:7738] Loss: 0.089 | Acc: 96.552% \n",
      "[epoch:81, iter:7739] Loss: 0.092 | Acc: 96.398% \n",
      "[epoch:81, iter:7740] Loss: 0.091 | Acc: 96.458% \n",
      "[epoch:81, iter:7741] Loss: 0.090 | Acc: 96.516% \n",
      "[epoch:81, iter:7742] Loss: 0.090 | Acc: 96.472% \n",
      "[epoch:81, iter:7743] Loss: 0.089 | Acc: 96.528% \n",
      "[epoch:81, iter:7744] Loss: 0.092 | Acc: 96.484% \n",
      "[epoch:81, iter:7745] Loss: 0.093 | Acc: 96.442% \n",
      "[epoch:81, iter:7746] Loss: 0.092 | Acc: 96.496% \n",
      "[epoch:81, iter:7747] Loss: 0.091 | Acc: 96.549% \n",
      "[epoch:81, iter:7748] Loss: 0.096 | Acc: 96.507% \n",
      "[epoch:81, iter:7749] Loss: 0.096 | Acc: 96.467% \n",
      "[epoch:81, iter:7750] Loss: 0.097 | Acc: 96.429% \n",
      "[epoch:81, iter:7751] Loss: 0.096 | Acc: 96.479% \n",
      "[epoch:81, iter:7752] Loss: 0.095 | Acc: 96.528% \n",
      "[epoch:81, iter:7753] Loss: 0.094 | Acc: 96.575% \n",
      "[epoch:81, iter:7754] Loss: 0.105 | Acc: 96.199% \n",
      "[epoch:81, iter:7755] Loss: 0.105 | Acc: 96.167% \n",
      "[epoch:81, iter:7756] Loss: 0.104 | Acc: 96.217% \n",
      "[epoch:81, iter:7757] Loss: 0.108 | Acc: 95.942% \n",
      "[epoch:81, iter:7758] Loss: 0.108 | Acc: 95.913% \n",
      "[epoch:81, iter:7759] Loss: 0.107 | Acc: 95.965% \n",
      "[epoch:81, iter:7760] Loss: 0.106 | Acc: 96.016% \n",
      "[epoch:81, iter:7761] Loss: 0.107 | Acc: 95.988% \n",
      "[epoch:81, iter:7762] Loss: 0.106 | Acc: 96.037% \n",
      "[epoch:81, iter:7763] Loss: 0.105 | Acc: 96.084% \n",
      "[epoch:81, iter:7764] Loss: 0.104 | Acc: 96.131% \n",
      "[epoch:81, iter:7765] Loss: 0.105 | Acc: 96.029% \n",
      "[epoch:81, iter:7766] Loss: 0.107 | Acc: 95.930% \n",
      "[epoch:81, iter:7767] Loss: 0.112 | Acc: 95.833% \n",
      "[epoch:81, iter:7768] Loss: 0.111 | Acc: 95.881% \n",
      "[epoch:81, iter:7769] Loss: 0.111 | Acc: 95.857% \n",
      "[epoch:81, iter:7770] Loss: 0.111 | Acc: 95.903% \n",
      "[epoch:81, iter:7771] Loss: 0.110 | Acc: 95.879% \n",
      "[epoch:81, iter:7772] Loss: 0.111 | Acc: 95.788% \n",
      "[epoch:81, iter:7773] Loss: 0.111 | Acc: 95.833% \n",
      "[epoch:81, iter:7774] Loss: 0.111 | Acc: 95.811% \n",
      "[epoch:81, iter:7775] Loss: 0.111 | Acc: 95.855% \n",
      "[epoch:81, iter:7776] Loss: 0.110 | Acc: 95.880% \n",
      "Waiting Test!\n",
      "测试分类准确率为：70.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 82\n",
      "[epoch:82, iter:7777] Loss: 0.061 | Acc: 100.000% \n",
      "[epoch:82, iter:7778] Loss: 0.034 | Acc: 100.000% \n",
      "[epoch:82, iter:7779] Loss: 0.074 | Acc: 97.917% \n",
      "[epoch:82, iter:7780] Loss: 0.057 | Acc: 98.438% \n",
      "[epoch:82, iter:7781] Loss: 0.057 | Acc: 98.750% \n",
      "[epoch:82, iter:7782] Loss: 0.057 | Acc: 98.958% \n",
      "[epoch:82, iter:7783] Loss: 0.050 | Acc: 99.107% \n",
      "[epoch:82, iter:7784] Loss: 0.045 | Acc: 99.219% \n",
      "[epoch:82, iter:7785] Loss: 0.060 | Acc: 98.611% \n",
      "[epoch:82, iter:7786] Loss: 0.085 | Acc: 98.125% \n",
      "[epoch:82, iter:7787] Loss: 0.109 | Acc: 96.591% \n",
      "[epoch:82, iter:7788] Loss: 0.107 | Acc: 96.354% \n",
      "[epoch:82, iter:7789] Loss: 0.131 | Acc: 95.673% \n",
      "[epoch:82, iter:7790] Loss: 0.127 | Acc: 95.536% \n",
      "[epoch:82, iter:7791] Loss: 0.124 | Acc: 95.833% \n",
      "[epoch:82, iter:7792] Loss: 0.119 | Acc: 96.094% \n",
      "[epoch:82, iter:7793] Loss: 0.116 | Acc: 96.324% \n",
      "[epoch:82, iter:7794] Loss: 0.118 | Acc: 96.181% \n",
      "[epoch:82, iter:7795] Loss: 0.113 | Acc: 96.382% \n",
      "[epoch:82, iter:7796] Loss: 0.109 | Acc: 96.562% \n",
      "[epoch:82, iter:7797] Loss: 0.106 | Acc: 96.726% \n",
      "[epoch:82, iter:7798] Loss: 0.103 | Acc: 96.875% \n",
      "[epoch:82, iter:7799] Loss: 0.100 | Acc: 97.011% \n",
      "[epoch:82, iter:7800] Loss: 0.100 | Acc: 96.615% \n",
      "[epoch:82, iter:7801] Loss: 0.102 | Acc: 96.500% \n",
      "[epoch:82, iter:7802] Loss: 0.105 | Acc: 96.394% \n",
      "[epoch:82, iter:7803] Loss: 0.101 | Acc: 96.528% \n",
      "[epoch:82, iter:7804] Loss: 0.098 | Acc: 96.652% \n",
      "[epoch:82, iter:7805] Loss: 0.097 | Acc: 96.552% \n",
      "[epoch:82, iter:7806] Loss: 0.097 | Acc: 96.458% \n",
      "[epoch:82, iter:7807] Loss: 0.097 | Acc: 96.371% \n",
      "[epoch:82, iter:7808] Loss: 0.104 | Acc: 96.289% \n",
      "[epoch:82, iter:7809] Loss: 0.101 | Acc: 96.402% \n",
      "[epoch:82, iter:7810] Loss: 0.101 | Acc: 96.324% \n",
      "[epoch:82, iter:7811] Loss: 0.101 | Acc: 96.250% \n",
      "[epoch:82, iter:7812] Loss: 0.108 | Acc: 96.181% \n",
      "[epoch:82, iter:7813] Loss: 0.107 | Acc: 96.284% \n",
      "[epoch:82, iter:7814] Loss: 0.104 | Acc: 96.382% \n",
      "[epoch:82, iter:7815] Loss: 0.103 | Acc: 96.314% \n",
      "[epoch:82, iter:7816] Loss: 0.101 | Acc: 96.406% \n",
      "[epoch:82, iter:7817] Loss: 0.099 | Acc: 96.494% \n",
      "[epoch:82, iter:7818] Loss: 0.101 | Acc: 96.280% \n",
      "[epoch:82, iter:7819] Loss: 0.099 | Acc: 96.366% \n",
      "[epoch:82, iter:7820] Loss: 0.097 | Acc: 96.449% \n",
      "[epoch:82, iter:7821] Loss: 0.096 | Acc: 96.528% \n",
      "[epoch:82, iter:7822] Loss: 0.094 | Acc: 96.603% \n",
      "[epoch:82, iter:7823] Loss: 0.092 | Acc: 96.676% \n",
      "[epoch:82, iter:7824] Loss: 0.092 | Acc: 96.745% \n",
      "[epoch:82, iter:7825] Loss: 0.090 | Acc: 96.811% \n",
      "[epoch:82, iter:7826] Loss: 0.090 | Acc: 96.750% \n",
      "[epoch:82, iter:7827] Loss: 0.092 | Acc: 96.569% \n",
      "[epoch:82, iter:7828] Loss: 0.098 | Acc: 96.514% \n",
      "[epoch:82, iter:7829] Loss: 0.097 | Acc: 96.580% \n",
      "[epoch:82, iter:7830] Loss: 0.098 | Acc: 96.528% \n",
      "[epoch:82, iter:7831] Loss: 0.096 | Acc: 96.591% \n",
      "[epoch:82, iter:7832] Loss: 0.094 | Acc: 96.652% \n",
      "[epoch:82, iter:7833] Loss: 0.093 | Acc: 96.711% \n",
      "[epoch:82, iter:7834] Loss: 0.092 | Acc: 96.767% \n",
      "[epoch:82, iter:7835] Loss: 0.095 | Acc: 96.504% \n",
      "[epoch:82, iter:7836] Loss: 0.094 | Acc: 96.562% \n",
      "[epoch:82, iter:7837] Loss: 0.094 | Acc: 96.619% \n",
      "[epoch:82, iter:7838] Loss: 0.093 | Acc: 96.673% \n",
      "[epoch:82, iter:7839] Loss: 0.093 | Acc: 96.627% \n",
      "[epoch:82, iter:7840] Loss: 0.092 | Acc: 96.680% \n",
      "[epoch:82, iter:7841] Loss: 0.096 | Acc: 96.635% \n",
      "[epoch:82, iter:7842] Loss: 0.095 | Acc: 96.686% \n",
      "[epoch:82, iter:7843] Loss: 0.096 | Acc: 96.642% \n",
      "[epoch:82, iter:7844] Loss: 0.095 | Acc: 96.691% \n",
      "[epoch:82, iter:7845] Loss: 0.094 | Acc: 96.739% \n",
      "[epoch:82, iter:7846] Loss: 0.095 | Acc: 96.696% \n",
      "[epoch:82, iter:7847] Loss: 0.095 | Acc: 96.655% \n",
      "[epoch:82, iter:7848] Loss: 0.094 | Acc: 96.701% \n",
      "[epoch:82, iter:7849] Loss: 0.094 | Acc: 96.661% \n",
      "[epoch:82, iter:7850] Loss: 0.094 | Acc: 96.706% \n",
      "[epoch:82, iter:7851] Loss: 0.093 | Acc: 96.667% \n",
      "[epoch:82, iter:7852] Loss: 0.093 | Acc: 96.628% \n",
      "[epoch:82, iter:7853] Loss: 0.093 | Acc: 96.591% \n",
      "[epoch:82, iter:7854] Loss: 0.096 | Acc: 96.394% \n",
      "[epoch:82, iter:7855] Loss: 0.096 | Acc: 96.361% \n",
      "[epoch:82, iter:7856] Loss: 0.095 | Acc: 96.406% \n",
      "[epoch:82, iter:7857] Loss: 0.096 | Acc: 96.373% \n",
      "[epoch:82, iter:7858] Loss: 0.097 | Acc: 96.341% \n",
      "[epoch:82, iter:7859] Loss: 0.096 | Acc: 96.386% \n",
      "[epoch:82, iter:7860] Loss: 0.095 | Acc: 96.429% \n",
      "[epoch:82, iter:7861] Loss: 0.094 | Acc: 96.471% \n",
      "[epoch:82, iter:7862] Loss: 0.093 | Acc: 96.512% \n",
      "[epoch:82, iter:7863] Loss: 0.092 | Acc: 96.552% \n",
      "[epoch:82, iter:7864] Loss: 0.092 | Acc: 96.591% \n",
      "[epoch:82, iter:7865] Loss: 0.092 | Acc: 96.559% \n",
      "[epoch:82, iter:7866] Loss: 0.092 | Acc: 96.597% \n",
      "[epoch:82, iter:7867] Loss: 0.091 | Acc: 96.635% \n",
      "[epoch:82, iter:7868] Loss: 0.093 | Acc: 96.603% \n",
      "[epoch:82, iter:7869] Loss: 0.094 | Acc: 96.573% \n",
      "[epoch:82, iter:7870] Loss: 0.093 | Acc: 96.609% \n",
      "[epoch:82, iter:7871] Loss: 0.093 | Acc: 96.579% \n",
      "[epoch:82, iter:7872] Loss: 0.092 | Acc: 96.599% \n",
      "Waiting Test!\n",
      "测试分类准确率为：76.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 83\n",
      "[epoch:83, iter:7873] Loss: 0.012 | Acc: 100.000% \n",
      "[epoch:83, iter:7874] Loss: 0.023 | Acc: 100.000% \n",
      "[epoch:83, iter:7875] Loss: 0.019 | Acc: 100.000% \n",
      "[epoch:83, iter:7876] Loss: 0.015 | Acc: 100.000% \n",
      "[epoch:83, iter:7877] Loss: 0.013 | Acc: 100.000% \n",
      "[epoch:83, iter:7878] Loss: 0.014 | Acc: 100.000% \n",
      "[epoch:83, iter:7879] Loss: 0.024 | Acc: 99.107% \n",
      "[epoch:83, iter:7880] Loss: 0.029 | Acc: 98.438% \n",
      "[epoch:83, iter:7881] Loss: 0.026 | Acc: 98.611% \n",
      "[epoch:83, iter:7882] Loss: 0.031 | Acc: 98.125% \n",
      "[epoch:83, iter:7883] Loss: 0.029 | Acc: 98.295% \n",
      "[epoch:83, iter:7884] Loss: 0.027 | Acc: 98.438% \n",
      "[epoch:83, iter:7885] Loss: 0.044 | Acc: 97.596% \n",
      "[epoch:83, iter:7886] Loss: 0.043 | Acc: 97.768% \n",
      "[epoch:83, iter:7887] Loss: 0.044 | Acc: 97.500% \n",
      "[epoch:83, iter:7888] Loss: 0.042 | Acc: 97.656% \n",
      "[epoch:83, iter:7889] Loss: 0.041 | Acc: 97.794% \n",
      "[epoch:83, iter:7890] Loss: 0.040 | Acc: 97.917% \n",
      "[epoch:83, iter:7891] Loss: 0.039 | Acc: 98.026% \n",
      "[epoch:83, iter:7892] Loss: 0.038 | Acc: 98.125% \n",
      "[epoch:83, iter:7893] Loss: 0.040 | Acc: 97.917% \n",
      "[epoch:83, iter:7894] Loss: 0.039 | Acc: 98.011% \n",
      "[epoch:83, iter:7895] Loss: 0.040 | Acc: 98.098% \n",
      "[epoch:83, iter:7896] Loss: 0.039 | Acc: 98.177% \n",
      "[epoch:83, iter:7897] Loss: 0.041 | Acc: 98.000% \n",
      "[epoch:83, iter:7898] Loss: 0.039 | Acc: 98.077% \n",
      "[epoch:83, iter:7899] Loss: 0.039 | Acc: 98.148% \n",
      "[epoch:83, iter:7900] Loss: 0.038 | Acc: 98.214% \n",
      "[epoch:83, iter:7901] Loss: 0.037 | Acc: 98.276% \n",
      "[epoch:83, iter:7902] Loss: 0.036 | Acc: 98.333% \n",
      "[epoch:83, iter:7903] Loss: 0.036 | Acc: 98.387% \n",
      "[epoch:83, iter:7904] Loss: 0.035 | Acc: 98.438% \n",
      "[epoch:83, iter:7905] Loss: 0.036 | Acc: 98.295% \n",
      "[epoch:83, iter:7906] Loss: 0.035 | Acc: 98.346% \n",
      "[epoch:83, iter:7907] Loss: 0.034 | Acc: 98.393% \n",
      "[epoch:83, iter:7908] Loss: 0.033 | Acc: 98.438% \n",
      "[epoch:83, iter:7909] Loss: 0.038 | Acc: 98.142% \n",
      "[epoch:83, iter:7910] Loss: 0.037 | Acc: 98.191% \n",
      "[epoch:83, iter:7911] Loss: 0.036 | Acc: 98.237% \n",
      "[epoch:83, iter:7912] Loss: 0.035 | Acc: 98.281% \n",
      "[epoch:83, iter:7913] Loss: 0.035 | Acc: 98.323% \n",
      "[epoch:83, iter:7914] Loss: 0.035 | Acc: 98.363% \n",
      "[epoch:83, iter:7915] Loss: 0.034 | Acc: 98.401% \n",
      "[epoch:83, iter:7916] Loss: 0.037 | Acc: 98.295% \n",
      "[epoch:83, iter:7917] Loss: 0.037 | Acc: 98.194% \n",
      "[epoch:83, iter:7918] Loss: 0.037 | Acc: 98.234% \n",
      "[epoch:83, iter:7919] Loss: 0.037 | Acc: 98.138% \n",
      "[epoch:83, iter:7920] Loss: 0.036 | Acc: 98.177% \n",
      "[epoch:83, iter:7921] Loss: 0.036 | Acc: 98.214% \n",
      "[epoch:83, iter:7922] Loss: 0.035 | Acc: 98.250% \n",
      "[epoch:83, iter:7923] Loss: 0.036 | Acc: 98.162% \n",
      "[epoch:83, iter:7924] Loss: 0.035 | Acc: 98.197% \n",
      "[epoch:83, iter:7925] Loss: 0.035 | Acc: 98.231% \n",
      "[epoch:83, iter:7926] Loss: 0.034 | Acc: 98.264% \n",
      "[epoch:83, iter:7927] Loss: 0.035 | Acc: 98.182% \n",
      "[epoch:83, iter:7928] Loss: 0.035 | Acc: 98.214% \n",
      "[epoch:83, iter:7929] Loss: 0.034 | Acc: 98.246% \n",
      "[epoch:83, iter:7930] Loss: 0.034 | Acc: 98.276% \n",
      "[epoch:83, iter:7931] Loss: 0.033 | Acc: 98.305% \n",
      "[epoch:83, iter:7932] Loss: 0.033 | Acc: 98.333% \n",
      "[epoch:83, iter:7933] Loss: 0.032 | Acc: 98.361% \n",
      "[epoch:83, iter:7934] Loss: 0.032 | Acc: 98.387% \n",
      "[epoch:83, iter:7935] Loss: 0.034 | Acc: 98.313% \n",
      "[epoch:83, iter:7936] Loss: 0.033 | Acc: 98.340% \n",
      "[epoch:83, iter:7937] Loss: 0.033 | Acc: 98.365% \n",
      "[epoch:83, iter:7938] Loss: 0.032 | Acc: 98.390% \n",
      "[epoch:83, iter:7939] Loss: 0.032 | Acc: 98.414% \n",
      "[epoch:83, iter:7940] Loss: 0.034 | Acc: 98.346% \n",
      "[epoch:83, iter:7941] Loss: 0.033 | Acc: 98.370% \n",
      "[epoch:83, iter:7942] Loss: 0.033 | Acc: 98.393% \n",
      "[epoch:83, iter:7943] Loss: 0.033 | Acc: 98.415% \n",
      "[epoch:83, iter:7944] Loss: 0.033 | Acc: 98.438% \n",
      "[epoch:83, iter:7945] Loss: 0.032 | Acc: 98.459% \n",
      "[epoch:83, iter:7946] Loss: 0.032 | Acc: 98.480% \n",
      "[epoch:83, iter:7947] Loss: 0.033 | Acc: 98.417% \n",
      "[epoch:83, iter:7948] Loss: 0.033 | Acc: 98.355% \n",
      "[epoch:83, iter:7949] Loss: 0.034 | Acc: 98.295% \n",
      "[epoch:83, iter:7950] Loss: 0.035 | Acc: 98.317% \n",
      "[epoch:83, iter:7951] Loss: 0.034 | Acc: 98.339% \n",
      "[epoch:83, iter:7952] Loss: 0.035 | Acc: 98.281% \n",
      "[epoch:83, iter:7953] Loss: 0.034 | Acc: 98.302% \n",
      "[epoch:83, iter:7954] Loss: 0.034 | Acc: 98.323% \n",
      "[epoch:83, iter:7955] Loss: 0.034 | Acc: 98.343% \n",
      "[epoch:83, iter:7956] Loss: 0.036 | Acc: 98.289% \n",
      "[epoch:83, iter:7957] Loss: 0.036 | Acc: 98.309% \n",
      "[epoch:83, iter:7958] Loss: 0.036 | Acc: 98.328% \n",
      "[epoch:83, iter:7959] Loss: 0.035 | Acc: 98.348% \n",
      "[epoch:83, iter:7960] Loss: 0.036 | Acc: 98.295% \n",
      "[epoch:83, iter:7961] Loss: 0.037 | Acc: 98.315% \n",
      "[epoch:83, iter:7962] Loss: 0.036 | Acc: 98.333% \n",
      "[epoch:83, iter:7963] Loss: 0.038 | Acc: 98.283% \n",
      "[epoch:83, iter:7964] Loss: 0.037 | Acc: 98.302% \n",
      "[epoch:83, iter:7965] Loss: 0.039 | Acc: 98.253% \n",
      "[epoch:83, iter:7966] Loss: 0.039 | Acc: 98.271% \n",
      "[epoch:83, iter:7967] Loss: 0.042 | Acc: 98.158% \n",
      "[epoch:83, iter:7968] Loss: 0.041 | Acc: 98.169% \n",
      "Waiting Test!\n",
      "测试分类准确率为：86.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 84\n",
      "[epoch:84, iter:7969] Loss: 0.063 | Acc: 93.750% \n",
      "[epoch:84, iter:7970] Loss: 0.032 | Acc: 96.875% \n",
      "[epoch:84, iter:7971] Loss: 0.024 | Acc: 97.917% \n",
      "[epoch:84, iter:7972] Loss: 0.033 | Acc: 96.875% \n",
      "[epoch:84, iter:7973] Loss: 0.026 | Acc: 97.500% \n",
      "[epoch:84, iter:7974] Loss: 0.022 | Acc: 97.917% \n",
      "[epoch:84, iter:7975] Loss: 0.020 | Acc: 98.214% \n",
      "[epoch:84, iter:7976] Loss: 0.019 | Acc: 98.438% \n",
      "[epoch:84, iter:7977] Loss: 0.017 | Acc: 98.611% \n",
      "[epoch:84, iter:7978] Loss: 0.016 | Acc: 98.750% \n",
      "[epoch:84, iter:7979] Loss: 0.014 | Acc: 98.864% \n",
      "[epoch:84, iter:7980] Loss: 0.018 | Acc: 98.438% \n",
      "[epoch:84, iter:7981] Loss: 0.019 | Acc: 98.558% \n",
      "[epoch:84, iter:7982] Loss: 0.021 | Acc: 98.214% \n",
      "[epoch:84, iter:7983] Loss: 0.020 | Acc: 98.333% \n",
      "[epoch:84, iter:7984] Loss: 0.019 | Acc: 98.438% \n",
      "[epoch:84, iter:7985] Loss: 0.020 | Acc: 98.529% \n",
      "[epoch:84, iter:7986] Loss: 0.020 | Acc: 98.611% \n",
      "[epoch:84, iter:7987] Loss: 0.019 | Acc: 98.684% \n",
      "[epoch:84, iter:7988] Loss: 0.019 | Acc: 98.750% \n",
      "[epoch:84, iter:7989] Loss: 0.019 | Acc: 98.810% \n",
      "[epoch:84, iter:7990] Loss: 0.018 | Acc: 98.864% \n",
      "[epoch:84, iter:7991] Loss: 0.022 | Acc: 98.641% \n",
      "[epoch:84, iter:7992] Loss: 0.024 | Acc: 98.698% \n",
      "[epoch:84, iter:7993] Loss: 0.024 | Acc: 98.750% \n",
      "[epoch:84, iter:7994] Loss: 0.024 | Acc: 98.798% \n",
      "[epoch:84, iter:7995] Loss: 0.023 | Acc: 98.843% \n",
      "[epoch:84, iter:7996] Loss: 0.025 | Acc: 98.661% \n",
      "[epoch:84, iter:7997] Loss: 0.025 | Acc: 98.707% \n",
      "[epoch:84, iter:7998] Loss: 0.024 | Acc: 98.750% \n",
      "[epoch:84, iter:7999] Loss: 0.024 | Acc: 98.790% \n",
      "[epoch:84, iter:8000] Loss: 0.024 | Acc: 98.828% \n",
      "[epoch:84, iter:8001] Loss: 0.023 | Acc: 98.864% \n",
      "[epoch:84, iter:8002] Loss: 0.023 | Acc: 98.897% \n",
      "[epoch:84, iter:8003] Loss: 0.023 | Acc: 98.929% \n",
      "[epoch:84, iter:8004] Loss: 0.022 | Acc: 98.958% \n",
      "[epoch:84, iter:8005] Loss: 0.023 | Acc: 98.986% \n",
      "[epoch:84, iter:8006] Loss: 0.022 | Acc: 99.013% \n",
      "[epoch:84, iter:8007] Loss: 0.022 | Acc: 99.038% \n",
      "[epoch:84, iter:8008] Loss: 0.021 | Acc: 99.062% \n",
      "[epoch:84, iter:8009] Loss: 0.022 | Acc: 98.933% \n",
      "[epoch:84, iter:8010] Loss: 0.022 | Acc: 98.958% \n",
      "[epoch:84, iter:8011] Loss: 0.023 | Acc: 98.837% \n",
      "[epoch:84, iter:8012] Loss: 0.022 | Acc: 98.864% \n",
      "[epoch:84, iter:8013] Loss: 0.022 | Acc: 98.889% \n",
      "[epoch:84, iter:8014] Loss: 0.023 | Acc: 98.913% \n",
      "[epoch:84, iter:8015] Loss: 0.022 | Acc: 98.936% \n",
      "[epoch:84, iter:8016] Loss: 0.022 | Acc: 98.958% \n",
      "[epoch:84, iter:8017] Loss: 0.022 | Acc: 98.980% \n",
      "[epoch:84, iter:8018] Loss: 0.021 | Acc: 99.000% \n",
      "[epoch:84, iter:8019] Loss: 0.021 | Acc: 99.020% \n",
      "[epoch:84, iter:8020] Loss: 0.021 | Acc: 99.038% \n",
      "[epoch:84, iter:8021] Loss: 0.021 | Acc: 99.057% \n",
      "[epoch:84, iter:8022] Loss: 0.021 | Acc: 99.074% \n",
      "[epoch:84, iter:8023] Loss: 0.021 | Acc: 99.091% \n",
      "[epoch:84, iter:8024] Loss: 0.020 | Acc: 99.107% \n",
      "[epoch:84, iter:8025] Loss: 0.022 | Acc: 99.013% \n",
      "[epoch:84, iter:8026] Loss: 0.022 | Acc: 99.030% \n",
      "[epoch:84, iter:8027] Loss: 0.022 | Acc: 99.047% \n",
      "[epoch:84, iter:8028] Loss: 0.022 | Acc: 99.062% \n",
      "[epoch:84, iter:8029] Loss: 0.022 | Acc: 99.078% \n",
      "[epoch:84, iter:8030] Loss: 0.021 | Acc: 99.093% \n",
      "[epoch:84, iter:8031] Loss: 0.021 | Acc: 99.107% \n",
      "[epoch:84, iter:8032] Loss: 0.023 | Acc: 99.023% \n",
      "[epoch:84, iter:8033] Loss: 0.022 | Acc: 99.038% \n",
      "[epoch:84, iter:8034] Loss: 0.022 | Acc: 99.053% \n",
      "[epoch:84, iter:8035] Loss: 0.022 | Acc: 99.067% \n",
      "[epoch:84, iter:8036] Loss: 0.022 | Acc: 99.081% \n",
      "[epoch:84, iter:8037] Loss: 0.021 | Acc: 99.094% \n",
      "[epoch:84, iter:8038] Loss: 0.026 | Acc: 98.929% \n",
      "[epoch:84, iter:8039] Loss: 0.026 | Acc: 98.944% \n",
      "[epoch:84, iter:8040] Loss: 0.026 | Acc: 98.958% \n",
      "[epoch:84, iter:8041] Loss: 0.028 | Acc: 98.887% \n",
      "[epoch:84, iter:8042] Loss: 0.028 | Acc: 98.902% \n",
      "[epoch:84, iter:8043] Loss: 0.031 | Acc: 98.750% \n",
      "[epoch:84, iter:8044] Loss: 0.030 | Acc: 98.766% \n",
      "[epoch:84, iter:8045] Loss: 0.030 | Acc: 98.782% \n",
      "[epoch:84, iter:8046] Loss: 0.030 | Acc: 98.798% \n",
      "[epoch:84, iter:8047] Loss: 0.031 | Acc: 98.734% \n",
      "[epoch:84, iter:8048] Loss: 0.031 | Acc: 98.672% \n",
      "[epoch:84, iter:8049] Loss: 0.031 | Acc: 98.688% \n",
      "[epoch:84, iter:8050] Loss: 0.031 | Acc: 98.704% \n",
      "[epoch:84, iter:8051] Loss: 0.032 | Acc: 98.645% \n",
      "[epoch:84, iter:8052] Loss: 0.032 | Acc: 98.586% \n",
      "[epoch:84, iter:8053] Loss: 0.032 | Acc: 98.603% \n",
      "[epoch:84, iter:8054] Loss: 0.032 | Acc: 98.619% \n",
      "[epoch:84, iter:8055] Loss: 0.032 | Acc: 98.563% \n",
      "[epoch:84, iter:8056] Loss: 0.032 | Acc: 98.580% \n",
      "[epoch:84, iter:8057] Loss: 0.032 | Acc: 98.596% \n",
      "[epoch:84, iter:8058] Loss: 0.032 | Acc: 98.611% \n",
      "[epoch:84, iter:8059] Loss: 0.032 | Acc: 98.626% \n",
      "[epoch:84, iter:8060] Loss: 0.032 | Acc: 98.641% \n",
      "[epoch:84, iter:8061] Loss: 0.037 | Acc: 98.522% \n",
      "[epoch:84, iter:8062] Loss: 0.036 | Acc: 98.537% \n",
      "[epoch:84, iter:8063] Loss: 0.036 | Acc: 98.553% \n",
      "[epoch:84, iter:8064] Loss: 0.037 | Acc: 98.496% \n",
      "Waiting Test!\n",
      "测试分类准确率为：84.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 85\n",
      "[epoch:85, iter:8065] Loss: 0.007 | Acc: 100.000% \n",
      "[epoch:85, iter:8066] Loss: 0.006 | Acc: 100.000% \n",
      "[epoch:85, iter:8067] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:85, iter:8068] Loss: 0.007 | Acc: 100.000% \n",
      "[epoch:85, iter:8069] Loss: 0.006 | Acc: 100.000% \n",
      "[epoch:85, iter:8070] Loss: 0.034 | Acc: 98.958% \n",
      "[epoch:85, iter:8071] Loss: 0.032 | Acc: 99.107% \n",
      "[epoch:85, iter:8072] Loss: 0.028 | Acc: 99.219% \n",
      "[epoch:85, iter:8073] Loss: 0.027 | Acc: 99.306% \n",
      "[epoch:85, iter:8074] Loss: 0.026 | Acc: 99.375% \n",
      "[epoch:85, iter:8075] Loss: 0.023 | Acc: 99.432% \n",
      "[epoch:85, iter:8076] Loss: 0.024 | Acc: 99.479% \n",
      "[epoch:85, iter:8077] Loss: 0.027 | Acc: 99.038% \n",
      "[epoch:85, iter:8078] Loss: 0.025 | Acc: 99.107% \n",
      "[epoch:85, iter:8079] Loss: 0.027 | Acc: 98.750% \n",
      "[epoch:85, iter:8080] Loss: 0.026 | Acc: 98.828% \n",
      "[epoch:85, iter:8081] Loss: 0.025 | Acc: 98.897% \n",
      "[epoch:85, iter:8082] Loss: 0.027 | Acc: 98.611% \n",
      "[epoch:85, iter:8083] Loss: 0.027 | Acc: 98.684% \n",
      "[epoch:85, iter:8084] Loss: 0.026 | Acc: 98.750% \n",
      "[epoch:85, iter:8085] Loss: 0.025 | Acc: 98.810% \n",
      "[epoch:85, iter:8086] Loss: 0.026 | Acc: 98.864% \n",
      "[epoch:85, iter:8087] Loss: 0.025 | Acc: 98.913% \n",
      "[epoch:85, iter:8088] Loss: 0.024 | Acc: 98.958% \n",
      "[epoch:85, iter:8089] Loss: 0.023 | Acc: 99.000% \n",
      "[epoch:85, iter:8090] Loss: 0.023 | Acc: 99.038% \n",
      "[epoch:85, iter:8091] Loss: 0.026 | Acc: 98.843% \n",
      "[epoch:85, iter:8092] Loss: 0.025 | Acc: 98.884% \n",
      "[epoch:85, iter:8093] Loss: 0.024 | Acc: 98.922% \n",
      "[epoch:85, iter:8094] Loss: 0.024 | Acc: 98.958% \n",
      "[epoch:85, iter:8095] Loss: 0.023 | Acc: 98.992% \n",
      "[epoch:85, iter:8096] Loss: 0.025 | Acc: 98.633% \n",
      "[epoch:85, iter:8097] Loss: 0.026 | Acc: 98.674% \n",
      "[epoch:85, iter:8098] Loss: 0.025 | Acc: 98.713% \n",
      "[epoch:85, iter:8099] Loss: 0.024 | Acc: 98.750% \n",
      "[epoch:85, iter:8100] Loss: 0.024 | Acc: 98.785% \n",
      "[epoch:85, iter:8101] Loss: 0.023 | Acc: 98.818% \n",
      "[epoch:85, iter:8102] Loss: 0.024 | Acc: 98.849% \n",
      "[epoch:85, iter:8103] Loss: 0.023 | Acc: 98.878% \n",
      "[epoch:85, iter:8104] Loss: 0.023 | Acc: 98.906% \n",
      "[epoch:85, iter:8105] Loss: 0.023 | Acc: 98.933% \n",
      "[epoch:85, iter:8106] Loss: 0.024 | Acc: 98.958% \n",
      "[epoch:85, iter:8107] Loss: 0.026 | Acc: 98.837% \n",
      "[epoch:85, iter:8108] Loss: 0.028 | Acc: 98.722% \n",
      "[epoch:85, iter:8109] Loss: 0.027 | Acc: 98.750% \n",
      "[epoch:85, iter:8110] Loss: 0.027 | Acc: 98.777% \n",
      "[epoch:85, iter:8111] Loss: 0.026 | Acc: 98.803% \n",
      "[epoch:85, iter:8112] Loss: 0.031 | Acc: 98.568% \n",
      "[epoch:85, iter:8113] Loss: 0.031 | Acc: 98.469% \n",
      "[epoch:85, iter:8114] Loss: 0.031 | Acc: 98.500% \n",
      "[epoch:85, iter:8115] Loss: 0.037 | Acc: 98.407% \n",
      "[epoch:85, iter:8116] Loss: 0.037 | Acc: 98.317% \n",
      "[epoch:85, iter:8117] Loss: 0.037 | Acc: 98.349% \n",
      "[epoch:85, iter:8118] Loss: 0.036 | Acc: 98.380% \n",
      "[epoch:85, iter:8119] Loss: 0.036 | Acc: 98.409% \n",
      "[epoch:85, iter:8120] Loss: 0.035 | Acc: 98.438% \n",
      "[epoch:85, iter:8121] Loss: 0.034 | Acc: 98.465% \n",
      "[epoch:85, iter:8122] Loss: 0.035 | Acc: 98.384% \n",
      "[epoch:85, iter:8123] Loss: 0.035 | Acc: 98.411% \n",
      "[epoch:85, iter:8124] Loss: 0.035 | Acc: 98.333% \n",
      "[epoch:85, iter:8125] Loss: 0.034 | Acc: 98.361% \n",
      "[epoch:85, iter:8126] Loss: 0.035 | Acc: 98.387% \n",
      "[epoch:85, iter:8127] Loss: 0.034 | Acc: 98.413% \n",
      "[epoch:85, iter:8128] Loss: 0.034 | Acc: 98.438% \n",
      "[epoch:85, iter:8129] Loss: 0.033 | Acc: 98.462% \n",
      "[epoch:85, iter:8130] Loss: 0.033 | Acc: 98.485% \n",
      "[epoch:85, iter:8131] Loss: 0.032 | Acc: 98.507% \n",
      "[epoch:85, iter:8132] Loss: 0.033 | Acc: 98.438% \n",
      "[epoch:85, iter:8133] Loss: 0.033 | Acc: 98.460% \n",
      "[epoch:85, iter:8134] Loss: 0.033 | Acc: 98.482% \n",
      "[epoch:85, iter:8135] Loss: 0.032 | Acc: 98.504% \n",
      "[epoch:85, iter:8136] Loss: 0.032 | Acc: 98.524% \n",
      "[epoch:85, iter:8137] Loss: 0.032 | Acc: 98.545% \n",
      "[epoch:85, iter:8138] Loss: 0.031 | Acc: 98.564% \n",
      "[epoch:85, iter:8139] Loss: 0.031 | Acc: 98.583% \n",
      "[epoch:85, iter:8140] Loss: 0.031 | Acc: 98.520% \n",
      "[epoch:85, iter:8141] Loss: 0.032 | Acc: 98.458% \n",
      "[epoch:85, iter:8142] Loss: 0.031 | Acc: 98.478% \n",
      "[epoch:85, iter:8143] Loss: 0.031 | Acc: 98.497% \n",
      "[epoch:85, iter:8144] Loss: 0.031 | Acc: 98.438% \n",
      "[epoch:85, iter:8145] Loss: 0.031 | Acc: 98.457% \n",
      "[epoch:85, iter:8146] Loss: 0.031 | Acc: 98.476% \n",
      "[epoch:85, iter:8147] Loss: 0.031 | Acc: 98.494% \n",
      "[epoch:85, iter:8148] Loss: 0.031 | Acc: 98.438% \n",
      "[epoch:85, iter:8149] Loss: 0.033 | Acc: 98.309% \n",
      "[epoch:85, iter:8150] Loss: 0.033 | Acc: 98.328% \n",
      "[epoch:85, iter:8151] Loss: 0.033 | Acc: 98.348% \n",
      "[epoch:85, iter:8152] Loss: 0.036 | Acc: 98.295% \n",
      "[epoch:85, iter:8153] Loss: 0.036 | Acc: 98.315% \n",
      "[epoch:85, iter:8154] Loss: 0.036 | Acc: 98.333% \n",
      "[epoch:85, iter:8155] Loss: 0.036 | Acc: 98.283% \n",
      "[epoch:85, iter:8156] Loss: 0.036 | Acc: 98.302% \n",
      "[epoch:85, iter:8157] Loss: 0.037 | Acc: 98.253% \n",
      "[epoch:85, iter:8158] Loss: 0.037 | Acc: 98.271% \n",
      "[epoch:85, iter:8159] Loss: 0.036 | Acc: 98.289% \n",
      "[epoch:85, iter:8160] Loss: 0.037 | Acc: 98.300% \n",
      "Waiting Test!\n",
      "测试分类准确率为：78.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 86\n",
      "[epoch:86, iter:8161] Loss: 0.043 | Acc: 100.000% \n",
      "[epoch:86, iter:8162] Loss: 0.022 | Acc: 100.000% \n",
      "[epoch:86, iter:8163] Loss: 0.015 | Acc: 100.000% \n",
      "[epoch:86, iter:8164] Loss: 0.016 | Acc: 100.000% \n",
      "[epoch:86, iter:8165] Loss: 0.013 | Acc: 100.000% \n",
      "[epoch:86, iter:8166] Loss: 0.013 | Acc: 100.000% \n",
      "[epoch:86, iter:8167] Loss: 0.016 | Acc: 100.000% \n",
      "[epoch:86, iter:8168] Loss: 0.021 | Acc: 100.000% \n",
      "[epoch:86, iter:8169] Loss: 0.022 | Acc: 100.000% \n",
      "[epoch:86, iter:8170] Loss: 0.023 | Acc: 100.000% \n",
      "[epoch:86, iter:8171] Loss: 0.021 | Acc: 100.000% \n",
      "[epoch:86, iter:8172] Loss: 0.020 | Acc: 100.000% \n",
      "[epoch:86, iter:8173] Loss: 0.018 | Acc: 100.000% \n",
      "[epoch:86, iter:8174] Loss: 0.017 | Acc: 100.000% \n",
      "[epoch:86, iter:8175] Loss: 0.017 | Acc: 100.000% \n",
      "[epoch:86, iter:8176] Loss: 0.017 | Acc: 100.000% \n",
      "[epoch:86, iter:8177] Loss: 0.016 | Acc: 100.000% \n",
      "[epoch:86, iter:8178] Loss: 0.018 | Acc: 99.653% \n",
      "[epoch:86, iter:8179] Loss: 0.020 | Acc: 99.342% \n",
      "[epoch:86, iter:8180] Loss: 0.019 | Acc: 99.375% \n",
      "[epoch:86, iter:8181] Loss: 0.021 | Acc: 99.107% \n",
      "[epoch:86, iter:8182] Loss: 0.029 | Acc: 98.864% \n",
      "[epoch:86, iter:8183] Loss: 0.028 | Acc: 98.913% \n",
      "[epoch:86, iter:8184] Loss: 0.027 | Acc: 98.958% \n",
      "[epoch:86, iter:8185] Loss: 0.026 | Acc: 99.000% \n",
      "[epoch:86, iter:8186] Loss: 0.025 | Acc: 99.038% \n",
      "[epoch:86, iter:8187] Loss: 0.024 | Acc: 99.074% \n",
      "[epoch:86, iter:8188] Loss: 0.023 | Acc: 99.107% \n",
      "[epoch:86, iter:8189] Loss: 0.024 | Acc: 98.922% \n",
      "[epoch:86, iter:8190] Loss: 0.024 | Acc: 98.958% \n",
      "[epoch:86, iter:8191] Loss: 0.025 | Acc: 98.790% \n",
      "[epoch:86, iter:8192] Loss: 0.025 | Acc: 98.828% \n",
      "[epoch:86, iter:8193] Loss: 0.024 | Acc: 98.864% \n",
      "[epoch:86, iter:8194] Loss: 0.025 | Acc: 98.713% \n",
      "[epoch:86, iter:8195] Loss: 0.024 | Acc: 98.750% \n",
      "[epoch:86, iter:8196] Loss: 0.025 | Acc: 98.785% \n",
      "[epoch:86, iter:8197] Loss: 0.024 | Acc: 98.818% \n",
      "[epoch:86, iter:8198] Loss: 0.025 | Acc: 98.849% \n",
      "[epoch:86, iter:8199] Loss: 0.024 | Acc: 98.878% \n",
      "[epoch:86, iter:8200] Loss: 0.025 | Acc: 98.750% \n",
      "[epoch:86, iter:8201] Loss: 0.030 | Acc: 98.628% \n",
      "[epoch:86, iter:8202] Loss: 0.031 | Acc: 98.512% \n",
      "[epoch:86, iter:8203] Loss: 0.030 | Acc: 98.547% \n",
      "[epoch:86, iter:8204] Loss: 0.032 | Acc: 98.295% \n",
      "[epoch:86, iter:8205] Loss: 0.031 | Acc: 98.333% \n",
      "[epoch:86, iter:8206] Loss: 0.031 | Acc: 98.370% \n",
      "[epoch:86, iter:8207] Loss: 0.030 | Acc: 98.404% \n",
      "[epoch:86, iter:8208] Loss: 0.032 | Acc: 98.177% \n",
      "[epoch:86, iter:8209] Loss: 0.031 | Acc: 98.214% \n",
      "[epoch:86, iter:8210] Loss: 0.033 | Acc: 98.125% \n",
      "[epoch:86, iter:8211] Loss: 0.033 | Acc: 98.039% \n",
      "[epoch:86, iter:8212] Loss: 0.033 | Acc: 98.077% \n",
      "[epoch:86, iter:8213] Loss: 0.032 | Acc: 98.113% \n",
      "[epoch:86, iter:8214] Loss: 0.033 | Acc: 98.032% \n",
      "[epoch:86, iter:8215] Loss: 0.032 | Acc: 98.068% \n",
      "[epoch:86, iter:8216] Loss: 0.032 | Acc: 97.991% \n",
      "[epoch:86, iter:8217] Loss: 0.032 | Acc: 98.026% \n",
      "[epoch:86, iter:8218] Loss: 0.031 | Acc: 98.060% \n",
      "[epoch:86, iter:8219] Loss: 0.032 | Acc: 97.987% \n",
      "[epoch:86, iter:8220] Loss: 0.032 | Acc: 98.021% \n",
      "[epoch:86, iter:8221] Loss: 0.031 | Acc: 98.053% \n",
      "[epoch:86, iter:8222] Loss: 0.031 | Acc: 98.085% \n",
      "[epoch:86, iter:8223] Loss: 0.031 | Acc: 98.115% \n",
      "[epoch:86, iter:8224] Loss: 0.031 | Acc: 98.145% \n",
      "[epoch:86, iter:8225] Loss: 0.032 | Acc: 98.077% \n",
      "[epoch:86, iter:8226] Loss: 0.032 | Acc: 98.011% \n",
      "[epoch:86, iter:8227] Loss: 0.032 | Acc: 98.041% \n",
      "[epoch:86, iter:8228] Loss: 0.032 | Acc: 98.070% \n",
      "[epoch:86, iter:8229] Loss: 0.031 | Acc: 98.098% \n",
      "[epoch:86, iter:8230] Loss: 0.031 | Acc: 98.125% \n",
      "[epoch:86, iter:8231] Loss: 0.032 | Acc: 98.063% \n",
      "[epoch:86, iter:8232] Loss: 0.032 | Acc: 98.090% \n",
      "[epoch:86, iter:8233] Loss: 0.031 | Acc: 98.116% \n",
      "[epoch:86, iter:8234] Loss: 0.032 | Acc: 98.057% \n",
      "[epoch:86, iter:8235] Loss: 0.032 | Acc: 98.083% \n",
      "[epoch:86, iter:8236] Loss: 0.031 | Acc: 98.109% \n",
      "[epoch:86, iter:8237] Loss: 0.031 | Acc: 98.133% \n",
      "[epoch:86, iter:8238] Loss: 0.031 | Acc: 98.157% \n",
      "[epoch:86, iter:8239] Loss: 0.031 | Acc: 98.180% \n",
      "[epoch:86, iter:8240] Loss: 0.031 | Acc: 98.203% \n",
      "[epoch:86, iter:8241] Loss: 0.031 | Acc: 98.148% \n",
      "[epoch:86, iter:8242] Loss: 0.031 | Acc: 98.171% \n",
      "[epoch:86, iter:8243] Loss: 0.031 | Acc: 98.193% \n",
      "[epoch:86, iter:8244] Loss: 0.030 | Acc: 98.214% \n",
      "[epoch:86, iter:8245] Loss: 0.030 | Acc: 98.235% \n",
      "[epoch:86, iter:8246] Loss: 0.030 | Acc: 98.256% \n",
      "[epoch:86, iter:8247] Loss: 0.030 | Acc: 98.276% \n",
      "[epoch:86, iter:8248] Loss: 0.030 | Acc: 98.224% \n",
      "[epoch:86, iter:8249] Loss: 0.030 | Acc: 98.244% \n",
      "[epoch:86, iter:8250] Loss: 0.029 | Acc: 98.264% \n",
      "[epoch:86, iter:8251] Loss: 0.029 | Acc: 98.283% \n",
      "[epoch:86, iter:8252] Loss: 0.029 | Acc: 98.302% \n",
      "[epoch:86, iter:8253] Loss: 0.028 | Acc: 98.320% \n",
      "[epoch:86, iter:8254] Loss: 0.029 | Acc: 98.271% \n",
      "[epoch:86, iter:8255] Loss: 0.028 | Acc: 98.289% \n",
      "[epoch:86, iter:8256] Loss: 0.028 | Acc: 98.300% \n",
      "Waiting Test!\n",
      "测试分类准确率为：80.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 87\n",
      "[epoch:87, iter:8257] Loss: 0.042 | Acc: 100.000% \n",
      "[epoch:87, iter:8258] Loss: 0.022 | Acc: 100.000% \n",
      "[epoch:87, iter:8259] Loss: 0.015 | Acc: 100.000% \n",
      "[epoch:87, iter:8260] Loss: 0.017 | Acc: 100.000% \n",
      "[epoch:87, iter:8261] Loss: 0.014 | Acc: 100.000% \n",
      "[epoch:87, iter:8262] Loss: 0.016 | Acc: 100.000% \n",
      "[epoch:87, iter:8263] Loss: 0.019 | Acc: 100.000% \n",
      "[epoch:87, iter:8264] Loss: 0.025 | Acc: 99.219% \n",
      "[epoch:87, iter:8265] Loss: 0.022 | Acc: 99.306% \n",
      "[epoch:87, iter:8266] Loss: 0.021 | Acc: 99.375% \n",
      "[epoch:87, iter:8267] Loss: 0.019 | Acc: 99.432% \n",
      "[epoch:87, iter:8268] Loss: 0.017 | Acc: 99.479% \n",
      "[epoch:87, iter:8269] Loss: 0.016 | Acc: 99.519% \n",
      "[epoch:87, iter:8270] Loss: 0.015 | Acc: 99.554% \n",
      "[epoch:87, iter:8271] Loss: 0.014 | Acc: 99.583% \n",
      "[epoch:87, iter:8272] Loss: 0.013 | Acc: 99.609% \n",
      "[epoch:87, iter:8273] Loss: 0.013 | Acc: 99.632% \n",
      "[epoch:87, iter:8274] Loss: 0.014 | Acc: 99.653% \n",
      "[epoch:87, iter:8275] Loss: 0.014 | Acc: 99.671% \n",
      "[epoch:87, iter:8276] Loss: 0.013 | Acc: 99.688% \n",
      "[epoch:87, iter:8277] Loss: 0.013 | Acc: 99.702% \n",
      "[epoch:87, iter:8278] Loss: 0.012 | Acc: 99.716% \n",
      "[epoch:87, iter:8279] Loss: 0.012 | Acc: 99.728% \n",
      "[epoch:87, iter:8280] Loss: 0.011 | Acc: 99.740% \n",
      "[epoch:87, iter:8281] Loss: 0.011 | Acc: 99.750% \n",
      "[epoch:87, iter:8282] Loss: 0.010 | Acc: 99.760% \n",
      "[epoch:87, iter:8283] Loss: 0.010 | Acc: 99.769% \n",
      "[epoch:87, iter:8284] Loss: 0.010 | Acc: 99.777% \n",
      "[epoch:87, iter:8285] Loss: 0.012 | Acc: 99.569% \n",
      "[epoch:87, iter:8286] Loss: 0.012 | Acc: 99.583% \n",
      "[epoch:87, iter:8287] Loss: 0.012 | Acc: 99.597% \n",
      "[epoch:87, iter:8288] Loss: 0.012 | Acc: 99.609% \n",
      "[epoch:87, iter:8289] Loss: 0.012 | Acc: 99.621% \n",
      "[epoch:87, iter:8290] Loss: 0.013 | Acc: 99.632% \n",
      "[epoch:87, iter:8291] Loss: 0.012 | Acc: 99.643% \n",
      "[epoch:87, iter:8292] Loss: 0.013 | Acc: 99.653% \n",
      "[epoch:87, iter:8293] Loss: 0.013 | Acc: 99.662% \n",
      "[epoch:87, iter:8294] Loss: 0.019 | Acc: 99.342% \n",
      "[epoch:87, iter:8295] Loss: 0.020 | Acc: 99.359% \n",
      "[epoch:87, iter:8296] Loss: 0.019 | Acc: 99.375% \n",
      "[epoch:87, iter:8297] Loss: 0.019 | Acc: 99.390% \n",
      "[epoch:87, iter:8298] Loss: 0.018 | Acc: 99.405% \n",
      "[epoch:87, iter:8299] Loss: 0.018 | Acc: 99.419% \n",
      "[epoch:87, iter:8300] Loss: 0.018 | Acc: 99.432% \n",
      "[epoch:87, iter:8301] Loss: 0.018 | Acc: 99.444% \n",
      "[epoch:87, iter:8302] Loss: 0.017 | Acc: 99.457% \n",
      "[epoch:87, iter:8303] Loss: 0.017 | Acc: 99.468% \n",
      "[epoch:87, iter:8304] Loss: 0.017 | Acc: 99.479% \n",
      "[epoch:87, iter:8305] Loss: 0.017 | Acc: 99.362% \n",
      "[epoch:87, iter:8306] Loss: 0.017 | Acc: 99.375% \n",
      "[epoch:87, iter:8307] Loss: 0.017 | Acc: 99.387% \n",
      "[epoch:87, iter:8308] Loss: 0.016 | Acc: 99.399% \n",
      "[epoch:87, iter:8309] Loss: 0.018 | Acc: 99.292% \n",
      "[epoch:87, iter:8310] Loss: 0.017 | Acc: 99.306% \n",
      "[epoch:87, iter:8311] Loss: 0.017 | Acc: 99.318% \n",
      "[epoch:87, iter:8312] Loss: 0.017 | Acc: 99.330% \n",
      "[epoch:87, iter:8313] Loss: 0.016 | Acc: 99.342% \n",
      "[epoch:87, iter:8314] Loss: 0.017 | Acc: 99.353% \n",
      "[epoch:87, iter:8315] Loss: 0.018 | Acc: 99.364% \n",
      "[epoch:87, iter:8316] Loss: 0.017 | Acc: 99.375% \n",
      "[epoch:87, iter:8317] Loss: 0.019 | Acc: 99.180% \n",
      "[epoch:87, iter:8318] Loss: 0.019 | Acc: 99.194% \n",
      "[epoch:87, iter:8319] Loss: 0.018 | Acc: 99.206% \n",
      "[epoch:87, iter:8320] Loss: 0.018 | Acc: 99.219% \n",
      "[epoch:87, iter:8321] Loss: 0.018 | Acc: 99.231% \n",
      "[epoch:87, iter:8322] Loss: 0.018 | Acc: 99.242% \n",
      "[epoch:87, iter:8323] Loss: 0.018 | Acc: 99.254% \n",
      "[epoch:87, iter:8324] Loss: 0.017 | Acc: 99.265% \n",
      "[epoch:87, iter:8325] Loss: 0.017 | Acc: 99.275% \n",
      "[epoch:87, iter:8326] Loss: 0.017 | Acc: 99.286% \n",
      "[epoch:87, iter:8327] Loss: 0.019 | Acc: 99.120% \n",
      "[epoch:87, iter:8328] Loss: 0.019 | Acc: 99.132% \n",
      "[epoch:87, iter:8329] Loss: 0.019 | Acc: 99.144% \n",
      "[epoch:87, iter:8330] Loss: 0.018 | Acc: 99.155% \n",
      "[epoch:87, iter:8331] Loss: 0.018 | Acc: 99.167% \n",
      "[epoch:87, iter:8332] Loss: 0.020 | Acc: 99.013% \n",
      "[epoch:87, iter:8333] Loss: 0.019 | Acc: 99.026% \n",
      "[epoch:87, iter:8334] Loss: 0.020 | Acc: 99.038% \n",
      "[epoch:87, iter:8335] Loss: 0.020 | Acc: 99.051% \n",
      "[epoch:87, iter:8336] Loss: 0.019 | Acc: 99.062% \n",
      "[epoch:87, iter:8337] Loss: 0.020 | Acc: 99.074% \n",
      "[epoch:87, iter:8338] Loss: 0.020 | Acc: 99.085% \n",
      "[epoch:87, iter:8339] Loss: 0.020 | Acc: 99.096% \n",
      "[epoch:87, iter:8340] Loss: 0.019 | Acc: 99.107% \n",
      "[epoch:87, iter:8341] Loss: 0.019 | Acc: 99.118% \n",
      "[epoch:87, iter:8342] Loss: 0.020 | Acc: 99.055% \n",
      "[epoch:87, iter:8343] Loss: 0.020 | Acc: 98.922% \n",
      "[epoch:87, iter:8344] Loss: 0.021 | Acc: 98.864% \n",
      "[epoch:87, iter:8345] Loss: 0.021 | Acc: 98.876% \n",
      "[epoch:87, iter:8346] Loss: 0.021 | Acc: 98.889% \n",
      "[epoch:87, iter:8347] Loss: 0.021 | Acc: 98.901% \n",
      "[epoch:87, iter:8348] Loss: 0.021 | Acc: 98.845% \n",
      "[epoch:87, iter:8349] Loss: 0.021 | Acc: 98.858% \n",
      "[epoch:87, iter:8350] Loss: 0.022 | Acc: 98.803% \n",
      "[epoch:87, iter:8351] Loss: 0.022 | Acc: 98.816% \n",
      "[epoch:87, iter:8352] Loss: 0.022 | Acc: 98.823% \n",
      "Waiting Test!\n",
      "测试分类准确率为：82.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 88\n",
      "[epoch:88, iter:8353] Loss: 0.033 | Acc: 100.000% \n",
      "[epoch:88, iter:8354] Loss: 0.035 | Acc: 100.000% \n",
      "[epoch:88, iter:8355] Loss: 0.039 | Acc: 97.917% \n",
      "[epoch:88, iter:8356] Loss: 0.029 | Acc: 98.438% \n",
      "[epoch:88, iter:8357] Loss: 0.023 | Acc: 98.750% \n",
      "[epoch:88, iter:8358] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:88, iter:8359] Loss: 0.017 | Acc: 99.107% \n",
      "[epoch:88, iter:8360] Loss: 0.020 | Acc: 99.219% \n",
      "[epoch:88, iter:8361] Loss: 0.018 | Acc: 99.306% \n",
      "[epoch:88, iter:8362] Loss: 0.020 | Acc: 99.375% \n",
      "[epoch:88, iter:8363] Loss: 0.018 | Acc: 99.432% \n",
      "[epoch:88, iter:8364] Loss: 0.016 | Acc: 99.479% \n",
      "[epoch:88, iter:8365] Loss: 0.015 | Acc: 99.519% \n",
      "[epoch:88, iter:8366] Loss: 0.015 | Acc: 99.554% \n",
      "[epoch:88, iter:8367] Loss: 0.016 | Acc: 99.583% \n",
      "[epoch:88, iter:8368] Loss: 0.015 | Acc: 99.609% \n",
      "[epoch:88, iter:8369] Loss: 0.014 | Acc: 99.632% \n",
      "[epoch:88, iter:8370] Loss: 0.013 | Acc: 99.653% \n",
      "[epoch:88, iter:8371] Loss: 0.013 | Acc: 99.671% \n",
      "[epoch:88, iter:8372] Loss: 0.012 | Acc: 99.688% \n",
      "[epoch:88, iter:8373] Loss: 0.012 | Acc: 99.702% \n",
      "[epoch:88, iter:8374] Loss: 0.011 | Acc: 99.716% \n",
      "[epoch:88, iter:8375] Loss: 0.014 | Acc: 99.457% \n",
      "[epoch:88, iter:8376] Loss: 0.016 | Acc: 99.479% \n",
      "[epoch:88, iter:8377] Loss: 0.015 | Acc: 99.500% \n",
      "[epoch:88, iter:8378] Loss: 0.014 | Acc: 99.519% \n",
      "[epoch:88, iter:8379] Loss: 0.014 | Acc: 99.537% \n",
      "[epoch:88, iter:8380] Loss: 0.015 | Acc: 99.554% \n",
      "[epoch:88, iter:8381] Loss: 0.014 | Acc: 99.569% \n",
      "[epoch:88, iter:8382] Loss: 0.014 | Acc: 99.583% \n",
      "[epoch:88, iter:8383] Loss: 0.013 | Acc: 99.597% \n",
      "[epoch:88, iter:8384] Loss: 0.015 | Acc: 99.414% \n",
      "[epoch:88, iter:8385] Loss: 0.015 | Acc: 99.432% \n",
      "[epoch:88, iter:8386] Loss: 0.016 | Acc: 99.449% \n",
      "[epoch:88, iter:8387] Loss: 0.015 | Acc: 99.464% \n",
      "[epoch:88, iter:8388] Loss: 0.015 | Acc: 99.479% \n",
      "[epoch:88, iter:8389] Loss: 0.015 | Acc: 99.493% \n",
      "[epoch:88, iter:8390] Loss: 0.015 | Acc: 99.507% \n",
      "[epoch:88, iter:8391] Loss: 0.015 | Acc: 99.519% \n",
      "[epoch:88, iter:8392] Loss: 0.014 | Acc: 99.531% \n",
      "[epoch:88, iter:8393] Loss: 0.014 | Acc: 99.543% \n",
      "[epoch:88, iter:8394] Loss: 0.015 | Acc: 99.554% \n",
      "[epoch:88, iter:8395] Loss: 0.016 | Acc: 99.419% \n",
      "[epoch:88, iter:8396] Loss: 0.016 | Acc: 99.432% \n",
      "[epoch:88, iter:8397] Loss: 0.016 | Acc: 99.444% \n",
      "[epoch:88, iter:8398] Loss: 0.015 | Acc: 99.457% \n",
      "[epoch:88, iter:8399] Loss: 0.016 | Acc: 99.468% \n",
      "[epoch:88, iter:8400] Loss: 0.015 | Acc: 99.479% \n",
      "[epoch:88, iter:8401] Loss: 0.015 | Acc: 99.490% \n",
      "[epoch:88, iter:8402] Loss: 0.015 | Acc: 99.500% \n",
      "[epoch:88, iter:8403] Loss: 0.015 | Acc: 99.510% \n",
      "[epoch:88, iter:8404] Loss: 0.014 | Acc: 99.519% \n",
      "[epoch:88, iter:8405] Loss: 0.014 | Acc: 99.528% \n",
      "[epoch:88, iter:8406] Loss: 0.014 | Acc: 99.537% \n",
      "[epoch:88, iter:8407] Loss: 0.014 | Acc: 99.432% \n",
      "[epoch:88, iter:8408] Loss: 0.014 | Acc: 99.442% \n",
      "[epoch:88, iter:8409] Loss: 0.016 | Acc: 99.232% \n",
      "[epoch:88, iter:8410] Loss: 0.015 | Acc: 99.246% \n",
      "[epoch:88, iter:8411] Loss: 0.015 | Acc: 99.258% \n",
      "[epoch:88, iter:8412] Loss: 0.015 | Acc: 99.271% \n",
      "[epoch:88, iter:8413] Loss: 0.015 | Acc: 99.283% \n",
      "[epoch:88, iter:8414] Loss: 0.015 | Acc: 99.294% \n",
      "[epoch:88, iter:8415] Loss: 0.014 | Acc: 99.306% \n",
      "[epoch:88, iter:8416] Loss: 0.014 | Acc: 99.316% \n",
      "[epoch:88, iter:8417] Loss: 0.015 | Acc: 99.327% \n",
      "[epoch:88, iter:8418] Loss: 0.014 | Acc: 99.337% \n",
      "[epoch:88, iter:8419] Loss: 0.014 | Acc: 99.347% \n",
      "[epoch:88, iter:8420] Loss: 0.014 | Acc: 99.357% \n",
      "[epoch:88, iter:8421] Loss: 0.014 | Acc: 99.366% \n",
      "[epoch:88, iter:8422] Loss: 0.014 | Acc: 99.375% \n",
      "[epoch:88, iter:8423] Loss: 0.014 | Acc: 99.296% \n",
      "[epoch:88, iter:8424] Loss: 0.014 | Acc: 99.306% \n",
      "[epoch:88, iter:8425] Loss: 0.015 | Acc: 99.229% \n",
      "[epoch:88, iter:8426] Loss: 0.015 | Acc: 99.240% \n",
      "[epoch:88, iter:8427] Loss: 0.015 | Acc: 99.167% \n",
      "[epoch:88, iter:8428] Loss: 0.016 | Acc: 99.095% \n",
      "[epoch:88, iter:8429] Loss: 0.016 | Acc: 99.107% \n",
      "[epoch:88, iter:8430] Loss: 0.016 | Acc: 99.119% \n",
      "[epoch:88, iter:8431] Loss: 0.017 | Acc: 99.051% \n",
      "[epoch:88, iter:8432] Loss: 0.017 | Acc: 99.062% \n",
      "[epoch:88, iter:8433] Loss: 0.017 | Acc: 99.074% \n",
      "[epoch:88, iter:8434] Loss: 0.017 | Acc: 99.085% \n",
      "[epoch:88, iter:8435] Loss: 0.016 | Acc: 99.096% \n",
      "[epoch:88, iter:8436] Loss: 0.016 | Acc: 99.107% \n",
      "[epoch:88, iter:8437] Loss: 0.017 | Acc: 99.044% \n",
      "[epoch:88, iter:8438] Loss: 0.016 | Acc: 99.055% \n",
      "[epoch:88, iter:8439] Loss: 0.016 | Acc: 99.066% \n",
      "[epoch:88, iter:8440] Loss: 0.017 | Acc: 99.006% \n",
      "[epoch:88, iter:8441] Loss: 0.016 | Acc: 99.017% \n",
      "[epoch:88, iter:8442] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:88, iter:8443] Loss: 0.017 | Acc: 98.970% \n",
      "[epoch:88, iter:8444] Loss: 0.017 | Acc: 98.981% \n",
      "[epoch:88, iter:8445] Loss: 0.017 | Acc: 98.992% \n",
      "[epoch:88, iter:8446] Loss: 0.017 | Acc: 98.936% \n",
      "[epoch:88, iter:8447] Loss: 0.018 | Acc: 98.882% \n",
      "[epoch:88, iter:8448] Loss: 0.018 | Acc: 98.888% \n",
      "Waiting Test!\n",
      "测试分类准确率为：84.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 89\n",
      "[epoch:89, iter:8449] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:89, iter:8450] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:89, iter:8451] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:89, iter:8452] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:89, iter:8453] Loss: 0.006 | Acc: 100.000% \n",
      "[epoch:89, iter:8454] Loss: 0.005 | Acc: 100.000% \n",
      "[epoch:89, iter:8455] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:89, iter:8456] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:89, iter:8457] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:89, iter:8458] Loss: 0.008 | Acc: 99.375% \n",
      "[epoch:89, iter:8459] Loss: 0.008 | Acc: 99.432% \n",
      "[epoch:89, iter:8460] Loss: 0.007 | Acc: 99.479% \n",
      "[epoch:89, iter:8461] Loss: 0.007 | Acc: 99.519% \n",
      "[epoch:89, iter:8462] Loss: 0.006 | Acc: 99.554% \n",
      "[epoch:89, iter:8463] Loss: 0.006 | Acc: 99.583% \n",
      "[epoch:89, iter:8464] Loss: 0.008 | Acc: 99.609% \n",
      "[epoch:89, iter:8465] Loss: 0.008 | Acc: 99.632% \n",
      "[epoch:89, iter:8466] Loss: 0.007 | Acc: 99.653% \n",
      "[epoch:89, iter:8467] Loss: 0.013 | Acc: 99.342% \n",
      "[epoch:89, iter:8468] Loss: 0.012 | Acc: 99.375% \n",
      "[epoch:89, iter:8469] Loss: 0.014 | Acc: 99.107% \n",
      "[epoch:89, iter:8470] Loss: 0.014 | Acc: 99.148% \n",
      "[epoch:89, iter:8471] Loss: 0.016 | Acc: 99.185% \n",
      "[epoch:89, iter:8472] Loss: 0.016 | Acc: 99.219% \n",
      "[epoch:89, iter:8473] Loss: 0.015 | Acc: 99.250% \n",
      "[epoch:89, iter:8474] Loss: 0.015 | Acc: 99.279% \n",
      "[epoch:89, iter:8475] Loss: 0.015 | Acc: 99.306% \n",
      "[epoch:89, iter:8476] Loss: 0.018 | Acc: 98.884% \n",
      "[epoch:89, iter:8477] Loss: 0.019 | Acc: 98.707% \n",
      "[epoch:89, iter:8478] Loss: 0.019 | Acc: 98.750% \n",
      "[epoch:89, iter:8479] Loss: 0.018 | Acc: 98.790% \n",
      "[epoch:89, iter:8480] Loss: 0.018 | Acc: 98.828% \n",
      "[epoch:89, iter:8481] Loss: 0.017 | Acc: 98.864% \n",
      "[epoch:89, iter:8482] Loss: 0.018 | Acc: 98.897% \n",
      "[epoch:89, iter:8483] Loss: 0.017 | Acc: 98.929% \n",
      "[epoch:89, iter:8484] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:89, iter:8485] Loss: 0.016 | Acc: 98.986% \n",
      "[epoch:89, iter:8486] Loss: 0.017 | Acc: 98.849% \n",
      "[epoch:89, iter:8487] Loss: 0.017 | Acc: 98.878% \n",
      "[epoch:89, iter:8488] Loss: 0.017 | Acc: 98.906% \n",
      "[epoch:89, iter:8489] Loss: 0.019 | Acc: 98.780% \n",
      "[epoch:89, iter:8490] Loss: 0.019 | Acc: 98.810% \n",
      "[epoch:89, iter:8491] Loss: 0.018 | Acc: 98.837% \n",
      "[epoch:89, iter:8492] Loss: 0.018 | Acc: 98.864% \n",
      "[epoch:89, iter:8493] Loss: 0.017 | Acc: 98.889% \n",
      "[epoch:89, iter:8494] Loss: 0.017 | Acc: 98.913% \n",
      "[epoch:89, iter:8495] Loss: 0.017 | Acc: 98.936% \n",
      "[epoch:89, iter:8496] Loss: 0.016 | Acc: 98.958% \n",
      "[epoch:89, iter:8497] Loss: 0.016 | Acc: 98.980% \n",
      "[epoch:89, iter:8498] Loss: 0.016 | Acc: 99.000% \n",
      "[epoch:89, iter:8499] Loss: 0.017 | Acc: 98.775% \n",
      "[epoch:89, iter:8500] Loss: 0.017 | Acc: 98.798% \n",
      "[epoch:89, iter:8501] Loss: 0.018 | Acc: 98.703% \n",
      "[epoch:89, iter:8502] Loss: 0.017 | Acc: 98.727% \n",
      "[epoch:89, iter:8503] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:89, iter:8504] Loss: 0.017 | Acc: 98.772% \n",
      "[epoch:89, iter:8505] Loss: 0.017 | Acc: 98.794% \n",
      "[epoch:89, iter:8506] Loss: 0.017 | Acc: 98.815% \n",
      "[epoch:89, iter:8507] Loss: 0.017 | Acc: 98.835% \n",
      "[epoch:89, iter:8508] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:89, iter:8509] Loss: 0.018 | Acc: 98.668% \n",
      "[epoch:89, iter:8510] Loss: 0.018 | Acc: 98.690% \n",
      "[epoch:89, iter:8511] Loss: 0.018 | Acc: 98.710% \n",
      "[epoch:89, iter:8512] Loss: 0.018 | Acc: 98.730% \n",
      "[epoch:89, iter:8513] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:89, iter:8514] Loss: 0.018 | Acc: 98.769% \n",
      "[epoch:89, iter:8515] Loss: 0.018 | Acc: 98.787% \n",
      "[epoch:89, iter:8516] Loss: 0.018 | Acc: 98.713% \n",
      "[epoch:89, iter:8517] Loss: 0.018 | Acc: 98.732% \n",
      "[epoch:89, iter:8518] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:89, iter:8519] Loss: 0.018 | Acc: 98.768% \n",
      "[epoch:89, iter:8520] Loss: 0.018 | Acc: 98.785% \n",
      "[epoch:89, iter:8521] Loss: 0.018 | Acc: 98.801% \n",
      "[epoch:89, iter:8522] Loss: 0.018 | Acc: 98.818% \n",
      "[epoch:89, iter:8523] Loss: 0.018 | Acc: 98.833% \n",
      "[epoch:89, iter:8524] Loss: 0.018 | Acc: 98.849% \n",
      "[epoch:89, iter:8525] Loss: 0.018 | Acc: 98.864% \n",
      "[epoch:89, iter:8526] Loss: 0.018 | Acc: 98.878% \n",
      "[epoch:89, iter:8527] Loss: 0.017 | Acc: 98.892% \n",
      "[epoch:89, iter:8528] Loss: 0.018 | Acc: 98.906% \n",
      "[epoch:89, iter:8529] Loss: 0.018 | Acc: 98.843% \n",
      "[epoch:89, iter:8530] Loss: 0.018 | Acc: 98.857% \n",
      "[epoch:89, iter:8531] Loss: 0.018 | Acc: 98.870% \n",
      "[epoch:89, iter:8532] Loss: 0.017 | Acc: 98.884% \n",
      "[epoch:89, iter:8533] Loss: 0.017 | Acc: 98.897% \n",
      "[epoch:89, iter:8534] Loss: 0.017 | Acc: 98.910% \n",
      "[epoch:89, iter:8535] Loss: 0.017 | Acc: 98.922% \n",
      "[epoch:89, iter:8536] Loss: 0.017 | Acc: 98.935% \n",
      "[epoch:89, iter:8537] Loss: 0.017 | Acc: 98.947% \n",
      "[epoch:89, iter:8538] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:89, iter:8539] Loss: 0.017 | Acc: 98.970% \n",
      "[epoch:89, iter:8540] Loss: 0.017 | Acc: 98.845% \n",
      "[epoch:89, iter:8541] Loss: 0.017 | Acc: 98.858% \n",
      "[epoch:89, iter:8542] Loss: 0.017 | Acc: 98.870% \n",
      "[epoch:89, iter:8543] Loss: 0.017 | Acc: 98.816% \n",
      "[epoch:89, iter:8544] Loss: 0.017 | Acc: 98.823% \n",
      "Waiting Test!\n",
      "测试分类准确率为：85.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 90\n",
      "[epoch:90, iter:8545] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:90, iter:8546] Loss: 0.042 | Acc: 96.875% \n",
      "[epoch:90, iter:8547] Loss: 0.028 | Acc: 97.917% \n",
      "[epoch:90, iter:8548] Loss: 0.021 | Acc: 98.438% \n",
      "[epoch:90, iter:8549] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:90, iter:8550] Loss: 0.021 | Acc: 98.958% \n",
      "[epoch:90, iter:8551] Loss: 0.018 | Acc: 99.107% \n",
      "[epoch:90, iter:8552] Loss: 0.023 | Acc: 99.219% \n",
      "[epoch:90, iter:8553] Loss: 0.021 | Acc: 99.306% \n",
      "[epoch:90, iter:8554] Loss: 0.018 | Acc: 99.375% \n",
      "[epoch:90, iter:8555] Loss: 0.017 | Acc: 99.432% \n",
      "[epoch:90, iter:8556] Loss: 0.015 | Acc: 99.479% \n",
      "[epoch:90, iter:8557] Loss: 0.023 | Acc: 98.558% \n",
      "[epoch:90, iter:8558] Loss: 0.023 | Acc: 98.661% \n",
      "[epoch:90, iter:8559] Loss: 0.022 | Acc: 98.750% \n",
      "[epoch:90, iter:8560] Loss: 0.021 | Acc: 98.828% \n",
      "[epoch:90, iter:8561] Loss: 0.023 | Acc: 98.529% \n",
      "[epoch:90, iter:8562] Loss: 0.022 | Acc: 98.611% \n",
      "[epoch:90, iter:8563] Loss: 0.021 | Acc: 98.684% \n",
      "[epoch:90, iter:8564] Loss: 0.022 | Acc: 98.438% \n",
      "[epoch:90, iter:8565] Loss: 0.023 | Acc: 98.512% \n",
      "[epoch:90, iter:8566] Loss: 0.022 | Acc: 98.580% \n",
      "[epoch:90, iter:8567] Loss: 0.021 | Acc: 98.641% \n",
      "[epoch:90, iter:8568] Loss: 0.022 | Acc: 98.698% \n",
      "[epoch:90, iter:8569] Loss: 0.021 | Acc: 98.750% \n",
      "[epoch:90, iter:8570] Loss: 0.020 | Acc: 98.798% \n",
      "[epoch:90, iter:8571] Loss: 0.019 | Acc: 98.843% \n",
      "[epoch:90, iter:8572] Loss: 0.019 | Acc: 98.884% \n",
      "[epoch:90, iter:8573] Loss: 0.019 | Acc: 98.922% \n",
      "[epoch:90, iter:8574] Loss: 0.019 | Acc: 98.958% \n",
      "[epoch:90, iter:8575] Loss: 0.021 | Acc: 98.790% \n",
      "[epoch:90, iter:8576] Loss: 0.022 | Acc: 98.633% \n",
      "[epoch:90, iter:8577] Loss: 0.021 | Acc: 98.674% \n",
      "[epoch:90, iter:8578] Loss: 0.021 | Acc: 98.713% \n",
      "[epoch:90, iter:8579] Loss: 0.020 | Acc: 98.750% \n",
      "[epoch:90, iter:8580] Loss: 0.020 | Acc: 98.785% \n",
      "[epoch:90, iter:8581] Loss: 0.022 | Acc: 98.480% \n",
      "[epoch:90, iter:8582] Loss: 0.022 | Acc: 98.520% \n",
      "[epoch:90, iter:8583] Loss: 0.022 | Acc: 98.558% \n",
      "[epoch:90, iter:8584] Loss: 0.021 | Acc: 98.594% \n",
      "[epoch:90, iter:8585] Loss: 0.021 | Acc: 98.628% \n",
      "[epoch:90, iter:8586] Loss: 0.021 | Acc: 98.661% \n",
      "[epoch:90, iter:8587] Loss: 0.020 | Acc: 98.692% \n",
      "[epoch:90, iter:8588] Loss: 0.020 | Acc: 98.722% \n",
      "[epoch:90, iter:8589] Loss: 0.021 | Acc: 98.611% \n",
      "[epoch:90, iter:8590] Loss: 0.021 | Acc: 98.641% \n",
      "[epoch:90, iter:8591] Loss: 0.020 | Acc: 98.670% \n",
      "[epoch:90, iter:8592] Loss: 0.020 | Acc: 98.698% \n",
      "[epoch:90, iter:8593] Loss: 0.020 | Acc: 98.724% \n",
      "[epoch:90, iter:8594] Loss: 0.020 | Acc: 98.750% \n",
      "[epoch:90, iter:8595] Loss: 0.020 | Acc: 98.775% \n",
      "[epoch:90, iter:8596] Loss: 0.020 | Acc: 98.678% \n",
      "[epoch:90, iter:8597] Loss: 0.020 | Acc: 98.703% \n",
      "[epoch:90, iter:8598] Loss: 0.019 | Acc: 98.727% \n",
      "[epoch:90, iter:8599] Loss: 0.019 | Acc: 98.750% \n",
      "[epoch:90, iter:8600] Loss: 0.019 | Acc: 98.772% \n",
      "[epoch:90, iter:8601] Loss: 0.018 | Acc: 98.794% \n",
      "[epoch:90, iter:8602] Loss: 0.018 | Acc: 98.815% \n",
      "[epoch:90, iter:8603] Loss: 0.018 | Acc: 98.835% \n",
      "[epoch:90, iter:8604] Loss: 0.019 | Acc: 98.750% \n",
      "[epoch:90, iter:8605] Loss: 0.018 | Acc: 98.770% \n",
      "[epoch:90, iter:8606] Loss: 0.018 | Acc: 98.790% \n",
      "[epoch:90, iter:8607] Loss: 0.018 | Acc: 98.810% \n",
      "[epoch:90, iter:8608] Loss: 0.019 | Acc: 98.633% \n",
      "[epoch:90, iter:8609] Loss: 0.019 | Acc: 98.654% \n",
      "[epoch:90, iter:8610] Loss: 0.021 | Acc: 98.485% \n",
      "[epoch:90, iter:8611] Loss: 0.021 | Acc: 98.414% \n",
      "[epoch:90, iter:8612] Loss: 0.022 | Acc: 98.438% \n",
      "[epoch:90, iter:8613] Loss: 0.022 | Acc: 98.370% \n",
      "[epoch:90, iter:8614] Loss: 0.022 | Acc: 98.393% \n",
      "[epoch:90, iter:8615] Loss: 0.021 | Acc: 98.415% \n",
      "[epoch:90, iter:8616] Loss: 0.021 | Acc: 98.438% \n",
      "[epoch:90, iter:8617] Loss: 0.021 | Acc: 98.459% \n",
      "[epoch:90, iter:8618] Loss: 0.021 | Acc: 98.480% \n",
      "[epoch:90, iter:8619] Loss: 0.021 | Acc: 98.500% \n",
      "[epoch:90, iter:8620] Loss: 0.021 | Acc: 98.520% \n",
      "[epoch:90, iter:8621] Loss: 0.020 | Acc: 98.539% \n",
      "[epoch:90, iter:8622] Loss: 0.020 | Acc: 98.558% \n",
      "[epoch:90, iter:8623] Loss: 0.020 | Acc: 98.576% \n",
      "[epoch:90, iter:8624] Loss: 0.020 | Acc: 98.594% \n",
      "[epoch:90, iter:8625] Loss: 0.020 | Acc: 98.611% \n",
      "[epoch:90, iter:8626] Loss: 0.020 | Acc: 98.628% \n",
      "[epoch:90, iter:8627] Loss: 0.021 | Acc: 98.494% \n",
      "[epoch:90, iter:8628] Loss: 0.021 | Acc: 98.512% \n",
      "[epoch:90, iter:8629] Loss: 0.021 | Acc: 98.529% \n",
      "[epoch:90, iter:8630] Loss: 0.020 | Acc: 98.547% \n",
      "[epoch:90, iter:8631] Loss: 0.020 | Acc: 98.563% \n",
      "[epoch:90, iter:8632] Loss: 0.020 | Acc: 98.580% \n",
      "[epoch:90, iter:8633] Loss: 0.020 | Acc: 98.596% \n",
      "[epoch:90, iter:8634] Loss: 0.021 | Acc: 98.542% \n",
      "[epoch:90, iter:8635] Loss: 0.021 | Acc: 98.558% \n",
      "[epoch:90, iter:8636] Loss: 0.020 | Acc: 98.573% \n",
      "[epoch:90, iter:8637] Loss: 0.020 | Acc: 98.589% \n",
      "[epoch:90, iter:8638] Loss: 0.020 | Acc: 98.604% \n",
      "[epoch:90, iter:8639] Loss: 0.020 | Acc: 98.618% \n",
      "[epoch:90, iter:8640] Loss: 0.021 | Acc: 98.561% \n",
      "Waiting Test!\n",
      "测试分类准确率为：87.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 91\n",
      "[epoch:91, iter:8641] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:91, iter:8642] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:91, iter:8643] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:91, iter:8644] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:91, iter:8645] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:91, iter:8646] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:91, iter:8647] Loss: 0.011 | Acc: 99.107% \n",
      "[epoch:91, iter:8648] Loss: 0.010 | Acc: 99.219% \n",
      "[epoch:91, iter:8649] Loss: 0.009 | Acc: 99.306% \n",
      "[epoch:91, iter:8650] Loss: 0.013 | Acc: 98.750% \n",
      "[epoch:91, iter:8651] Loss: 0.016 | Acc: 98.295% \n",
      "[epoch:91, iter:8652] Loss: 0.018 | Acc: 98.438% \n",
      "[epoch:91, iter:8653] Loss: 0.016 | Acc: 98.558% \n",
      "[epoch:91, iter:8654] Loss: 0.015 | Acc: 98.661% \n",
      "[epoch:91, iter:8655] Loss: 0.018 | Acc: 98.333% \n",
      "[epoch:91, iter:8656] Loss: 0.017 | Acc: 98.438% \n",
      "[epoch:91, iter:8657] Loss: 0.016 | Acc: 98.529% \n",
      "[epoch:91, iter:8658] Loss: 0.017 | Acc: 98.611% \n",
      "[epoch:91, iter:8659] Loss: 0.016 | Acc: 98.684% \n",
      "[epoch:91, iter:8660] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:91, iter:8661] Loss: 0.017 | Acc: 98.810% \n",
      "[epoch:91, iter:8662] Loss: 0.018 | Acc: 98.864% \n",
      "[epoch:91, iter:8663] Loss: 0.017 | Acc: 98.913% \n",
      "[epoch:91, iter:8664] Loss: 0.021 | Acc: 98.438% \n",
      "[epoch:91, iter:8665] Loss: 0.020 | Acc: 98.500% \n",
      "[epoch:91, iter:8666] Loss: 0.019 | Acc: 98.558% \n",
      "[epoch:91, iter:8667] Loss: 0.020 | Acc: 98.611% \n",
      "[epoch:91, iter:8668] Loss: 0.019 | Acc: 98.661% \n",
      "[epoch:91, iter:8669] Loss: 0.018 | Acc: 98.707% \n",
      "[epoch:91, iter:8670] Loss: 0.022 | Acc: 98.333% \n",
      "[epoch:91, iter:8671] Loss: 0.021 | Acc: 98.387% \n",
      "[epoch:91, iter:8672] Loss: 0.020 | Acc: 98.438% \n",
      "[epoch:91, iter:8673] Loss: 0.020 | Acc: 98.485% \n",
      "[epoch:91, iter:8674] Loss: 0.020 | Acc: 98.529% \n",
      "[epoch:91, iter:8675] Loss: 0.019 | Acc: 98.571% \n",
      "[epoch:91, iter:8676] Loss: 0.022 | Acc: 98.264% \n",
      "[epoch:91, iter:8677] Loss: 0.021 | Acc: 98.311% \n",
      "[epoch:91, iter:8678] Loss: 0.021 | Acc: 98.355% \n",
      "[epoch:91, iter:8679] Loss: 0.020 | Acc: 98.397% \n",
      "[epoch:91, iter:8680] Loss: 0.020 | Acc: 98.438% \n",
      "[epoch:91, iter:8681] Loss: 0.020 | Acc: 98.323% \n",
      "[epoch:91, iter:8682] Loss: 0.020 | Acc: 98.363% \n",
      "[epoch:91, iter:8683] Loss: 0.021 | Acc: 98.256% \n",
      "[epoch:91, iter:8684] Loss: 0.021 | Acc: 98.295% \n",
      "[epoch:91, iter:8685] Loss: 0.020 | Acc: 98.333% \n",
      "[epoch:91, iter:8686] Loss: 0.020 | Acc: 98.370% \n",
      "[epoch:91, iter:8687] Loss: 0.019 | Acc: 98.404% \n",
      "[epoch:91, iter:8688] Loss: 0.019 | Acc: 98.438% \n",
      "[epoch:91, iter:8689] Loss: 0.019 | Acc: 98.469% \n",
      "[epoch:91, iter:8690] Loss: 0.018 | Acc: 98.500% \n",
      "[epoch:91, iter:8691] Loss: 0.018 | Acc: 98.529% \n",
      "[epoch:91, iter:8692] Loss: 0.019 | Acc: 98.438% \n",
      "[epoch:91, iter:8693] Loss: 0.018 | Acc: 98.467% \n",
      "[epoch:91, iter:8694] Loss: 0.018 | Acc: 98.495% \n",
      "[epoch:91, iter:8695] Loss: 0.018 | Acc: 98.523% \n",
      "[epoch:91, iter:8696] Loss: 0.017 | Acc: 98.549% \n",
      "[epoch:91, iter:8697] Loss: 0.018 | Acc: 98.575% \n",
      "[epoch:91, iter:8698] Loss: 0.018 | Acc: 98.599% \n",
      "[epoch:91, iter:8699] Loss: 0.017 | Acc: 98.623% \n",
      "[epoch:91, iter:8700] Loss: 0.018 | Acc: 98.646% \n",
      "[epoch:91, iter:8701] Loss: 0.018 | Acc: 98.566% \n",
      "[epoch:91, iter:8702] Loss: 0.018 | Acc: 98.589% \n",
      "[epoch:91, iter:8703] Loss: 0.018 | Acc: 98.611% \n",
      "[epoch:91, iter:8704] Loss: 0.017 | Acc: 98.633% \n",
      "[epoch:91, iter:8705] Loss: 0.026 | Acc: 98.462% \n",
      "[epoch:91, iter:8706] Loss: 0.026 | Acc: 98.485% \n",
      "[epoch:91, iter:8707] Loss: 0.026 | Acc: 98.507% \n",
      "[epoch:91, iter:8708] Loss: 0.025 | Acc: 98.529% \n",
      "[epoch:91, iter:8709] Loss: 0.027 | Acc: 98.460% \n",
      "[epoch:91, iter:8710] Loss: 0.026 | Acc: 98.482% \n",
      "[epoch:91, iter:8711] Loss: 0.026 | Acc: 98.504% \n",
      "[epoch:91, iter:8712] Loss: 0.026 | Acc: 98.524% \n",
      "[epoch:91, iter:8713] Loss: 0.026 | Acc: 98.545% \n",
      "[epoch:91, iter:8714] Loss: 0.026 | Acc: 98.564% \n",
      "[epoch:91, iter:8715] Loss: 0.027 | Acc: 98.500% \n",
      "[epoch:91, iter:8716] Loss: 0.026 | Acc: 98.520% \n",
      "[epoch:91, iter:8717] Loss: 0.026 | Acc: 98.539% \n",
      "[epoch:91, iter:8718] Loss: 0.026 | Acc: 98.558% \n",
      "[epoch:91, iter:8719] Loss: 0.025 | Acc: 98.576% \n",
      "[epoch:91, iter:8720] Loss: 0.026 | Acc: 98.516% \n",
      "[epoch:91, iter:8721] Loss: 0.026 | Acc: 98.534% \n",
      "[epoch:91, iter:8722] Loss: 0.026 | Acc: 98.476% \n",
      "[epoch:91, iter:8723] Loss: 0.026 | Acc: 98.494% \n",
      "[epoch:91, iter:8724] Loss: 0.026 | Acc: 98.512% \n",
      "[epoch:91, iter:8725] Loss: 0.026 | Acc: 98.529% \n",
      "[epoch:91, iter:8726] Loss: 0.026 | Acc: 98.547% \n",
      "[epoch:91, iter:8727] Loss: 0.025 | Acc: 98.563% \n",
      "[epoch:91, iter:8728] Loss: 0.026 | Acc: 98.509% \n",
      "[epoch:91, iter:8729] Loss: 0.025 | Acc: 98.525% \n",
      "[epoch:91, iter:8730] Loss: 0.025 | Acc: 98.542% \n",
      "[epoch:91, iter:8731] Loss: 0.025 | Acc: 98.558% \n",
      "[epoch:91, iter:8732] Loss: 0.025 | Acc: 98.573% \n",
      "[epoch:91, iter:8733] Loss: 0.025 | Acc: 98.589% \n",
      "[epoch:91, iter:8734] Loss: 0.025 | Acc: 98.604% \n",
      "[epoch:91, iter:8735] Loss: 0.025 | Acc: 98.618% \n",
      "[epoch:91, iter:8736] Loss: 0.025 | Acc: 98.627% \n",
      "Waiting Test!\n",
      "测试分类准确率为：76.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 92\n",
      "[epoch:92, iter:8737] Loss: 0.077 | Acc: 93.750% \n",
      "[epoch:92, iter:8738] Loss: 0.039 | Acc: 96.875% \n",
      "[epoch:92, iter:8739] Loss: 0.026 | Acc: 97.917% \n",
      "[epoch:92, iter:8740] Loss: 0.019 | Acc: 98.438% \n",
      "[epoch:92, iter:8741] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:92, iter:8742] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:92, iter:8743] Loss: 0.017 | Acc: 99.107% \n",
      "[epoch:92, iter:8744] Loss: 0.022 | Acc: 99.219% \n",
      "[epoch:92, iter:8745] Loss: 0.025 | Acc: 99.306% \n",
      "[epoch:92, iter:8746] Loss: 0.023 | Acc: 99.375% \n",
      "[epoch:92, iter:8747] Loss: 0.025 | Acc: 99.432% \n",
      "[epoch:92, iter:8748] Loss: 0.023 | Acc: 99.479% \n",
      "[epoch:92, iter:8749] Loss: 0.025 | Acc: 99.038% \n",
      "[epoch:92, iter:8750] Loss: 0.023 | Acc: 99.107% \n",
      "[epoch:92, iter:8751] Loss: 0.023 | Acc: 99.167% \n",
      "[epoch:92, iter:8752] Loss: 0.022 | Acc: 99.219% \n",
      "[epoch:92, iter:8753] Loss: 0.028 | Acc: 98.529% \n",
      "[epoch:92, iter:8754] Loss: 0.027 | Acc: 98.611% \n",
      "[epoch:92, iter:8755] Loss: 0.026 | Acc: 98.684% \n",
      "[epoch:92, iter:8756] Loss: 0.024 | Acc: 98.750% \n",
      "[epoch:92, iter:8757] Loss: 0.023 | Acc: 98.810% \n",
      "[epoch:92, iter:8758] Loss: 0.024 | Acc: 98.864% \n",
      "[epoch:92, iter:8759] Loss: 0.023 | Acc: 98.913% \n",
      "[epoch:92, iter:8760] Loss: 0.023 | Acc: 98.958% \n",
      "[epoch:92, iter:8761] Loss: 0.022 | Acc: 99.000% \n",
      "[epoch:92, iter:8762] Loss: 0.022 | Acc: 99.038% \n",
      "[epoch:92, iter:8763] Loss: 0.023 | Acc: 99.074% \n",
      "[epoch:92, iter:8764] Loss: 0.025 | Acc: 98.884% \n",
      "[epoch:92, iter:8765] Loss: 0.024 | Acc: 98.922% \n",
      "[epoch:92, iter:8766] Loss: 0.024 | Acc: 98.958% \n",
      "[epoch:92, iter:8767] Loss: 0.023 | Acc: 98.992% \n",
      "[epoch:92, iter:8768] Loss: 0.022 | Acc: 99.023% \n",
      "[epoch:92, iter:8769] Loss: 0.022 | Acc: 99.053% \n",
      "[epoch:92, iter:8770] Loss: 0.021 | Acc: 99.081% \n",
      "[epoch:92, iter:8771] Loss: 0.022 | Acc: 98.929% \n",
      "[epoch:92, iter:8772] Loss: 0.022 | Acc: 98.958% \n",
      "[epoch:92, iter:8773] Loss: 0.023 | Acc: 98.818% \n",
      "[epoch:92, iter:8774] Loss: 0.023 | Acc: 98.849% \n",
      "[epoch:92, iter:8775] Loss: 0.022 | Acc: 98.878% \n",
      "[epoch:92, iter:8776] Loss: 0.022 | Acc: 98.906% \n",
      "[epoch:92, iter:8777] Loss: 0.021 | Acc: 98.933% \n",
      "[epoch:92, iter:8778] Loss: 0.021 | Acc: 98.958% \n",
      "[epoch:92, iter:8779] Loss: 0.020 | Acc: 98.983% \n",
      "[epoch:92, iter:8780] Loss: 0.022 | Acc: 98.864% \n",
      "[epoch:92, iter:8781] Loss: 0.021 | Acc: 98.889% \n",
      "[epoch:92, iter:8782] Loss: 0.021 | Acc: 98.913% \n",
      "[epoch:92, iter:8783] Loss: 0.020 | Acc: 98.936% \n",
      "[epoch:92, iter:8784] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:92, iter:8785] Loss: 0.020 | Acc: 98.980% \n",
      "[epoch:92, iter:8786] Loss: 0.019 | Acc: 99.000% \n",
      "[epoch:92, iter:8787] Loss: 0.019 | Acc: 99.020% \n",
      "[epoch:92, iter:8788] Loss: 0.019 | Acc: 99.038% \n",
      "[epoch:92, iter:8789] Loss: 0.019 | Acc: 99.057% \n",
      "[epoch:92, iter:8790] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:92, iter:8791] Loss: 0.020 | Acc: 98.977% \n",
      "[epoch:92, iter:8792] Loss: 0.020 | Acc: 98.996% \n",
      "[epoch:92, iter:8793] Loss: 0.020 | Acc: 99.013% \n",
      "[epoch:92, iter:8794] Loss: 0.019 | Acc: 99.030% \n",
      "[epoch:92, iter:8795] Loss: 0.019 | Acc: 99.047% \n",
      "[epoch:92, iter:8796] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:92, iter:8797] Loss: 0.020 | Acc: 98.975% \n",
      "[epoch:92, iter:8798] Loss: 0.019 | Acc: 98.992% \n",
      "[epoch:92, iter:8799] Loss: 0.021 | Acc: 98.909% \n",
      "[epoch:92, iter:8800] Loss: 0.020 | Acc: 98.926% \n",
      "[epoch:92, iter:8801] Loss: 0.020 | Acc: 98.942% \n",
      "[epoch:92, iter:8802] Loss: 0.021 | Acc: 98.864% \n",
      "[epoch:92, iter:8803] Loss: 0.021 | Acc: 98.881% \n",
      "[epoch:92, iter:8804] Loss: 0.020 | Acc: 98.897% \n",
      "[epoch:92, iter:8805] Loss: 0.020 | Acc: 98.913% \n",
      "[epoch:92, iter:8806] Loss: 0.020 | Acc: 98.929% \n",
      "[epoch:92, iter:8807] Loss: 0.020 | Acc: 98.944% \n",
      "[epoch:92, iter:8808] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:92, iter:8809] Loss: 0.020 | Acc: 98.973% \n",
      "[epoch:92, iter:8810] Loss: 0.019 | Acc: 98.986% \n",
      "[epoch:92, iter:8811] Loss: 0.019 | Acc: 99.000% \n",
      "[epoch:92, iter:8812] Loss: 0.020 | Acc: 98.931% \n",
      "[epoch:92, iter:8813] Loss: 0.020 | Acc: 98.945% \n",
      "[epoch:92, iter:8814] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:92, iter:8815] Loss: 0.020 | Acc: 98.892% \n",
      "[epoch:92, iter:8816] Loss: 0.020 | Acc: 98.906% \n",
      "[epoch:92, iter:8817] Loss: 0.020 | Acc: 98.920% \n",
      "[epoch:92, iter:8818] Loss: 0.020 | Acc: 98.857% \n",
      "[epoch:92, iter:8819] Loss: 0.021 | Acc: 98.795% \n",
      "[epoch:92, iter:8820] Loss: 0.021 | Acc: 98.810% \n",
      "[epoch:92, iter:8821] Loss: 0.021 | Acc: 98.824% \n",
      "[epoch:92, iter:8822] Loss: 0.021 | Acc: 98.837% \n",
      "[epoch:92, iter:8823] Loss: 0.021 | Acc: 98.851% \n",
      "[epoch:92, iter:8824] Loss: 0.020 | Acc: 98.864% \n",
      "[epoch:92, iter:8825] Loss: 0.021 | Acc: 98.876% \n",
      "[epoch:92, iter:8826] Loss: 0.027 | Acc: 98.819% \n",
      "[epoch:92, iter:8827] Loss: 0.028 | Acc: 98.764% \n",
      "[epoch:92, iter:8828] Loss: 0.027 | Acc: 98.777% \n",
      "[epoch:92, iter:8829] Loss: 0.027 | Acc: 98.790% \n",
      "[epoch:92, iter:8830] Loss: 0.027 | Acc: 98.803% \n",
      "[epoch:92, iter:8831] Loss: 0.027 | Acc: 98.816% \n",
      "[epoch:92, iter:8832] Loss: 0.026 | Acc: 98.823% \n",
      "Waiting Test!\n",
      "测试分类准确率为：75.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 93\n",
      "[epoch:93, iter:8833] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:93, iter:8834] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:93, iter:8835] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:93, iter:8836] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:93, iter:8837] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:93, iter:8838] Loss: 0.002 | Acc: 100.000% \n",
      "[epoch:93, iter:8839] Loss: 0.008 | Acc: 99.107% \n",
      "[epoch:93, iter:8840] Loss: 0.008 | Acc: 99.219% \n",
      "[epoch:93, iter:8841] Loss: 0.019 | Acc: 98.611% \n",
      "[epoch:93, iter:8842] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:93, iter:8843] Loss: 0.019 | Acc: 98.864% \n",
      "[epoch:93, iter:8844] Loss: 0.018 | Acc: 98.958% \n",
      "[epoch:93, iter:8845] Loss: 0.017 | Acc: 99.038% \n",
      "[epoch:93, iter:8846] Loss: 0.018 | Acc: 99.107% \n",
      "[epoch:93, iter:8847] Loss: 0.021 | Acc: 99.167% \n",
      "[epoch:93, iter:8848] Loss: 0.020 | Acc: 99.219% \n",
      "[epoch:93, iter:8849] Loss: 0.018 | Acc: 99.265% \n",
      "[epoch:93, iter:8850] Loss: 0.017 | Acc: 99.306% \n",
      "[epoch:93, iter:8851] Loss: 0.017 | Acc: 99.342% \n",
      "[epoch:93, iter:8852] Loss: 0.017 | Acc: 99.375% \n",
      "[epoch:93, iter:8853] Loss: 0.019 | Acc: 99.107% \n",
      "[epoch:93, iter:8854] Loss: 0.021 | Acc: 98.864% \n",
      "[epoch:93, iter:8855] Loss: 0.023 | Acc: 98.641% \n",
      "[epoch:93, iter:8856] Loss: 0.023 | Acc: 98.698% \n",
      "[epoch:93, iter:8857] Loss: 0.022 | Acc: 98.750% \n",
      "[epoch:93, iter:8858] Loss: 0.024 | Acc: 98.317% \n",
      "[epoch:93, iter:8859] Loss: 0.023 | Acc: 98.380% \n",
      "[epoch:93, iter:8860] Loss: 0.027 | Acc: 98.214% \n",
      "[epoch:93, iter:8861] Loss: 0.026 | Acc: 98.276% \n",
      "[epoch:93, iter:8862] Loss: 0.025 | Acc: 98.333% \n",
      "[epoch:93, iter:8863] Loss: 0.024 | Acc: 98.387% \n",
      "[epoch:93, iter:8864] Loss: 0.026 | Acc: 98.242% \n",
      "[epoch:93, iter:8865] Loss: 0.026 | Acc: 98.295% \n",
      "[epoch:93, iter:8866] Loss: 0.025 | Acc: 98.346% \n",
      "[epoch:93, iter:8867] Loss: 0.025 | Acc: 98.393% \n",
      "[epoch:93, iter:8868] Loss: 0.025 | Acc: 98.438% \n",
      "[epoch:93, iter:8869] Loss: 0.024 | Acc: 98.480% \n",
      "[epoch:93, iter:8870] Loss: 0.023 | Acc: 98.520% \n",
      "[epoch:93, iter:8871] Loss: 0.023 | Acc: 98.558% \n",
      "[epoch:93, iter:8872] Loss: 0.023 | Acc: 98.594% \n",
      "[epoch:93, iter:8873] Loss: 0.022 | Acc: 98.628% \n",
      "[epoch:93, iter:8874] Loss: 0.023 | Acc: 98.512% \n",
      "[epoch:93, iter:8875] Loss: 0.024 | Acc: 98.401% \n",
      "[epoch:93, iter:8876] Loss: 0.026 | Acc: 98.153% \n",
      "[epoch:93, iter:8877] Loss: 0.026 | Acc: 98.056% \n",
      "[epoch:93, iter:8878] Loss: 0.026 | Acc: 98.098% \n",
      "[epoch:93, iter:8879] Loss: 0.026 | Acc: 98.138% \n",
      "[epoch:93, iter:8880] Loss: 0.026 | Acc: 98.177% \n",
      "[epoch:93, iter:8881] Loss: 0.025 | Acc: 98.214% \n",
      "[epoch:93, iter:8882] Loss: 0.025 | Acc: 98.250% \n",
      "[epoch:93, iter:8883] Loss: 0.025 | Acc: 98.284% \n",
      "[epoch:93, iter:8884] Loss: 0.025 | Acc: 98.197% \n",
      "[epoch:93, iter:8885] Loss: 0.025 | Acc: 98.231% \n",
      "[epoch:93, iter:8886] Loss: 0.025 | Acc: 98.148% \n",
      "[epoch:93, iter:8887] Loss: 0.026 | Acc: 98.068% \n",
      "[epoch:93, iter:8888] Loss: 0.026 | Acc: 98.103% \n",
      "[epoch:93, iter:8889] Loss: 0.025 | Acc: 98.136% \n",
      "[epoch:93, iter:8890] Loss: 0.025 | Acc: 98.168% \n",
      "[epoch:93, iter:8891] Loss: 0.024 | Acc: 98.199% \n",
      "[epoch:93, iter:8892] Loss: 0.024 | Acc: 98.229% \n",
      "[epoch:93, iter:8893] Loss: 0.024 | Acc: 98.258% \n",
      "[epoch:93, iter:8894] Loss: 0.024 | Acc: 98.185% \n",
      "[epoch:93, iter:8895] Loss: 0.024 | Acc: 98.214% \n",
      "[epoch:93, iter:8896] Loss: 0.025 | Acc: 98.145% \n",
      "[epoch:93, iter:8897] Loss: 0.025 | Acc: 98.173% \n",
      "[epoch:93, iter:8898] Loss: 0.024 | Acc: 98.201% \n",
      "[epoch:93, iter:8899] Loss: 0.024 | Acc: 98.228% \n",
      "[epoch:93, iter:8900] Loss: 0.024 | Acc: 98.254% \n",
      "[epoch:93, iter:8901] Loss: 0.023 | Acc: 98.279% \n",
      "[epoch:93, iter:8902] Loss: 0.023 | Acc: 98.304% \n",
      "[epoch:93, iter:8903] Loss: 0.023 | Acc: 98.327% \n",
      "[epoch:93, iter:8904] Loss: 0.023 | Acc: 98.351% \n",
      "[epoch:93, iter:8905] Loss: 0.023 | Acc: 98.373% \n",
      "[epoch:93, iter:8906] Loss: 0.022 | Acc: 98.395% \n",
      "[epoch:93, iter:8907] Loss: 0.023 | Acc: 98.333% \n",
      "[epoch:93, iter:8908] Loss: 0.023 | Acc: 98.355% \n",
      "[epoch:93, iter:8909] Loss: 0.023 | Acc: 98.377% \n",
      "[epoch:93, iter:8910] Loss: 0.022 | Acc: 98.397% \n",
      "[epoch:93, iter:8911] Loss: 0.022 | Acc: 98.418% \n",
      "[epoch:93, iter:8912] Loss: 0.022 | Acc: 98.438% \n",
      "[epoch:93, iter:8913] Loss: 0.022 | Acc: 98.380% \n",
      "[epoch:93, iter:8914] Loss: 0.022 | Acc: 98.399% \n",
      "[epoch:93, iter:8915] Loss: 0.022 | Acc: 98.419% \n",
      "[epoch:93, iter:8916] Loss: 0.022 | Acc: 98.438% \n",
      "[epoch:93, iter:8917] Loss: 0.023 | Acc: 98.382% \n",
      "[epoch:93, iter:8918] Loss: 0.022 | Acc: 98.401% \n",
      "[epoch:93, iter:8919] Loss: 0.022 | Acc: 98.420% \n",
      "[epoch:93, iter:8920] Loss: 0.022 | Acc: 98.438% \n",
      "[epoch:93, iter:8921] Loss: 0.022 | Acc: 98.455% \n",
      "[epoch:93, iter:8922] Loss: 0.023 | Acc: 98.403% \n",
      "[epoch:93, iter:8923] Loss: 0.023 | Acc: 98.420% \n",
      "[epoch:93, iter:8924] Loss: 0.023 | Acc: 98.438% \n",
      "[epoch:93, iter:8925] Loss: 0.023 | Acc: 98.454% \n",
      "[epoch:93, iter:8926] Loss: 0.023 | Acc: 98.471% \n",
      "[epoch:93, iter:8927] Loss: 0.022 | Acc: 98.487% \n",
      "[epoch:93, iter:8928] Loss: 0.022 | Acc: 98.496% \n",
      "Waiting Test!\n",
      "测试分类准确率为：86.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 94\n",
      "[epoch:94, iter:8929] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:94, iter:8930] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:94, iter:8931] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:94, iter:8932] Loss: 0.011 | Acc: 100.000% \n",
      "[epoch:94, iter:8933] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:94, iter:8934] Loss: 0.008 | Acc: 100.000% \n",
      "[epoch:94, iter:8935] Loss: 0.013 | Acc: 99.107% \n",
      "[epoch:94, iter:8936] Loss: 0.012 | Acc: 99.219% \n",
      "[epoch:94, iter:8937] Loss: 0.016 | Acc: 98.611% \n",
      "[epoch:94, iter:8938] Loss: 0.014 | Acc: 98.750% \n",
      "[epoch:94, iter:8939] Loss: 0.013 | Acc: 98.864% \n",
      "[epoch:94, iter:8940] Loss: 0.017 | Acc: 98.438% \n",
      "[epoch:94, iter:8941] Loss: 0.023 | Acc: 97.596% \n",
      "[epoch:94, iter:8942] Loss: 0.024 | Acc: 97.768% \n",
      "[epoch:94, iter:8943] Loss: 0.022 | Acc: 97.917% \n",
      "[epoch:94, iter:8944] Loss: 0.023 | Acc: 98.047% \n",
      "[epoch:94, iter:8945] Loss: 0.022 | Acc: 98.162% \n",
      "[epoch:94, iter:8946] Loss: 0.021 | Acc: 98.264% \n",
      "[epoch:94, iter:8947] Loss: 0.020 | Acc: 98.355% \n",
      "[epoch:94, iter:8948] Loss: 0.019 | Acc: 98.438% \n",
      "[epoch:94, iter:8949] Loss: 0.019 | Acc: 98.512% \n",
      "[epoch:94, iter:8950] Loss: 0.019 | Acc: 98.580% \n",
      "[epoch:94, iter:8951] Loss: 0.018 | Acc: 98.641% \n",
      "[epoch:94, iter:8952] Loss: 0.020 | Acc: 98.698% \n",
      "[epoch:94, iter:8953] Loss: 0.019 | Acc: 98.750% \n",
      "[epoch:94, iter:8954] Loss: 0.019 | Acc: 98.798% \n",
      "[epoch:94, iter:8955] Loss: 0.018 | Acc: 98.843% \n",
      "[epoch:94, iter:8956] Loss: 0.019 | Acc: 98.884% \n",
      "[epoch:94, iter:8957] Loss: 0.018 | Acc: 98.922% \n",
      "[epoch:94, iter:8958] Loss: 0.019 | Acc: 98.750% \n",
      "[epoch:94, iter:8959] Loss: 0.018 | Acc: 98.790% \n",
      "[epoch:94, iter:8960] Loss: 0.018 | Acc: 98.828% \n",
      "[epoch:94, iter:8961] Loss: 0.017 | Acc: 98.864% \n",
      "[epoch:94, iter:8962] Loss: 0.017 | Acc: 98.897% \n",
      "[epoch:94, iter:8963] Loss: 0.016 | Acc: 98.929% \n",
      "[epoch:94, iter:8964] Loss: 0.016 | Acc: 98.958% \n",
      "[epoch:94, iter:8965] Loss: 0.018 | Acc: 98.818% \n",
      "[epoch:94, iter:8966] Loss: 0.019 | Acc: 98.684% \n",
      "[epoch:94, iter:8967] Loss: 0.018 | Acc: 98.718% \n",
      "[epoch:94, iter:8968] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:94, iter:8969] Loss: 0.017 | Acc: 98.780% \n",
      "[epoch:94, iter:8970] Loss: 0.017 | Acc: 98.810% \n",
      "[epoch:94, iter:8971] Loss: 0.019 | Acc: 98.692% \n",
      "[epoch:94, iter:8972] Loss: 0.018 | Acc: 98.722% \n",
      "[epoch:94, iter:8973] Loss: 0.019 | Acc: 98.750% \n",
      "[epoch:94, iter:8974] Loss: 0.018 | Acc: 98.777% \n",
      "[epoch:94, iter:8975] Loss: 0.018 | Acc: 98.803% \n",
      "[epoch:94, iter:8976] Loss: 0.018 | Acc: 98.828% \n",
      "[epoch:94, iter:8977] Loss: 0.017 | Acc: 98.852% \n",
      "[epoch:94, iter:8978] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:94, iter:8979] Loss: 0.019 | Acc: 98.652% \n",
      "[epoch:94, iter:8980] Loss: 0.018 | Acc: 98.678% \n",
      "[epoch:94, iter:8981] Loss: 0.018 | Acc: 98.703% \n",
      "[epoch:94, iter:8982] Loss: 0.018 | Acc: 98.727% \n",
      "[epoch:94, iter:8983] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:94, iter:8984] Loss: 0.017 | Acc: 98.772% \n",
      "[epoch:94, iter:8985] Loss: 0.017 | Acc: 98.794% \n",
      "[epoch:94, iter:8986] Loss: 0.018 | Acc: 98.707% \n",
      "[epoch:94, iter:8987] Loss: 0.018 | Acc: 98.729% \n",
      "[epoch:94, iter:8988] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:94, iter:8989] Loss: 0.017 | Acc: 98.770% \n",
      "[epoch:94, iter:8990] Loss: 0.017 | Acc: 98.790% \n",
      "[epoch:94, iter:8991] Loss: 0.018 | Acc: 98.710% \n",
      "[epoch:94, iter:8992] Loss: 0.017 | Acc: 98.730% \n",
      "[epoch:94, iter:8993] Loss: 0.018 | Acc: 98.654% \n",
      "[epoch:94, iter:8994] Loss: 0.018 | Acc: 98.674% \n",
      "[epoch:94, iter:8995] Loss: 0.018 | Acc: 98.694% \n",
      "[epoch:94, iter:8996] Loss: 0.017 | Acc: 98.713% \n",
      "[epoch:94, iter:8997] Loss: 0.018 | Acc: 98.641% \n",
      "[epoch:94, iter:8998] Loss: 0.017 | Acc: 98.661% \n",
      "[epoch:94, iter:8999] Loss: 0.017 | Acc: 98.680% \n",
      "[epoch:94, iter:9000] Loss: 0.018 | Acc: 98.611% \n",
      "[epoch:94, iter:9001] Loss: 0.018 | Acc: 98.545% \n",
      "[epoch:94, iter:9002] Loss: 0.018 | Acc: 98.564% \n",
      "[epoch:94, iter:9003] Loss: 0.018 | Acc: 98.583% \n",
      "[epoch:94, iter:9004] Loss: 0.017 | Acc: 98.602% \n",
      "[epoch:94, iter:9005] Loss: 0.017 | Acc: 98.620% \n",
      "[epoch:94, iter:9006] Loss: 0.017 | Acc: 98.558% \n",
      "[epoch:94, iter:9007] Loss: 0.017 | Acc: 98.576% \n",
      "[epoch:94, iter:9008] Loss: 0.018 | Acc: 98.594% \n",
      "[epoch:94, iter:9009] Loss: 0.017 | Acc: 98.611% \n",
      "[epoch:94, iter:9010] Loss: 0.017 | Acc: 98.628% \n",
      "[epoch:94, iter:9011] Loss: 0.017 | Acc: 98.569% \n",
      "[epoch:94, iter:9012] Loss: 0.018 | Acc: 98.586% \n",
      "[epoch:94, iter:9013] Loss: 0.018 | Acc: 98.603% \n",
      "[epoch:94, iter:9014] Loss: 0.018 | Acc: 98.619% \n",
      "[epoch:94, iter:9015] Loss: 0.018 | Acc: 98.635% \n",
      "[epoch:94, iter:9016] Loss: 0.018 | Acc: 98.651% \n",
      "[epoch:94, iter:9017] Loss: 0.018 | Acc: 98.596% \n",
      "[epoch:94, iter:9018] Loss: 0.018 | Acc: 98.611% \n",
      "[epoch:94, iter:9019] Loss: 0.018 | Acc: 98.626% \n",
      "[epoch:94, iter:9020] Loss: 0.018 | Acc: 98.641% \n",
      "[epoch:94, iter:9021] Loss: 0.019 | Acc: 98.522% \n",
      "[epoch:94, iter:9022] Loss: 0.018 | Acc: 98.537% \n",
      "[epoch:94, iter:9023] Loss: 0.018 | Acc: 98.553% \n",
      "[epoch:94, iter:9024] Loss: 0.018 | Acc: 98.561% \n",
      "Waiting Test!\n",
      "测试分类准确率为：88.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 95\n",
      "[epoch:95, iter:9025] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:95, iter:9026] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:95, iter:9027] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:95, iter:9028] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:95, iter:9029] Loss: 0.005 | Acc: 100.000% \n",
      "[epoch:95, iter:9030] Loss: 0.004 | Acc: 100.000% \n",
      "[epoch:95, iter:9031] Loss: 0.010 | Acc: 100.000% \n",
      "[epoch:95, iter:9032] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:95, iter:9033] Loss: 0.008 | Acc: 100.000% \n",
      "[epoch:95, iter:9034] Loss: 0.007 | Acc: 100.000% \n",
      "[epoch:95, iter:9035] Loss: 0.007 | Acc: 100.000% \n",
      "[epoch:95, iter:9036] Loss: 0.008 | Acc: 100.000% \n",
      "[epoch:95, iter:9037] Loss: 0.008 | Acc: 100.000% \n",
      "[epoch:95, iter:9038] Loss: 0.011 | Acc: 99.554% \n",
      "[epoch:95, iter:9039] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:95, iter:9040] Loss: 0.018 | Acc: 98.828% \n",
      "[epoch:95, iter:9041] Loss: 0.017 | Acc: 98.897% \n",
      "[epoch:95, iter:9042] Loss: 0.018 | Acc: 98.958% \n",
      "[epoch:95, iter:9043] Loss: 0.018 | Acc: 99.013% \n",
      "[epoch:95, iter:9044] Loss: 0.017 | Acc: 99.062% \n",
      "[epoch:95, iter:9045] Loss: 0.016 | Acc: 99.107% \n",
      "[epoch:95, iter:9046] Loss: 0.017 | Acc: 99.148% \n",
      "[epoch:95, iter:9047] Loss: 0.023 | Acc: 98.913% \n",
      "[epoch:95, iter:9048] Loss: 0.022 | Acc: 98.958% \n",
      "[epoch:95, iter:9049] Loss: 0.021 | Acc: 99.000% \n",
      "[epoch:95, iter:9050] Loss: 0.020 | Acc: 99.038% \n",
      "[epoch:95, iter:9051] Loss: 0.021 | Acc: 98.843% \n",
      "[epoch:95, iter:9052] Loss: 0.021 | Acc: 98.884% \n",
      "[epoch:95, iter:9053] Loss: 0.021 | Acc: 98.922% \n",
      "[epoch:95, iter:9054] Loss: 0.020 | Acc: 98.958% \n",
      "[epoch:95, iter:9055] Loss: 0.021 | Acc: 98.790% \n",
      "[epoch:95, iter:9056] Loss: 0.020 | Acc: 98.828% \n",
      "[epoch:95, iter:9057] Loss: 0.020 | Acc: 98.864% \n",
      "[epoch:95, iter:9058] Loss: 0.019 | Acc: 98.897% \n",
      "[epoch:95, iter:9059] Loss: 0.018 | Acc: 98.929% \n",
      "[epoch:95, iter:9060] Loss: 0.019 | Acc: 98.785% \n",
      "[epoch:95, iter:9061] Loss: 0.019 | Acc: 98.818% \n",
      "[epoch:95, iter:9062] Loss: 0.018 | Acc: 98.849% \n",
      "[epoch:95, iter:9063] Loss: 0.018 | Acc: 98.878% \n",
      "[epoch:95, iter:9064] Loss: 0.017 | Acc: 98.906% \n",
      "[epoch:95, iter:9065] Loss: 0.017 | Acc: 98.933% \n",
      "[epoch:95, iter:9066] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:95, iter:9067] Loss: 0.016 | Acc: 98.983% \n",
      "[epoch:95, iter:9068] Loss: 0.016 | Acc: 99.006% \n",
      "[epoch:95, iter:9069] Loss: 0.016 | Acc: 99.028% \n",
      "[epoch:95, iter:9070] Loss: 0.017 | Acc: 98.913% \n",
      "[epoch:95, iter:9071] Loss: 0.017 | Acc: 98.936% \n",
      "[epoch:95, iter:9072] Loss: 0.017 | Acc: 98.828% \n",
      "[epoch:95, iter:9073] Loss: 0.017 | Acc: 98.852% \n",
      "[epoch:95, iter:9074] Loss: 0.017 | Acc: 98.875% \n",
      "[epoch:95, iter:9075] Loss: 0.017 | Acc: 98.775% \n",
      "[epoch:95, iter:9076] Loss: 0.018 | Acc: 98.678% \n",
      "[epoch:95, iter:9077] Loss: 0.019 | Acc: 98.585% \n",
      "[epoch:95, iter:9078] Loss: 0.018 | Acc: 98.611% \n",
      "[epoch:95, iter:9079] Loss: 0.018 | Acc: 98.636% \n",
      "[epoch:95, iter:9080] Loss: 0.018 | Acc: 98.661% \n",
      "[epoch:95, iter:9081] Loss: 0.018 | Acc: 98.684% \n",
      "[epoch:95, iter:9082] Loss: 0.017 | Acc: 98.707% \n",
      "[epoch:95, iter:9083] Loss: 0.018 | Acc: 98.729% \n",
      "[epoch:95, iter:9084] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:95, iter:9085] Loss: 0.017 | Acc: 98.770% \n",
      "[epoch:95, iter:9086] Loss: 0.017 | Acc: 98.790% \n",
      "[epoch:95, iter:9087] Loss: 0.017 | Acc: 98.810% \n",
      "[epoch:95, iter:9088] Loss: 0.017 | Acc: 98.730% \n",
      "[epoch:95, iter:9089] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:95, iter:9090] Loss: 0.017 | Acc: 98.769% \n",
      "[epoch:95, iter:9091] Loss: 0.016 | Acc: 98.787% \n",
      "[epoch:95, iter:9092] Loss: 0.016 | Acc: 98.805% \n",
      "[epoch:95, iter:9093] Loss: 0.016 | Acc: 98.822% \n",
      "[epoch:95, iter:9094] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:95, iter:9095] Loss: 0.017 | Acc: 98.768% \n",
      "[epoch:95, iter:9096] Loss: 0.017 | Acc: 98.785% \n",
      "[epoch:95, iter:9097] Loss: 0.017 | Acc: 98.716% \n",
      "[epoch:95, iter:9098] Loss: 0.017 | Acc: 98.733% \n",
      "[epoch:95, iter:9099] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:95, iter:9100] Loss: 0.017 | Acc: 98.684% \n",
      "[epoch:95, iter:9101] Loss: 0.017 | Acc: 98.701% \n",
      "[epoch:95, iter:9102] Loss: 0.017 | Acc: 98.718% \n",
      "[epoch:95, iter:9103] Loss: 0.016 | Acc: 98.734% \n",
      "[epoch:95, iter:9104] Loss: 0.016 | Acc: 98.750% \n",
      "[epoch:95, iter:9105] Loss: 0.017 | Acc: 98.765% \n",
      "[epoch:95, iter:9106] Loss: 0.016 | Acc: 98.780% \n",
      "[epoch:95, iter:9107] Loss: 0.017 | Acc: 98.795% \n",
      "[epoch:95, iter:9108] Loss: 0.017 | Acc: 98.810% \n",
      "[epoch:95, iter:9109] Loss: 0.016 | Acc: 98.824% \n",
      "[epoch:95, iter:9110] Loss: 0.016 | Acc: 98.837% \n",
      "[epoch:95, iter:9111] Loss: 0.016 | Acc: 98.851% \n",
      "[epoch:95, iter:9112] Loss: 0.017 | Acc: 98.793% \n",
      "[epoch:95, iter:9113] Loss: 0.016 | Acc: 98.806% \n",
      "[epoch:95, iter:9114] Loss: 0.018 | Acc: 98.681% \n",
      "[epoch:95, iter:9115] Loss: 0.018 | Acc: 98.695% \n",
      "[epoch:95, iter:9116] Loss: 0.017 | Acc: 98.709% \n",
      "[epoch:95, iter:9117] Loss: 0.017 | Acc: 98.723% \n",
      "[epoch:95, iter:9118] Loss: 0.017 | Acc: 98.737% \n",
      "[epoch:95, iter:9119] Loss: 0.017 | Acc: 98.750% \n",
      "[epoch:95, iter:9120] Loss: 0.018 | Acc: 98.692% \n",
      "Waiting Test!\n",
      "测试分类准确率为：88.500%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 96\n",
      "[epoch:96, iter:9121] Loss: 0.013 | Acc: 100.000% \n",
      "[epoch:96, iter:9122] Loss: 0.007 | Acc: 100.000% \n",
      "[epoch:96, iter:9123] Loss: 0.011 | Acc: 100.000% \n",
      "[epoch:96, iter:9124] Loss: 0.009 | Acc: 100.000% \n",
      "[epoch:96, iter:9125] Loss: 0.015 | Acc: 100.000% \n",
      "[epoch:96, iter:9126] Loss: 0.012 | Acc: 100.000% \n",
      "[epoch:96, iter:9127] Loss: 0.011 | Acc: 100.000% \n",
      "[epoch:96, iter:9128] Loss: 0.014 | Acc: 100.000% \n",
      "[epoch:96, iter:9129] Loss: 0.017 | Acc: 99.306% \n",
      "[epoch:96, iter:9130] Loss: 0.019 | Acc: 99.375% \n",
      "[epoch:96, iter:9131] Loss: 0.021 | Acc: 99.432% \n",
      "[epoch:96, iter:9132] Loss: 0.019 | Acc: 99.479% \n",
      "[epoch:96, iter:9133] Loss: 0.017 | Acc: 99.519% \n",
      "[epoch:96, iter:9134] Loss: 0.016 | Acc: 99.554% \n",
      "[epoch:96, iter:9135] Loss: 0.015 | Acc: 99.583% \n",
      "[epoch:96, iter:9136] Loss: 0.017 | Acc: 99.609% \n",
      "[epoch:96, iter:9137] Loss: 0.019 | Acc: 99.265% \n",
      "[epoch:96, iter:9138] Loss: 0.018 | Acc: 99.306% \n",
      "[epoch:96, iter:9139] Loss: 0.017 | Acc: 99.342% \n",
      "[epoch:96, iter:9140] Loss: 0.016 | Acc: 99.375% \n",
      "[epoch:96, iter:9141] Loss: 0.019 | Acc: 99.107% \n",
      "[epoch:96, iter:9142] Loss: 0.021 | Acc: 98.864% \n",
      "[epoch:96, iter:9143] Loss: 0.020 | Acc: 98.913% \n",
      "[epoch:96, iter:9144] Loss: 0.021 | Acc: 98.698% \n",
      "[epoch:96, iter:9145] Loss: 0.021 | Acc: 98.750% \n",
      "[epoch:96, iter:9146] Loss: 0.020 | Acc: 98.798% \n",
      "[epoch:96, iter:9147] Loss: 0.019 | Acc: 98.843% \n",
      "[epoch:96, iter:9148] Loss: 0.020 | Acc: 98.884% \n",
      "[epoch:96, iter:9149] Loss: 0.019 | Acc: 98.922% \n",
      "[epoch:96, iter:9150] Loss: 0.018 | Acc: 98.958% \n",
      "[epoch:96, iter:9151] Loss: 0.019 | Acc: 98.992% \n",
      "[epoch:96, iter:9152] Loss: 0.018 | Acc: 99.023% \n",
      "[epoch:96, iter:9153] Loss: 0.018 | Acc: 99.053% \n",
      "[epoch:96, iter:9154] Loss: 0.017 | Acc: 99.081% \n",
      "[epoch:96, iter:9155] Loss: 0.017 | Acc: 99.107% \n",
      "[epoch:96, iter:9156] Loss: 0.017 | Acc: 99.132% \n",
      "[epoch:96, iter:9157] Loss: 0.017 | Acc: 99.155% \n",
      "[epoch:96, iter:9158] Loss: 0.017 | Acc: 99.178% \n",
      "[epoch:96, iter:9159] Loss: 0.016 | Acc: 99.199% \n",
      "[epoch:96, iter:9160] Loss: 0.016 | Acc: 99.219% \n",
      "[epoch:96, iter:9161] Loss: 0.015 | Acc: 99.238% \n",
      "[epoch:96, iter:9162] Loss: 0.017 | Acc: 99.256% \n",
      "[epoch:96, iter:9163] Loss: 0.017 | Acc: 99.128% \n",
      "[epoch:96, iter:9164] Loss: 0.017 | Acc: 99.148% \n",
      "[epoch:96, iter:9165] Loss: 0.018 | Acc: 99.028% \n",
      "[epoch:96, iter:9166] Loss: 0.019 | Acc: 98.913% \n",
      "[epoch:96, iter:9167] Loss: 0.019 | Acc: 98.803% \n",
      "[epoch:96, iter:9168] Loss: 0.019 | Acc: 98.828% \n",
      "[epoch:96, iter:9169] Loss: 0.019 | Acc: 98.852% \n",
      "[epoch:96, iter:9170] Loss: 0.019 | Acc: 98.750% \n",
      "[epoch:96, iter:9171] Loss: 0.019 | Acc: 98.775% \n",
      "[epoch:96, iter:9172] Loss: 0.019 | Acc: 98.798% \n",
      "[epoch:96, iter:9173] Loss: 0.019 | Acc: 98.821% \n",
      "[epoch:96, iter:9174] Loss: 0.018 | Acc: 98.843% \n",
      "[epoch:96, iter:9175] Loss: 0.018 | Acc: 98.864% \n",
      "[epoch:96, iter:9176] Loss: 0.018 | Acc: 98.884% \n",
      "[epoch:96, iter:9177] Loss: 0.017 | Acc: 98.904% \n",
      "[epoch:96, iter:9178] Loss: 0.018 | Acc: 98.922% \n",
      "[epoch:96, iter:9179] Loss: 0.018 | Acc: 98.941% \n",
      "[epoch:96, iter:9180] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:96, iter:9181] Loss: 0.017 | Acc: 98.975% \n",
      "[epoch:96, iter:9182] Loss: 0.018 | Acc: 98.891% \n",
      "[epoch:96, iter:9183] Loss: 0.017 | Acc: 98.909% \n",
      "[epoch:96, iter:9184] Loss: 0.017 | Acc: 98.926% \n",
      "[epoch:96, iter:9185] Loss: 0.017 | Acc: 98.942% \n",
      "[epoch:96, iter:9186] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:96, iter:9187] Loss: 0.017 | Acc: 98.881% \n",
      "[epoch:96, iter:9188] Loss: 0.018 | Acc: 98.897% \n",
      "[epoch:96, iter:9189] Loss: 0.017 | Acc: 98.913% \n",
      "[epoch:96, iter:9190] Loss: 0.017 | Acc: 98.929% \n",
      "[epoch:96, iter:9191] Loss: 0.017 | Acc: 98.944% \n",
      "[epoch:96, iter:9192] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:96, iter:9193] Loss: 0.017 | Acc: 98.973% \n",
      "[epoch:96, iter:9194] Loss: 0.018 | Acc: 98.902% \n",
      "[epoch:96, iter:9195] Loss: 0.018 | Acc: 98.917% \n",
      "[epoch:96, iter:9196] Loss: 0.017 | Acc: 98.931% \n",
      "[epoch:96, iter:9197] Loss: 0.017 | Acc: 98.945% \n",
      "[epoch:96, iter:9198] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:96, iter:9199] Loss: 0.017 | Acc: 98.972% \n",
      "[epoch:96, iter:9200] Loss: 0.017 | Acc: 98.906% \n",
      "[epoch:96, iter:9201] Loss: 0.017 | Acc: 98.920% \n",
      "[epoch:96, iter:9202] Loss: 0.017 | Acc: 98.933% \n",
      "[epoch:96, iter:9203] Loss: 0.017 | Acc: 98.946% \n",
      "[epoch:96, iter:9204] Loss: 0.017 | Acc: 98.958% \n",
      "[epoch:96, iter:9205] Loss: 0.017 | Acc: 98.971% \n",
      "[epoch:96, iter:9206] Loss: 0.018 | Acc: 98.837% \n",
      "[epoch:96, iter:9207] Loss: 0.018 | Acc: 98.851% \n",
      "[epoch:96, iter:9208] Loss: 0.018 | Acc: 98.864% \n",
      "[epoch:96, iter:9209] Loss: 0.019 | Acc: 98.736% \n",
      "[epoch:96, iter:9210] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:96, iter:9211] Loss: 0.019 | Acc: 98.695% \n",
      "[epoch:96, iter:9212] Loss: 0.019 | Acc: 98.709% \n",
      "[epoch:96, iter:9213] Loss: 0.019 | Acc: 98.723% \n",
      "[epoch:96, iter:9214] Loss: 0.018 | Acc: 98.737% \n",
      "[epoch:96, iter:9215] Loss: 0.018 | Acc: 98.750% \n",
      "[epoch:96, iter:9216] Loss: 0.018 | Acc: 98.757% \n",
      "Waiting Test!\n",
      "测试分类准确率为：86.000%\n",
      "Saving model......\n",
      "\n",
      "Epoch: 97\n",
      "[epoch:97, iter:9217] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:97, iter:9218] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:97, iter:9219] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:97, iter:9220] Loss: 0.001 | Acc: 100.000% \n",
      "[epoch:97, iter:9221] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:97, iter:9222] Loss: 0.000 | Acc: 100.000% \n",
      "[epoch:97, iter:9223] Loss: 0.008 | Acc: 99.107% \n",
      "[epoch:97, iter:9224] Loss: 0.007 | Acc: 99.219% \n",
      "[epoch:97, iter:9225] Loss: 0.007 | Acc: 99.306% \n",
      "[epoch:97, iter:9226] Loss: 0.010 | Acc: 99.375% \n",
      "[epoch:97, iter:9227] Loss: 0.009 | Acc: 99.432% \n",
      "[epoch:97, iter:9228] Loss: 0.008 | Acc: 99.479% \n",
      "[epoch:97, iter:9229] Loss: 0.008 | Acc: 99.519% \n",
      "[epoch:97, iter:9230] Loss: 0.015 | Acc: 99.107% \n",
      "[epoch:97, iter:9231] Loss: 0.017 | Acc: 99.167% \n",
      "[epoch:97, iter:9232] Loss: 0.018 | Acc: 98.828% \n",
      "[epoch:97, iter:9233] Loss: 0.017 | Acc: 98.897% \n",
      "[epoch:97, iter:9234] Loss: 0.016 | Acc: 98.958% \n",
      "[epoch:97, iter:9235] Loss: 0.016 | Acc: 99.013% \n",
      "[epoch:97, iter:9236] Loss: 0.015 | Acc: 99.062% \n",
      "[epoch:97, iter:9237] Loss: 0.014 | Acc: 99.107% \n",
      "[epoch:97, iter:9238] Loss: 0.014 | Acc: 99.148% \n",
      "[epoch:97, iter:9239] Loss: 0.015 | Acc: 98.913% \n",
      "[epoch:97, iter:9240] Loss: 0.017 | Acc: 98.698% \n",
      "[epoch:97, iter:9241] Loss: 0.016 | Acc: 98.750% \n",
      "[epoch:97, iter:9242] Loss: 0.015 | Acc: 98.798% \n",
      "[epoch:97, iter:9243] Loss: 0.015 | Acc: 98.843% \n",
      "[epoch:97, iter:9244] Loss: 0.014 | Acc: 98.884% \n",
      "[epoch:97, iter:9245] Loss: 0.014 | Acc: 98.922% \n",
      "[epoch:97, iter:9246] Loss: 0.013 | Acc: 98.958% \n",
      "[epoch:97, iter:9247] Loss: 0.014 | Acc: 98.790% \n",
      "[epoch:97, iter:9248] Loss: 0.014 | Acc: 98.828% \n",
      "[epoch:97, iter:9249] Loss: 0.014 | Acc: 98.864% \n",
      "[epoch:97, iter:9250] Loss: 0.013 | Acc: 98.897% \n",
      "[epoch:97, iter:9251] Loss: 0.013 | Acc: 98.929% \n",
      "[epoch:97, iter:9252] Loss: 0.013 | Acc: 98.958% \n",
      "[epoch:97, iter:9253] Loss: 0.013 | Acc: 98.986% \n",
      "[epoch:97, iter:9254] Loss: 0.013 | Acc: 99.013% \n",
      "[epoch:97, iter:9255] Loss: 0.012 | Acc: 99.038% \n",
      "[epoch:97, iter:9256] Loss: 0.012 | Acc: 99.062% \n",
      "[epoch:97, iter:9257] Loss: 0.013 | Acc: 98.933% \n",
      "[epoch:97, iter:9258] Loss: 0.013 | Acc: 98.958% \n",
      "[epoch:97, iter:9259] Loss: 0.013 | Acc: 98.983% \n",
      "[epoch:97, iter:9260] Loss: 0.013 | Acc: 99.006% \n",
      "[epoch:97, iter:9261] Loss: 0.013 | Acc: 99.028% \n",
      "[epoch:97, iter:9262] Loss: 0.013 | Acc: 99.049% \n",
      "[epoch:97, iter:9263] Loss: 0.012 | Acc: 99.069% \n",
      "[epoch:97, iter:9264] Loss: 0.014 | Acc: 98.958% \n",
      "[epoch:97, iter:9265] Loss: 0.014 | Acc: 98.980% \n",
      "[epoch:97, iter:9266] Loss: 0.014 | Acc: 99.000% \n",
      "[epoch:97, iter:9267] Loss: 0.014 | Acc: 99.020% \n",
      "[epoch:97, iter:9268] Loss: 0.015 | Acc: 98.918% \n",
      "[epoch:97, iter:9269] Loss: 0.016 | Acc: 98.821% \n",
      "[epoch:97, iter:9270] Loss: 0.017 | Acc: 98.843% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fa89ee6edf0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/boer/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     38\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# 准备数据\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader)\n\u001b[1;32m     42\u001b[0m     inputs,inputs1, labels \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m, in \u001b[0;36mSmileDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m---> 13\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(label))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img,img1, label\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:167\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    166\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 167\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    170\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py:701\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 701\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py:771\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    769\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     l, s, d \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    772\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(d)\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "\n",
    "# 定义是否使用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 模型定义-ResNet\n",
    "\n",
    "model=ResNet()\n",
    "model.to(device)\n",
    "\n",
    "# 定义损失函数和优化方式\n",
    "criterion = nn.CrossEntropyLoss()  #损失函数为交叉熵，多用于多分类问题\n",
    "# optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) #优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "\n",
    "# 训练\n",
    "if __name__ == \"__main__\":\n",
    "    outf=\"./resnet_flip/\"\n",
    "    if not os.path.exists(outf):\n",
    "        os.makedirs(outf)\n",
    "    best_acc = 65  #2 初始化best test accuracy\n",
    "    print(\"Start Training, Resnet-50!\")  # 定义遍历数据集的次数\n",
    "    with open(outf+\"acc.txt\", \"w\") as f:\n",
    "        with open(outf+\"log.txt\", \"w\")as f2:\n",
    "            for epoch in range(pre_epoch, EPOCH):\n",
    "                print('\\nEpoch: %d' % (epoch + 1))\n",
    "                model.train()\n",
    "                sum_loss = 0.0\n",
    "                correct = 0.0\n",
    "                total = 0.0\n",
    "                for i, data in enumerate(trainloader, 0):\n",
    "                    # 准备数据\n",
    "                    length = len(trainloader)\n",
    "                    inputs,inputs1, labels = data\n",
    "                    inputs,inputs1, labels = inputs.to(device),inputs1.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "    \n",
    "                    # forward + backward\n",
    "                    outputs = model(inputs,inputs1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "    \n",
    "                    # 每训练1个batch打印一次loss和准确率\n",
    "                    sum_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels.data).cpu().sum()\n",
    "                    print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '\n",
    "                          % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1), 100. * correct / total))\n",
    "                    f2.write('%03d  %05d |Loss: %.03f | Acc: %.3f%% '\n",
    "                          % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1), 100. * correct / total))\n",
    "                    f2.write('\\n')\n",
    "                    f2.flush()\n",
    "    \n",
    "                # 每训练完一个epoch测试一下准确率\n",
    "                print(\"Waiting Test!\")\n",
    "                with torch.no_grad():\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    for data in testloader:\n",
    "                        model.eval()\n",
    "                        # images, labels = data\n",
    "                        # images, labels = images.to(device), labels.to(device)\n",
    "                        images,images1, labels = data\n",
    "                        images,images1, labels = images.to(device),images1.to(device), labels.to(device)\n",
    "                        outputs = model(images,images1)\n",
    "                        # 取得分最高的那个类 (outputs.data的索引号)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum()\n",
    "                    print('测试分类准确率为：%.3f%%' % (100 * correct / total))\n",
    "                    acc = 100. * correct / total\n",
    "                    # 将每次测试结果实时写入acc.txt文件中\n",
    "                    print('Saving model......')\n",
    "                    torch.save(model.state_dict(), '%s/net_%03d.pth' % (outf, epoch + 1))\n",
    "                    f.write(\"EPOCH=%03d,Accuracy= %.3f%%\" % (epoch + 1, acc))\n",
    "                    f.write('\\n')\n",
    "                    f.flush()\n",
    "                    # 记录最佳测试分类准确率并写入best_acc.txt文件中\n",
    "                    if acc > best_acc:\n",
    "                        f3 = open(\"best_acc.txt\", \"w\")\n",
    "                        f3.write(\"EPOCH=%d,best_acc= %.3f%%\" % (epoch + 1, acc))\n",
    "                        f3.close()\n",
    "                        best_acc = acc\n",
    "            print(\"Training Finished, TotalEPOCH=%d\" % EPOCH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672eff3-0cc0-4812-a96b-06b751499429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae789a2-a736-4bc7-a400-c8b17f86aac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8caf83-d5d9-471b-add1-45a7bf4a3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# 加载预训练的 VGG16 模型\n",
    "# pthfile = './vgg16_2/Model_epoch_0.75.pth'\n",
    "# model = torch.load(pthfile)\n",
    "# device = 'cuda'\n",
    "# model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 准备输入图像的变换\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "# preprocess = A.Compose([\n",
    "#     A.HorizontalFlip(p=0.5),  #水平翻转\n",
    "#     # A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0, rotate_limit=15, p=0.5),   # 平移、缩放和旋转\n",
    "#     ToTensorV2()\n",
    "# #     A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.5),  # 锐化操作\n",
    "# #     A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),    # 颜色抖动\n",
    "# #     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "# ])\n",
    "\n",
    "# 加载图像并应用变换\n",
    "img_path = './pics1_imglabel_datasets/val/yes/101_髁突形态基本正常_髁突基本对称_下颌升支基本对称_周海红.jpg'\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "# 确保输入图像的梯度被计算\n",
    "img_tensor.requires_grad = True\n",
    "\n",
    "# 获取最后一个卷积层的输出\n",
    "def hook_feature(module, input, output):\n",
    "    global feature\n",
    "    feature = output\n",
    "    feature.retain_grad()  # 保留 feature 张量的梯度\n",
    "\n",
    "\n",
    "# 注册钩子\n",
    "hook = net.layer4[-1].register_forward_hook(hook_feature)\n",
    "\n",
    "# 前向传播\n",
    "output = net(img_tensor)\n",
    "\n",
    "# 获取预测分数和类别\n",
    "pred_scores = F.softmax(output, dim=1)\n",
    "pred_class = pred_scores.argmax(dim=1).item()\n",
    "\n",
    "print(f\"Predicted class: {pred_class}\")\n",
    "print(f\"Predicted scores: {pred_scores}\")\n",
    "\n",
    "# 反向传播获取梯度\n",
    "net.zero_grad()\n",
    "one_hot_output = torch.zeros((1, output.size()[-1]), dtype=torch.float32, device=output.device)\n",
    "one_hot_output[0][pred_class] = 2\n",
    "output.backward(gradient=one_hot_output, retain_graph=True)\n",
    "\n",
    "# 检查特征图是否正确计算了梯度\n",
    "if feature is None:\n",
    "    print(\"Feature is None. Ensure hook function is correctly set.\")\n",
    "elif feature.grad is None:\n",
    "    print(\"Feature gradients are None. Ensure backward pass is done correctly.\")\n",
    "else:\n",
    "    gradients = feature.grad[0]\n",
    "    print(\"Gradients computed\")\n",
    "\n",
    "    # 计算权重\n",
    "    weights = torch.mean(gradients, dim=[1, 2], keepdim=True)\n",
    "    cam = torch.sum(weights * feature[0], dim=0)\n",
    "\n",
    "    # 将 CAM 标准化\n",
    "    cam = F.relu(cam)\n",
    "    cam = cam - cam.min()\n",
    "    cam = cam / cam.max()\n",
    "\n",
    "    # 将 CAM 调整到输入图像的尺寸\n",
    "    cam = cam.detach().cpu().numpy()\n",
    "    cam = np.uint8(255 * cam)\n",
    "    cam = Image.fromarray(cam).resize(img.size, Image.LANCZOS)\n",
    "    cam = np.array(cam)\n",
    "\n",
    "    # 可视化热力图\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    ax.imshow(cam, cmap='jet', alpha=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2dcbff-9350-4ea1-838f-00fa12a4d3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746522b6-9420-47bf-b817-40758bbd67ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
